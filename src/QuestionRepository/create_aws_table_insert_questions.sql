-- Create the schema for the prepper application
CREATE SCHEMA IF NOT EXISTS prepper;

-- Set the search path to the prepper schema
SET
search_path TO prepper;

-- Drop the old table if it exists to apply the new schema
DROP TABLE IF EXISTS prepper.comptia_cloud_plus_questions;
DROP TABLE IF EXISTS prepper.aws_certified_architect_associate_questions;


-- Create the table for CompTIA Cloud+ questions
-- Note: The schema for this table is kept simple as no new data was provided for it.
CREATE TABLE IF NOT EXISTS prepper.comptia_cloud_plus_questions
(
    id
    SERIAL
    PRIMARY
    KEY,
    question_number
    INTEGER
    UNIQUE
    NOT
    NULL,
    question
    TEXT
    NOT
    NULL,
    options
    JSONB
    NOT
    NULL,
    correct_answer
    TEXT
    NOT
    NULL,
    explanation
    TEXT
);

-- Create the updated table for AWS Certified Architect Associate questions
CREATE TABLE IF NOT EXISTS prepper.aws_certified_architect_associate_questions
(
    id
    SERIAL
    PRIMARY
    KEY,
    question_id
    INTEGER
    UNIQUE
    NOT
    NULL,
    question_number
    INTEGER
    NOT
    NULL,
    category
    TEXT,
    difficulty
    TEXT,
    domain
    TEXT,
    question_text
    TEXT
    NOT
    NULL,
    options
    JSONB
    NOT
    NULL,
    correct_answer
    TEXT
    NOT
    NULL,
    explanation
    TEXT,
    explanation_details
    JSONB
);

-- Insert all AWS Certified Architect Associate questions
INSERT INTO prepper.aws_certified_architect_associate_questions (question_id, question_number, category, difficulty,
                                                                 domain, question_text, options, correct_answer,
                                                                 explanation, explanation_details)
VALUES (101, 1, 'AWS Architecture - High Availability', 'Application', 'Domain 1: Design Resilient Architectures',
        'A company runs a critical e-commerce application on a single EC2 instance in us-east-1a. The application must achieve 99.99% uptime and automatically recover from AZ failures. Which solution provides the MOST cost-effective approach to meet these requirements?',
        '[{"text": "A) Deploy a larger EC2 instance type in the same AZ", "isCorrect": false}, {"text": "B) Create an AMI and manually launch instances in other AZs when needed", "isCorrect": false}, {"text": "C) Use Auto Scaling Group with instances across multiple AZs behind an ALB", "isCorrect": true}, {"text": "D) Use Elastic Beanstalk with a single instance", "isCorrect": false}]',
        'C) Use Auto Scaling Group with instances across multiple AZs behind an ALB',
        'Auto Scaling Groups with multiple AZs provide automatic failure detection and recovery, while ALB distributes traffic and handles health checks.',
        '{"summary": "Multi-AZ Auto Scaling provides automated resilience:", "breakdown": ["Auto Scaling Group automatically replaces failed instances", "Multiple AZs protect against entire AZ failures", "ALB provides health checks and traffic distribution", "Most cost-effective as it only runs minimum required instances"], "otherOptions": "A) Larger instance doesn''t solve AZ failure\nB) Manual process doesn''t meet automation requirement\nD) Single instance still has single point of failure"}'),
       (102, 2, 'AWS Architecture - Data Resilience', 'Application', 'Domain 1: Design Resilient Architectures',
        'A financial services company stores critical transaction data in RDS MySQL. They need to ensure data can be recovered with RPO of 5 minutes and RTO of 15 minutes in case of primary database failure. Which combination of solutions meets these requirements? (Choose TWO)',
        '[{"text": "A) Enable automated backups with 5-minute backup frequency", "isCorrect": false}, {"text": "B) Configure RDS Multi-AZ deployment", "isCorrect": true}, {"text": "C) Create hourly manual snapshots", "isCorrect": false}, {"text": "D) Enable point-in-time recovery", "isCorrect": true}]',
        'B) Configure RDS Multi-AZ deployment',
        'Multi-AZ provides automatic failover within minutes, while point-in-time recovery enables restoration to any second within the backup retention period.',
        '{"summary": "Meeting RPO/RTO requirements:", "breakdown": ["Multi-AZ: Automatic failover typically completes in 60-120 seconds", "Point-in-time recovery: Can restore to any second (meets 5-min RPO)", "Automated backups: Only daily, can''t meet 5-minute RPO", "Manual snapshots: Too infrequent for RPO requirement"], "otherOptions": "A) Automated backups are daily, not every 5 minutes\nC) Hourly snapshots exceed 5-minute RPO requirement"}'),
       (103, 3, 'AWS Architecture - Cross-Region Resilience', 'Expert', 'Domain 1: Design Resilient Architectures',
        'A media company streams video content globally and needs to ensure service availability even if an entire AWS region becomes unavailable. The application uses ALB, EC2 Auto Scaling, and RDS MySQL. Which design provides the MOST comprehensive disaster recovery solution?',
        '[{"text": "A) Create read replicas in multiple regions with manual failover", "isCorrect": false}, {"text": "B) Use Route 53 health checks with cross-region ALBs and RDS cross-region automated backups", "isCorrect": false}, {"text": "C) Deploy identical infrastructure in two regions with Route 53 failover routing and RDS cross-region read replicas with promotion capability", "isCorrect": true}, {"text": "D) Use CloudFormation to deploy infrastructure in multiple regions manually when needed", "isCorrect": false}]',
        'C) Deploy identical infrastructure in two regions with Route 53 failover routing and RDS cross-region read replicas with promotion capability',
        'Full cross-region deployment with automated DNS failover and database replica promotion provides the fastest recovery from regional failures.',
        '{"summary": "Comprehensive multi-region DR strategy:", "breakdown": ["Active-passive setup in multiple regions", "Route 53 automatic failover based on health checks", "RDS read replicas can be promoted to standalone databases", "Infrastructure already running in secondary region"], "otherOptions": "A) Manual failover too slow for video streaming\nB) Cross-region backups take too long to restore\nD) Manual deployment during disaster is too slow"}'),
       (104, 4, 'AWS Storage - Backup Strategy', 'Application', 'Domain 1: Design Resilient Architectures',
        'A company needs to backup 50TB of data from on-premises to AWS. The data must be available within 4 hours during a disaster, and they want to minimize ongoing costs. The initial backup can take up to 2 weeks. Which solution is MOST appropriate?',
        '[{"text": "A) AWS Storage Gateway with Tape Gateway to S3 Glacier", "isCorrect": false}, {"text": "B) AWS Snowball Edge to S3, then lifecycle policy to S3 Intelligent-Tiering", "isCorrect": true}, {"text": "C) Direct upload to S3 Standard over internet connection", "isCorrect": false}, {"text": "D) AWS DataSync to S3 Glacier Deep Archive", "isCorrect": false}]',
        'B) AWS Snowball Edge to S3, then lifecycle policy to S3 Intelligent-Tiering',
        'Snowball Edge handles the initial 50TB transfer efficiently, while S3 Intelligent-Tiering automatically optimizes costs and meets the 4-hour recovery requirement.',
        '{"summary": "Large data backup strategy considerations:", "breakdown": ["Snowball Edge: Cost-effective for 50TB initial transfer", "S3 Intelligent-Tiering: Automatic cost optimization", "Standard tier availability: Immediate access for 4-hour RTO", "2-week initial timeline: Perfect for Snowball shipping"], "otherOptions": "A) Tape Gateway too slow for 4-hour recovery\nC) 50TB over internet too slow and expensive\nD) Glacier Deep Archive takes 12+ hours to retrieve"}'),
       (105, 5, 'AWS Performance - Database Optimization', 'Application',
        'Domain 2: Design High-Performing Architectures',
        'An application experiences slow database performance during peak hours. The RDS MySQL instance shows 95% CPU utilization, and 80% of queries are read operations. The application cannot tolerate any downtime. Which solution provides the BEST performance improvement with minimal operational overhead?',
        '[{"text": "A) Create a larger RDS instance and schedule maintenance window for upgrade", "isCorrect": false}, {"text": "B) Enable RDS Performance Insights to identify slow queries", "isCorrect": false}, {"text": "C) Create RDS read replicas and modify application to use them for read queries", "isCorrect": true}, {"text": "D) Migrate to Amazon Aurora MySQL with automatic scaling", "isCorrect": false}]',
        'C) Create RDS read replicas and modify application to use them for read queries',
        'Read replicas directly address the 80% read workload without downtime, providing immediate performance relief for read-heavy applications.',
        '{"summary": "Read replica benefits for read-heavy workloads:", "breakdown": ["80% read queries can be offloaded to replicas", "No downtime required for setup", "Immediate performance improvement", "Cost-effective solution for read scaling"], "otherOptions": "A) Instance upgrade requires downtime\nB) Performance Insights only identifies issues, doesn''t solve them\nD) Migration requires significant effort and testing"}'),
       (106, 6, 'AWS Performance - Caching Strategy', 'Expert', 'Domain 2: Design High-Performing Architectures',
        'A social media application serves user profiles stored in DynamoDB. The same profiles are accessed frequently by millions of users, causing high read costs and occasional throttling. The application requires sub-millisecond response times. Which caching strategy provides the BEST performance and cost optimization?',
        '[{"text": "A) Implement application-level caching with Redis ElastiCache", "isCorrect": false}, {"text": "B) Enable DynamoDB DAX (DynamoDB Accelerator)", "isCorrect": true}, {"text": "C) Use CloudFront to cache DynamoDB API responses", "isCorrect": false}, {"text": "D) Increase DynamoDB provisioned read capacity", "isCorrect": false}]',
        'B) Enable DynamoDB DAX (DynamoDB Accelerator)',
        'DAX provides microsecond-level response times specifically designed for DynamoDB, with automatic cache management and no application changes required.',
        '{"summary": "DAX advantages for DynamoDB caching:", "breakdown": ["Microsecond response times (sub-millisecond requirement)", "Transparent to application (no code changes)", "Automatic cache invalidation and management", "Reduces DynamoDB read costs significantly"], "otherOptions": "A) Redis requires application code changes and cache management\nC) CloudFront can''t cache DynamoDB API calls\nD) Increasing capacity doesn''t solve latency, only costs more"}'),
       (107, 7, 'AWS Performance - Content Delivery', 'Comprehension', 'Domain 2: Design High-Performing Architectures',
        'A global news website serves static content (images, videos, articles) to users worldwide. Users in Asia report slow page load times, while users in the US experience good performance. The origin servers are located in us-east-1. Which solution provides the MOST effective performance improvement for Asian users?',
        '[{"text": "A) Deploy additional EC2 instances in ap-southeast-1", "isCorrect": false}, {"text": "B) Implement CloudFront distribution with edge locations in Asia", "isCorrect": true}, {"text": "C) Use Transfer Acceleration for S3 uploads", "isCorrect": false}, {"text": "D) Create Route 53 latency-based routing to multiple regions", "isCorrect": false}]',
        'B) Implement CloudFront distribution with edge locations in Asia',
        'CloudFront edge locations cache static content closer to Asian users, dramatically reducing latency for static content delivery.',
        '{"summary": "CloudFront benefits for global content delivery:", "breakdown": ["Edge locations in Asia cache content locally", "Reduces latency from us-east-1 to Asia", "Handles static content (images, videos, articles) efficiently", "Automatic cache management and optimization"], "otherOptions": "A) Additional instances help dynamic content, not static\nC) Transfer Acceleration for uploads, not downloads\nD) Latency routing helps with dynamic content, not static"}'),
       (108, 8, 'AWS Security - IAM Policies', 'Expert', 'Domain 3: Design Secure Applications',
        'A company has multiple AWS accounts and needs to grant developers access to specific S3 buckets based on their team assignment. Developers should only access buckets prefixed with their team name (e.g., team-alpha-*). Which approach provides the MOST secure and scalable solution?',
        '[{"text": "A) Create individual IAM users in each account with specific S3 bucket policies", "isCorrect": false}, {"text": "B) Use AWS Organizations with SCPs and cross-account IAM roles with dynamic policy conditions", "isCorrect": true}, {"text": "C) Create shared IAM users with different access keys for each team", "isCorrect": false}, {"text": "D) Use S3 bucket policies with IAM user conditions for each team", "isCorrect": false}]',
        'B) Use AWS Organizations with SCPs and cross-account IAM roles with dynamic policy conditions',
        'Organizations with SCPs provide account-level governance, while cross-account roles with dynamic conditions enable secure, scalable team-based access.',
        '{"summary": "Multi-account secure access strategy:", "breakdown": ["SCPs provide organization-wide security boundaries", "Cross-account roles eliminate need for multiple IAM users", "Dynamic conditions based on user attributes (team tags)", "Centralized access management across accounts"], "otherOptions": "A) Individual users don''t scale across multiple accounts\nC) Shared users violate security best practices\nD) Bucket policies alone can''t handle multi-account scenarios efficiently"}'),
       (109, 9, 'AWS Security - Data Encryption', 'Application', 'Domain 3: Design Secure Applications',
        'A healthcare company must encrypt all data at rest and in transit. They need to maintain full control over encryption keys and meet HIPAA compliance requirements. The solution must integrate with RDS, S3, and EBS. Which key management approach is MOST appropriate?',
        '[{"text": "A) Use AWS managed keys (SSE-S3, default RDS encryption)", "isCorrect": false}, {"text": "B) Use AWS KMS customer managed keys with CloudTrail logging", "isCorrect": true}, {"text": "C) Use client-side encryption with application-managed keys", "isCorrect": false}, {"text": "D) Use AWS KMS AWS managed keys with detailed monitoring", "isCorrect": false}]',
        'B) Use AWS KMS customer managed keys with CloudTrail logging',
        'Customer managed KMS keys provide full control over key policies, rotation, and access, while CloudTrail provides complete audit trails required for HIPAA compliance.',
        '{"summary": "HIPAA-compliant key management requirements:", "breakdown": ["Customer managed keys: Full control over key policies and access", "CloudTrail: Complete audit trail of key usage", "Cross-service integration: Works with RDS, S3, EBS", "HIPAA compliance: Meets regulatory requirements for key control"], "otherOptions": "A) AWS managed keys don''t provide sufficient control for HIPAA\nC) Client-side encryption complex and doesn''t integrate well\nD) AWS managed keys still don''t give customer full control"}'),
       (110, 10, 'AWS Security - Network Security', 'Application', 'Domain 3: Design Secure Applications',
        'A three-tier web application (web, app, database) needs to be secured according to the principle of least privilege. The web tier must be accessible from the internet, app tier only from web tier, and database tier only from app tier. Which security group configuration is CORRECT?',
        '[{"text": "A) Web SG: 0.0.0.0/0:80,443 | App SG: Web-SG:8080 | DB SG: App-SG:3306", "isCorrect": true}, {"text": "B) Web SG: 0.0.0.0/0:80,443 | App SG: 0.0.0.0/0:8080 | DB SG: 0.0.0.0/0:3306", "isCorrect": false}, {"text": "C) All tiers: VPC CIDR:All ports for internal communication", "isCorrect": false}, {"text": "D) Web SG: 0.0.0.0/0:All | App SG: VPC CIDR:All | DB SG: VPC CIDR:All", "isCorrect": false}]',
        'A) Web SG: 0.0.0.0/0:80,443 | App SG: Web-SG:8080 | DB SG: App-SG:3306',
        'Security groups should reference other security groups for internal communication, implementing true least privilege access between tiers.',
        '{"summary": "Proper three-tier security group design:", "breakdown": ["Web tier: Internet access on standard web ports (80, 443)", "App tier: Only accepts traffic from web security group", "Database tier: Only accepts traffic from app security group", "Security group references: More secure than IP-based rules"], "otherOptions": "B) App and DB exposed to internet\nC) Too broad access within VPC\nD) Overly permissive on all tiers"}'),
       (111, 11, 'AWS Security - Advanced Threats', 'Expert', 'Domain 3: Design Secure Applications',
        'A company''s web application experiences DDoS attacks and SQL injection attempts. They need comprehensive protection that automatically adapts to new attack patterns. The solution must integrate with CloudFront and ALB. Which combination provides the MOST comprehensive protection?',
        '[{"text": "A) AWS Shield Standard + CloudFront + Security Groups", "isCorrect": false}, {"text": "B) AWS Shield Advanced + AWS WAF + AWS Firewall Manager", "isCorrect": true}, {"text": "C) AWS Shield Standard + AWS WAF + VPC Flow Logs", "isCorrect": false}, {"text": "D) Network ACLs + AWS Inspector + CloudWatch", "isCorrect": false}]',
        'B) AWS Shield Advanced + AWS WAF + AWS Firewall Manager',
        'Shield Advanced provides enhanced DDoS protection, WAF blocks application-layer attacks, and Firewall Manager centrally manages security policies across resources.',
        '{"summary": "Comprehensive web application protection stack:", "breakdown": ["Shield Advanced: Enhanced DDoS protection with attack response team", "WAF: Blocks SQL injection, XSS, and other application attacks", "Firewall Manager: Centralized security policy management", "Auto-adaptation: Machine learning capabilities for new threats"], "otherOptions": "A) Shield Standard insufficient for advanced DDoS\nC) VPC Flow Logs don''t provide active protection\nD) Inspector scans instances, doesn''t provide runtime protection"}'),
       (112, 12, 'AWS Cost Optimization - Instance Selection', 'Application',
        'Domain 4: Design Cost-Optimized Architectures',
        'A batch processing workload runs predictably every night from 2 AM to 6 AM for data analytics. The workload can tolerate interruptions and can resume from checkpoints. Current costs using On-Demand instances are $500/month. Which pricing model combination provides the MAXIMUM cost savings?',
        '[{"text": "A) Reserved Instances for consistent baseline capacity", "isCorrect": false}, {"text": "B) Spot Instances with Auto Scaling groups across multiple AZs", "isCorrect": true}, {"text": "C) Savings Plans for compute usage commitment", "isCorrect": false}, {"text": "D) Dedicated Hosts for workload isolation", "isCorrect": false}]',
        'B) Spot Instances with Auto Scaling groups across multiple AZs',
        'Spot Instances can provide up to 90% savings for fault-tolerant batch workloads, and multiple AZs reduce interruption risk.',
        '{"summary": "Spot Instances optimal for batch processing:", "breakdown": ["Up to 90% cost savings vs On-Demand", "Workload tolerates interruptions (checkpoint resume)", "Predictable 4-hour window reduces interruption risk", "Multiple AZs provide capacity diversification"], "otherOptions": "A) Reserved Instances only ~75% savings, overkill for 4-hour workload\nC) Savings Plans better for consistent 24/7 usage\nD) Dedicated Hosts most expensive option"}'),
       (113, 13, 'AWS Cost Optimization - Storage Lifecycle', 'Application',
        'Domain 4: Design Cost-Optimized Architectures',
        'A company stores application logs in S3. Logs are actively analyzed for 30 days, occasionally accessed for 90 days, and must be retained for 7 years for compliance. Current storage costs are $1000/month in S3 Standard. Which lifecycle policy provides the BEST cost optimization?',
        '[{"text": "A) Transition to S3 IA after 30 days, then S3 Glacier after 90 days, then Deep Archive after 1 year", "isCorrect": true}, {"text": "B) Keep all data in S3 Standard for quick access when needed", "isCorrect": false}, {"text": "C) Move everything to S3 Glacier immediately after upload", "isCorrect": false}, {"text": "D) Use S3 Intelligent-Tiering for automatic optimization", "isCorrect": false}]',
        'A) Transition to S3 IA after 30 days, then S3 Glacier after 90 days, then Deep Archive after 1 year',
        'Lifecycle policy matching access patterns: S3 Standard for active use, S3 IA for occasional access, Glacier for long-term retention, Deep Archive for compliance.',
        '{"summary": "Optimal lifecycle transitions for log data:", "breakdown": ["Days 1-30: S3 Standard for active analysis", "Days 31-90: S3 IA for occasional access (50% savings)", "Days 91-365: S3 Glacier for backup (68% savings)", "Years 1-7: Deep Archive for compliance (77% savings)"], "otherOptions": "B) Standard storage most expensive for long-term retention\nC) Glacier immediate transition prevents active analysis\nD) Intelligent-Tiering adds overhead for predictable patterns"}'),
       (114, 14, 'AWS Cost Optimization - Database Costs', 'Expert', 'Domain 4: Design Cost-Optimized Architectures',
        'A startup''s MySQL database experiences unpredictable traffic patterns ranging from 5% to 95% CPU utilization throughout the day. Current RDS costs are $800/month with frequent over-provisioning during low-traffic periods. Which solution provides the BEST cost optimization while maintaining performance?',
        '[{"text": "A) Migrate to Aurora Serverless v2 with automatic scaling", "isCorrect": true}, {"text": "B) Use RDS with scheduled Auto Scaling based on time patterns", "isCorrect": false}, {"text": "C) Switch to smaller RDS instance and add read replicas", "isCorrect": false}, {"text": "D) Migrate to DynamoDB with on-demand billing", "isCorrect": false}]',
        'A) Migrate to Aurora Serverless v2 with automatic scaling',
        'Aurora Serverless v2 automatically scales compute capacity based on actual demand, eliminating over-provisioning costs during low-traffic periods.',
        '{"summary": "Aurora Serverless v2 benefits for variable workloads:", "breakdown": ["Automatic scaling from 0.5 to 128 ACUs based on demand", "Pay only for actual compute used (per-second billing)", "No over-provisioning during low-traffic periods", "MySQL compatibility maintains application compatibility"], "otherOptions": "B) Scheduled scaling can''t handle unpredictable patterns\nC) Smaller instance may cause performance issues during peaks\nD) DynamoDB requires application rewrite from MySQL"}'),
       (115, 15, 'AWS Cost Optimization - Data Transfer', 'Application',
        'Domain 4: Design Cost-Optimized Architectures',
        'A company transfers 100TB of data monthly from EC2 instances to users worldwide. Current data transfer costs are $9,000/month. The data consists of software downloads that rarely change. Which solution provides the MOST significant cost reduction?',
        '[{"text": "A) Use VPC endpoints to reduce data transfer charges", "isCorrect": false}, {"text": "B) Implement CloudFront CDN with S3 origin", "isCorrect": true}, {"text": "C) Move EC2 instances to regions with lower data transfer costs", "isCorrect": false}, {"text": "D) Compress all data before transmission", "isCorrect": false}]',
        'B) Implement CloudFront CDN with S3 origin',
        'CloudFront eliminates most internet data transfer charges and caches static content globally, dramatically reducing costs for software downloads.',
        '{"summary": "CloudFront cost optimization for static content:", "breakdown": ["No data transfer charges between S3 and CloudFront", "Reduced CloudFront to internet rates vs EC2 to internet", "Caching reduces origin data transfer for repeated downloads", "Global edge locations improve performance and reduce costs"], "otherOptions": "A) VPC endpoints help AWS service costs, not internet transfer\nC) Regional pricing differences minimal for internet transfer\nD) Compression helps but doesn''t address core transfer costs"}'),
       (116, 16, 'AWS Architecture - Decoupling', 'Application', 'Domain 1: Design Resilient Architectures',
        'A photo processing application receives images via API, processes them, and stores results. During peak times, the processing queue becomes backlogged and API requests start failing. Which decoupling strategy provides the BEST resilience?',
        '[{"text": "A) Increase EC2 instance size for faster processing", "isCorrect": false}, {"text": "B) Use SQS queue between API and processing with Auto Scaling based on queue depth", "isCorrect": true}, {"text": "C) Add more API Gateway throttling to reduce incoming requests", "isCorrect": false}, {"text": "D) Use Lambda functions instead of EC2 for processing", "isCorrect": false}]',
        'B) Use SQS queue between API and processing with Auto Scaling based on queue depth',
        'SQS decouples components allowing the API to accept requests even when processing is slow, with Auto Scaling adding capacity based on queue backlog.',
        '{"summary": "SQS decoupling benefits:", "breakdown": ["API remains responsive during processing delays", "Queue acts as buffer for variable processing times", "Auto Scaling responds to actual demand (queue depth)", "No lost requests during traffic spikes"], "otherOptions": "A) Larger instances don''t solve architecture coupling\nC) Throttling causes request failures\nD) Lambda has 15-minute timeout limit for processing"}'),
       (117, 17, 'AWS Performance - Auto Scaling', 'Expert', 'Domain 2: Design High-Performing Architectures',
        'An application experiences sudden traffic spikes that cause Auto Scaling to launch instances, but by the time instances are ready, traffic has decreased. This causes unnecessary costs. Which scaling strategy provides the BEST optimization?',
        '[{"text": "A) Use predictive scaling with machine learning to anticipate traffic patterns", "isCorrect": true}, {"text": "B) Decrease Auto Scaling cooldown periods for faster scaling", "isCorrect": false}, {"text": "C) Use larger instance types that can handle traffic spikes", "isCorrect": false}, {"text": "D) Implement step scaling with smaller scaling increments", "isCorrect": false}]',
        'A) Use predictive scaling with machine learning to anticipate traffic patterns',
        'Predictive scaling uses machine learning to forecast traffic and pre-scale capacity, avoiding the lag time of reactive scaling.',
        '{"summary": "Predictive scaling advantages:", "breakdown": ["ML forecasts traffic patterns to pre-scale capacity", "Reduces lag time between traffic spike and available capacity", "Prevents over-scaling by understanding traffic duration", "Works with historical patterns and scheduled events"], "otherOptions": "B) Faster scaling still reactive, doesn''t solve core timing issue\nC) Over-provisioning increases costs unnecessarily\nD) Step scaling still reactive to traffic changes"}'),
       (118, 18, 'AWS Security - Secrets Management', 'Application', 'Domain 3: Design Secure Applications',
        'A microservices application needs to securely store and rotate database passwords, API keys, and certificates. The solution must integrate with RDS automatic rotation and provide audit trails. Which service combination is MOST appropriate?',
        '[{"text": "A) Store secrets in S3 with bucket encryption and versioning", "isCorrect": false}, {"text": "B) Use AWS Secrets Manager with CloudTrail logging", "isCorrect": true}, {"text": "C) Store secrets in Systems Manager Parameter Store with SecureString", "isCorrect": false}, {"text": "D) Use AWS KMS to encrypt secrets stored in DynamoDB", "isCorrect": false}]',
        'B) Use AWS Secrets Manager with CloudTrail logging',
        'Secrets Manager provides automatic rotation for RDS, integration with AWS services, and complete audit trails through CloudTrail.',
        '{"summary": "Secrets Manager comprehensive benefits:", "breakdown": ["Automatic rotation for RDS, Redshift, DocumentDB", "Native integration with AWS services", "Complete audit trail through CloudTrail", "Secure retrieval with fine-grained IAM permissions"], "otherOptions": "A) S3 not designed for secrets management\nC) Parameter Store lacks automatic rotation for databases\nD) Custom solution requires building rotation logic"}'),
       (119, 19, 'AWS Architecture - Service Limits', 'Expert', 'Domain 2: Design High-Performing Architectures',
        'An application needs to upload files up to 500GB to S3. Users report upload failures and timeouts. The application currently uses single PUT requests. Which solution addresses the root cause of the upload failures?',
        '[{"text": "A) Enable S3 Transfer Acceleration for faster uploads", "isCorrect": false}, {"text": "B) Implement S3 multipart upload for large files", "isCorrect": true}, {"text": "C) Use CloudFront for upload optimization", "isCorrect": false}, {"text": "D) Increase application timeout settings", "isCorrect": false}]',
        'B) Implement S3 multipart upload for large files',
        'S3 has a 5GB limit for single PUT operations. Files larger than 5GB require multipart upload, which is recommended for files over 100MB.',
        '{"summary": "S3 upload limits and best practices:", "breakdown": ["Single PUT limit: 5GB maximum file size", "Multipart upload: Required for files >5GB, recommended >100MB", "500GB files: Must use multipart upload", "Improves reliability with retry capability per part"], "otherOptions": "A) Transfer Acceleration helps speed, doesn''t solve size limit\nC) CloudFront not designed for large file uploads\nD) Timeouts don''t solve the 5GB PUT limit"}'),
       (120, 20, 'AWS Architecture - Lambda Limitations', 'Expert', 'Domain 1: Design Resilient Architectures',
        'A data processing function needs to run for 20 minutes to process large datasets. The current Lambda function times out after 15 minutes. Which solution provides the BEST architecture for this requirement?',
        '[{"text": "A) Increase Lambda timeout to maximum 15 minutes and optimize code", "isCorrect": false}, {"text": "B) Break processing into smaller Lambda functions using Step Functions", "isCorrect": true}, {"text": "C) Use Lambda with SQS to process data in smaller chunks", "isCorrect": false}, {"text": "D) Migrate to ECS Fargate for longer-running tasks", "isCorrect": false}]',
        'B) Break processing into smaller Lambda functions using Step Functions',
        'Lambda has a hard 15-minute timeout limit. Step Functions can orchestrate multiple Lambda functions to handle longer workflows.',
        '{"summary": "Lambda timeout limits and solutions:", "breakdown": ["Lambda maximum timeout: 15 minutes (hard limit)", "Step Functions: Can orchestrate multi-step workflows", "Break down processing: Multiple Lambda functions in sequence", "State management: Step Functions handles workflow state"], "otherOptions": "A) 15 minutes is the maximum timeout\nC) SQS helps with async processing but doesn''t solve timeout\nD) ECS Fargate valid but Step Functions more serverless-native"}'),
       (121, 21, 'AWS Cost Optimization - Compute Savings', 'Expert', 'Domain 4: Design Cost-Optimized Architectures',
        'A company runs a web application with the following pattern: 2 instances minimum 24/7, scales to 10 instances during business hours (9 AM-5 PM weekdays), and spikes to 50 instances during monthly sales (first 3 days). Current monthly On-Demand costs are $15,000. Which purchasing strategy provides MAXIMUM cost savings while maintaining performance?',
        '[{"text": "A) All-Upfront Reserved Instances for 10 instances, On-Demand for the rest", "isCorrect": false}, {"text": "B) Reserved Instances for 2 instances, Savings Plans for business hours capacity, Spot for sales spikes", "isCorrect": true}, {"text": "C) Compute Savings Plans for all capacity needs", "isCorrect": false}, {"text": "D) Scheduled Reserved Instances for business hours, On-Demand for spikes", "isCorrect": false}]',
        'B) Reserved Instances for 2 instances, Savings Plans for business hours capacity, Spot for sales spikes',
        'Layered purchasing strategy: RIs for baseline (70% savings), Savings Plans for predictable scaling (66% savings), Spot for spikes (up to 90% savings).',
        '{"summary": "Optimal cost strategy for variable workloads:", "breakdown": ["Reserved Instances: 2 baseline instances (24/7) - 70% savings", "Savings Plans: 8 additional instances for business hours - 66% savings", "Spot Instances: 40 instances for monthly sales - up to 90% savings", "Total savings: Approximately 75% reduction from $15,000"], "otherOptions": "A) Over-provisioning RIs for 10 instances wastes money nights/weekends\nC) Savings Plans less discount than RIs for baseline\nD) Scheduled RIs discontinued, wouldn''t cover spikes"}'),
       (122, 22, 'AWS Cost Optimization - Data Processing', 'Application',
        'Domain 4: Design Cost-Optimized Architectures',
        'A data analytics company processes 10TB of log files daily using EMR clusters. Processing takes 4 hours and runs once daily at 2 AM. The current approach uses On-Demand instances costing $8,000/month. Which architecture change provides the BEST cost optimization?',
        '[{"text": "A) Migrate to AWS Glue for serverless ETL processing", "isCorrect": false}, {"text": "B) Use EMR on Spot Instances with diverse instance types across multiple AZs", "isCorrect": true}, {"text": "C) Pre-process data with Lambda before EMR to reduce cluster size", "isCorrect": false}, {"text": "D) Use smaller EMR cluster running 24/7 with Reserved Instances", "isCorrect": false}]',
        'B) Use EMR on Spot Instances with diverse instance types across multiple AZs',
        'EMR on Spot Instances can provide 50-80% cost savings for batch processing workloads that can tolerate interruptions.',
        '{"summary": "EMR Spot Instance optimization:", "breakdown": ["Spot savings: 50-80% reduction from On-Demand pricing", "Instance diversity: Reduces interruption risk", "Multiple AZs: Ensures capacity availability", "4-hour processing window: Perfect for Spot reliability"], "otherOptions": "A) Glue more expensive for 10TB daily processing\nC) Lambda timeout and cost limitations for TB-scale data\nD) 24/7 cluster wasteful for 4-hour daily job"}'),
       (123, 23, 'AWS Cost Optimization - Storage Optimization', 'Expert',
        'Domain 4: Design Cost-Optimized Architectures',
        'A media company stores 500TB of video content in S3 Standard. Analytics show: 10% accessed daily (hot), 30% accessed weekly (warm), 60% accessed rarely but unpredictably. Retrieval must complete within 12 hours. Current cost is $11,500/month. Which storage strategy provides optimal cost reduction?',
        '[{"text": "A) S3 Intelligent-Tiering for all content with archive configuration", "isCorrect": true}, {"text": "B) S3 Standard for hot, S3 IA for warm, S3 Glacier for cold data", "isCorrect": false}, {"text": "C) S3 One Zone-IA for all content to reduce costs", "isCorrect": false}, {"text": "D) S3 Standard for hot, S3 Glacier Instant Retrieval for all other content", "isCorrect": false}]',
        'A) S3 Intelligent-Tiering for all content with archive configuration',
        'S3 Intelligent-Tiering automatically optimizes storage costs for unpredictable access patterns without retrieval fees, perfect for this use case.',
        '{"summary": "Intelligent-Tiering benefits for unpredictable access:", "breakdown": ["Automatic tiering: No manual lifecycle policies needed", "No retrieval fees: Critical for unpredictable access", "Archive configuration: Moves rarely accessed objects to archive tiers", "Cost savings: Up to 70% reduction while maintaining performance"], "otherOptions": "B) Manual lifecycle rules don''t handle unpredictable access well\nC) One Zone-IA risky for 500TB of valuable content\nD) Glacier Instant Retrieval has retrieval fees for 60% of content"}'),
       (124, 24, 'AWS Security - ML Data Protection', 'Application', 'Domain 3: Design Secure Applications',
        'A healthcare company uses Amazon Comprehend Medical to analyze patient records and Amazon Textract for insurance form processing. They must ensure HIPAA compliance and data isolation. Which security configuration is MOST appropriate?',
        '[{"text": "A) Use default service encryption and enable CloudTrail logging", "isCorrect": false}, {"text": "B) Enable VPC endpoints, use customer-managed KMS keys, and create separate IAM roles per service", "isCorrect": true}, {"text": "C) Process all data in a single private subnet with network ACLs", "isCorrect": false}, {"text": "D) Use AWS managed keys and restrict access with S3 bucket policies", "isCorrect": false}]',
        'B) Enable VPC endpoints, use customer-managed KMS keys, and create separate IAM roles per service',
        'VPC endpoints ensure private connectivity, customer-managed KMS keys provide encryption control, and separate IAM roles enable least privilege access for HIPAA compliance.',
        '{"summary": "HIPAA-compliant ML service configuration:", "breakdown": ["VPC endpoints: Keep data traffic within AWS network", "Customer-managed KMS: Full control over encryption keys", "Separate IAM roles: Least privilege per service", "Audit trail: CloudTrail logs all API calls for compliance"], "otherOptions": "A) Default encryption insufficient for HIPAA requirements\nC) Network ACLs don''t provide service-level isolation\nD) AWS managed keys don''t provide sufficient control"}'),
       (125, 25, 'AWS Security - Container Security', 'Expert', 'Domain 3: Design Secure Applications',
        'A fintech application runs on EKS with sensitive payment processing workloads. The security team requires pod-level network isolation, secrets rotation every 30 days, and runtime threat detection. Which solution meets ALL requirements?',
        '[{"text": "A) Calico network policies, AWS Secrets Manager, and Amazon Inspector", "isCorrect": false}, {"text": "B) AWS App Mesh, Systems Manager Parameter Store, and CloudWatch Container Insights", "isCorrect": false}, {"text": "C) EKS security groups for pods, AWS Secrets Manager with rotation, and Amazon GuardDuty EKS protection", "isCorrect": true}, {"text": "D) Network ACLs, HashiCorp Vault, and AWS Config rules", "isCorrect": false}]',
        'C) EKS security groups for pods, AWS Secrets Manager with rotation, and Amazon GuardDuty EKS protection',
        'EKS security groups provide pod-level isolation, Secrets Manager handles automatic rotation, and GuardDuty EKS protection offers runtime threat detection.',
        '{"summary": "Comprehensive EKS security architecture:", "breakdown": ["Security groups for pods: Fine-grained network isolation at pod level", "Secrets Manager: Automatic 30-day rotation with EKS integration", "GuardDuty EKS: Runtime threat detection and anomaly detection", "Native AWS integration: Simplified management and compliance"], "otherOptions": "A) Inspector scans vulnerabilities, not runtime threats\nB) App Mesh is service mesh, not security focused\nD) Network ACLs too coarse for pod-level control"}'),
       (126, 26, 'AWS Architecture - Event-Driven Resilience', 'Expert', 'Domain 1: Design Resilient Architectures',
        'An e-commerce platform must process orders from multiple channels (web, mobile, partners) with different formats. The system must handle 50,000 orders/hour during sales, ensure no order loss, and enable new channel integration without affecting existing ones. Which architecture provides the BEST resilience and scalability?',
        '[{"text": "A) API Gateway with Lambda functions writing directly to DynamoDB", "isCorrect": false}, {"text": "B) Amazon EventBridge with channel-specific rules routing to SQS queues and containerized processors", "isCorrect": true}, {"text": "C) Kinesis Data Streams with Lambda consumers for each channel", "isCorrect": false}, {"text": "D) Application Load Balancer with Auto Scaling EC2 instances", "isCorrect": false}]',
        'B) Amazon EventBridge with channel-specific rules routing to SQS queues and containerized processors',
        'EventBridge provides event routing with schema registry, SQS ensures message durability, and containerized processors enable independent scaling per channel.',
        '{"summary": "Event-driven architecture benefits:", "breakdown": ["EventBridge: Central event router with content-based filtering", "Schema Registry: Validates events and enables discovery", "SQS queues: Guaranteed message delivery and buffering", "Container isolation: Each channel processor scales independently"], "otherOptions": "A) Direct writes risk data loss during high load\nC) Kinesis overkill for transactional data, complex scaling\nD) Monolithic approach, difficult to add new channels"}'),
       (127, 27, 'AWS Architecture - Disaster Recovery', 'Application', 'Domain 1: Design Resilient Architectures',
        'A financial services application requires RPO of 1 hour and RTO of 4 hours. The application uses Aurora PostgreSQL, EC2 instances with custom AMIs, and stores documents in S3. The DR solution must be cost-effective. Which approach meets these requirements?',
        '[{"text": "A) Aurora Global Database with standby EC2 instances in secondary region", "isCorrect": false}, {"text": "B) Aurora backups to secondary region, AMIs copied cross-region, S3 cross-region replication, and AWS Backup", "isCorrect": true}, {"text": "C) Multi-region active-active deployment with Route 53 failover", "isCorrect": false}, {"text": "D) Database snapshots and EC2 snapshots copied hourly to secondary region", "isCorrect": false}]',
        'B) Aurora backups to secondary region, AMIs copied cross-region, S3 cross-region replication, and AWS Backup',
        'Backup-based DR meets RPO/RTO requirements cost-effectively: Aurora automated backups (continuous), AMIs ready for quick launch, S3 CRR for documents.',
        '{"summary": "Cost-effective DR strategy components:", "breakdown": ["Aurora backups: Point-in-time recovery within 5 minutes (meets 1-hour RPO)", "Cross-region AMI copies: Ready for quick EC2 launch", "S3 CRR: Near real-time replication for documents", "AWS Backup: Centralized backup management and compliance"], "otherOptions": "A) Global Database expensive for 4-hour RTO requirement\nC) Active-active overkill and complex for these requirements\nD) Manual snapshots miss continuous data changes"}'),
       (128, 28, 'AWS Performance - API Optimization', 'Application', 'Domain 2: Design High-Performing Architectures',
        'A mobile app backend serves personalized content to 10 million users globally. The API Gateway shows high latency for data aggregation from multiple microservices. Each request makes 5-7 service calls with 200ms average latency per call. Which solution provides the BEST performance improvement?',
        '[{"text": "A) Implement API Gateway caching for all endpoints", "isCorrect": false}, {"text": "B) Use AWS AppSync with GraphQL to batch and parallelize service calls", "isCorrect": true}, {"text": "C) Add ElastiCache in front of each microservice", "isCorrect": false}, {"text": "D) Increase Lambda memory allocation for faster processing", "isCorrect": false}]',
        'B) Use AWS AppSync with GraphQL to batch and parallelize service calls',
        'AppSync with GraphQL reduces API calls through query batching and parallel resolution, dramatically reducing overall latency for aggregated data.',
        '{"summary": "AppSync optimization benefits:", "breakdown": ["Query batching: Single request instead of 5-7 calls", "Parallel resolution: Service calls execute simultaneously", "Reduced latency: From 1000-1400ms to 200-300ms total", "Caching built-in: Further performance improvements"], "otherOptions": "A) Caching doesn''t help with personalized content\nC) Still requires serial service calls\nD) Lambda memory doesn''t solve aggregation latency"}'),
       (129, 29, 'AWS Performance - ML Inference', 'Expert', 'Domain 2: Design High-Performing Architectures',
        'A video streaming platform needs real-time content moderation using computer vision ML models. They process 1,000 concurrent streams with <100ms inference latency requirement. Current SageMaker endpoint shows 300ms latency. Which optimization provides the required performance?',
        '[{"text": "A) Use larger SageMaker instance types with more GPUs", "isCorrect": false}, {"text": "B) Deploy models to Lambda with Provisioned Concurrency", "isCorrect": false}, {"text": "C) Implement SageMaker multi-model endpoints with edge deployment using AWS IoT Greengrass", "isCorrect": false}, {"text": "D) Use Amazon EC2 Inf1 instances with AWS Neuron for optimized inference", "isCorrect": true}]',
        'D) Use Amazon EC2 Inf1 instances with AWS Neuron for optimized inference',
        'EC2 Inf1 instances with AWS Inferentia chips provide the lowest latency and highest throughput for real-time ML inference at scale.',
        '{"summary": "Inf1 instance optimization benefits:", "breakdown": ["AWS Inferentia chips: Purpose-built for ML inference", "Sub-100ms latency: Achievable with optimized models", "High throughput: Handle 1,000 concurrent streams", "Cost-effective: Up to 70% lower cost than GPU instances"], "otherOptions": "A) Larger instances don''t guarantee latency reduction\nB) Lambda cold starts and limits prevent consistent <100ms\nC) Edge deployment adds complexity without meeting core requirement"}'),
       (130, 30, 'AWS Cost Optimization - Hybrid Architecture', 'Expert',
        'Domain 4: Design Cost-Optimized Architectures',
        'A company wants to modernize their on-premises data warehouse (50TB) to AWS. They need to maintain some on-premises reporting tools while enabling cloud analytics. Budget is limited to $5,000/month. Query patterns: 20% hot data (daily), 30% warm (weekly), 50% cold (monthly). Which architecture provides the BEST cost optimization?',
        '[{"text": "A) Migrate everything to Redshift with Reserved Instances", "isCorrect": false}, {"text": "B) Use Redshift for hot data, Redshift Spectrum for warm/cold data in S3, with AWS Direct Connect", "isCorrect": true}, {"text": "C) Keep all data on-premises and use Athena federated queries", "isCorrect": false}, {"text": "D) Use RDS PostgreSQL with read replicas for all data", "isCorrect": false}]',
        'B) Use Redshift for hot data, Redshift Spectrum for warm/cold data in S3, with AWS Direct Connect',
        'Tiered approach with Redshift for hot data and Spectrum for S3-based warm/cold data provides optimal price-performance within budget constraints.',
        '{"summary": "Hybrid data warehouse optimization:", "breakdown": ["Redshift (10TB): ~$2,500/month for hot data performance", "S3 + Spectrum (40TB): ~$1,000/month for warm/cold storage", "Direct Connect: ~$1,500/month for reliable hybrid connectivity", "Total: ~$5,000/month within budget with optimal performance"], "otherOptions": "A) Full Redshift for 50TB exceeds budget significantly\nC) Athena federated queries too slow for hot data needs\nD) RDS not optimized for analytics workloads, would exceed budget"}'),
       (131, 31, 'AWS Cost Optimization - Serverless Economics', 'Expert',
        'Domain 4: Design Cost-Optimized Architectures',
        'A news aggregation service processes 50 million articles monthly with unpredictable spikes during breaking news (up to 10x normal load). Current EC2-based architecture costs $12,000/month and struggles with spike handling. Articles average 5KB, require text analysis, and must be searchable within 1 minute. Which serverless architecture provides the best cost optimization while meeting requirements?',
        '[{"text": "A) API Gateway → Lambda → DynamoDB with ElasticSearch", "isCorrect": false}, {"text": "B) EventBridge → SQS → Lambda with concurrent execution limits → S3 → Athena with Glue crawlers", "isCorrect": true}, {"text": "C) Kinesis Data Streams → Kinesis Analytics → RDS", "isCorrect": false}, {"text": "D) Step Functions orchestrating multiple Lambda functions → Aurora Serverless", "isCorrect": false}]',
        'B) EventBridge → SQS → Lambda with concurrent execution limits → S3 → Athena with Glue crawlers',
        'EventBridge with SQS provides event buffering for spikes, Lambda with concurrency limits controls costs, S3 offers cheap storage, and Athena enables SQL searches without database costs.',
        '{"summary": "Serverless architecture cost optimization:", "breakdown": ["EventBridge + SQS: Handles 10x spikes without over-provisioning", "Lambda concurrency limits: Prevents runaway costs during spikes", "S3 storage: $0.023/GB vs database storage at $0.10/GB", "Athena queries: Pay-per-query ($5/TB scanned) vs always-on database"], "otherOptions": "A) ElasticSearch cluster costs $3000+/month minimum\nC) Kinesis Analytics expensive for sporadic spikes\nD) Aurora Serverless still has minimum capacity costs"}'),
       (132, 32, 'AWS Cost Optimization - Container Right-Sizing', 'Application',
        'Domain 4: Design Cost-Optimized Architectures',
        'A microservices application runs 40 containers on ECS with varying resource needs: 10 containers need 4 vCPU/8GB (API servers), 20 need 1 vCPU/2GB (workers), and 10 need 0.5 vCPU/1GB (sidecars). Current costs using m5.4xlarge instances are $8,000/month with 35% CPU utilization. Which optimization strategy provides maximum cost savings?',
        '[{"text": "A) Switch to larger instances for better CPU:memory ratio", "isCorrect": false}, {"text": "B) Use ECS capacity providers with mixed instance types and Spot for workers", "isCorrect": true}, {"text": "C) Migrate all containers to Fargate with right-sized task definitions", "isCorrect": false}, {"text": "D) Implement cluster auto-scaling based on CPU utilization", "isCorrect": false}]',
        'B) Use ECS capacity providers with mixed instance types and Spot for workers',
        'Capacity providers enable mixing instance types optimized for different workloads, while Spot instances for stateless workers can reduce costs by 70-90%.',
        '{"summary": "Container cost optimization strategy:", "breakdown": ["Mixed instances: c5 for CPU-intensive, r5 for memory-intensive", "Spot for workers: 70% savings on 20 containers (stateless)", "Better bin packing: Increases utilization from 35% to 75%", "Total savings: Approximately 60% reduction from $8,000"], "otherOptions": "A) Larger instances worsen utilization problems\nC) Fargate more expensive for predictable workloads\nD) Auto-scaling doesn''t address instance type mismatch"}'),
       (133, 33, 'AWS ML Services Integration', 'Application', 'Domain 3: Design Secure Applications',
        'A social media platform needs to detect inappropriate content in user posts (text and images) in real-time. They process 1 million posts daily and must comply with COPPA regulations for users under 13. Which combination of AWS services provides comprehensive content moderation with compliance controls?',
        '[{"text": "A) Amazon Comprehend for text and custom SageMaker model for images", "isCorrect": false}, {"text": "B) Amazon Rekognition Content Moderation, Amazon Comprehend toxicity detection, with Lambda age-gating logic", "isCorrect": true}, {"text": "C) Amazon Textract for text extraction and Amazon Personalize for content filtering", "isCorrect": false}, {"text": "D) AWS Glue for data preparation and Amazon Forecast for predicting inappropriate content", "isCorrect": false}]',
        'B) Amazon Rekognition Content Moderation, Amazon Comprehend toxicity detection, with Lambda age-gating logic',
        'Rekognition provides pre-trained image moderation, Comprehend detects toxic text, and Lambda implements age-appropriate filtering logic for COPPA compliance.',
        '{"summary": "Content moderation architecture components:", "breakdown": ["Rekognition: Detects inappropriate images (violence, adult content)", "Comprehend: Identifies toxic text, PII, and sentiment", "Lambda age-gating: Applies stricter rules for users under 13", "Real-time processing: Sub-second moderation decisions"], "otherOptions": "A) Custom models require training data and maintenance\nC) Textract is for document processing, not moderation\nD) Forecast predicts time-series data, not content issues"}'),
       (134, 34, 'AWS Architecture - IoT and Edge', 'Expert', 'Domain 1: Design Resilient Architectures',
        'A manufacturing company operates 500 factories with 100 IoT sensors each, generating 1KB messages every second. They need local anomaly detection with sub-second response, cloud-based analytics, and must operate during internet outages. Which architecture provides the required edge computing capabilities?',
        '[{"text": "A) Direct IoT Core ingestion with Kinesis Analytics for anomaly detection", "isCorrect": false}, {"text": "B) AWS IoT Greengrass with Lambda functions, local message routing, and intermittent cloud sync", "isCorrect": true}, {"text": "C) EC2 instances in each factory with VPN connections to cloud", "isCorrect": false}, {"text": "D) AWS Outposts in each factory for local processing", "isCorrect": false}]',
        'B) AWS IoT Greengrass with Lambda functions, local message routing, and intermittent cloud sync',
        'IoT Greengrass enables local Lambda execution for sub-second anomaly detection, continues operating offline, and synchronizes with cloud when connected.',
        '{"summary": "Edge computing with IoT Greengrass benefits:", "breakdown": ["Local Lambda: Sub-second anomaly detection at edge", "Offline operation: Continues processing during outages", "Efficient sync: Batches data for cloud analytics when connected", "Cost-effective: No need for servers in 500 locations"], "otherOptions": "A) Requires constant internet, no local processing\nC) Managing 500 EC2 instances operationally complex\nD) Outposts overkill and expensive for IoT workloads"}'),
       (135, 35, 'AWS Cost Optimization - Streaming Data', 'Application',
        'Domain 4: Design Cost-Optimized Architectures',
        'A ride-sharing app ingests 100,000 location updates per second during peak hours (6-9 AM, 5-8 PM) but only 5,000 per second during off-peak. Current Kinesis Data Streams with provisioned shards costs $15,000/month. Real-time processing is required only during peak hours; batch processing is acceptable off-peak. Which architecture reduces costs while meeting requirements?',
        '[{"text": "A) Kinesis Data Streams with auto-scaling shards based on traffic", "isCorrect": false}, {"text": "B) Kinesis Data Streams On-Demand for peaks, Kinesis Firehose to S3 for off-peak batch", "isCorrect": true}, {"text": "C) Replace with SQS queues and Lambda functions", "isCorrect": false}, {"text": "D) DynamoDB Streams with Lambda triggers", "isCorrect": false}]',
        'B) Kinesis Data Streams On-Demand for peaks, Kinesis Firehose to S3 for off-peak batch',
        'Kinesis On-Demand eliminates over-provisioning costs while Firehose to S3 provides cost-effective batch processing during off-peak hours.',
        '{"summary": "Streaming cost optimization strategy:", "breakdown": ["Kinesis On-Demand: Pay only for actual throughput", "Peak hours: Real-time processing as required", "Off-peak Firehose: 90% cheaper than real-time streams", "Estimated savings: 65% reduction from $15,000/month"], "otherOptions": "A) Auto-scaling still requires minimum shards\nC) SQS lacks streaming semantics for real-time\nD) DynamoDB Streams tied to table operations"}'),
       (136, 36, 'AWS Architecture - Multi-Region', 'Expert', 'Domain 1: Design Resilient Architectures',
        'A global trading platform requires <10ms latency for 99% of users across US, EU, and APAC regions. The platform processes 1 million trades daily with strict consistency requirements. Each trade must be reflected globally within 100ms. Current single-region deployment shows 150ms+ latency for distant users. Which multi-region architecture best meets these requirements?',
        '[{"text": "A) Deploy full stack in each region with eventual consistency", "isCorrect": false}, {"text": "B) Aurora Global Database with local read replicas, Route 53 geolocation routing, and DynamoDB Global Tables for session data", "isCorrect": true}, {"text": "C) Single region with CloudFront caching for static content", "isCorrect": false}, {"text": "D) Active-active regions with asynchronous replication", "isCorrect": false}]',
        'B) Aurora Global Database with local read replicas, Route 53 geolocation routing, and DynamoDB Global Tables for session data',
        'Aurora Global Database provides <1 second replication with strong consistency, local read replicas ensure <10ms reads, while DynamoDB Global Tables handle session state with automatic conflict resolution.',
        '{"summary": "Multi-region architecture for low latency trading:", "breakdown": ["Aurora Global: <1 second cross-region replication", "Local read replicas: <10ms read latency per region", "Write forwarding: Maintains consistency for trades", "DynamoDB Global Tables: Multi-master for session data"], "otherOptions": "A) Eventual consistency violates trade requirements\nC) CloudFront doesn''t help with dynamic trade data\nD) Async replication can''t guarantee 100ms consistency"}'),
       (137, 37, 'AWS Security - Zero Trust Architecture', 'Expert', 'Domain 3: Design Secure Applications',
        'A financial services company needs to implement zero-trust access to their AWS environment for 500 developers across 50 teams. Requirements include: no long-lived credentials, audit trail of all actions, team-based access boundaries, and integration with existing Okta SSO. Which solution provides the most comprehensive zero-trust implementation?',
        '[{"text": "A) IAM users with MFA and IP restrictions for each developer", "isCorrect": false}, {"text": "B) AWS SSO with Okta integration, Permission Sets with session tags, and CloudTrail with EventBridge rules", "isCorrect": true}, {"text": "C) Cognito user pools with custom authorizers", "isCorrect": false}, {"text": "D) IAM roles with external ID and session policies", "isCorrect": false}]',
        'B) AWS SSO with Okta integration, Permission Sets with session tags, and CloudTrail with EventBridge rules',
        'AWS SSO provides temporary credentials, Okta integration enables SAML-based authentication, Permission Sets with session tags enforce team boundaries, while CloudTrail ensures complete audit trails.',
        '{"summary": "Zero-trust implementation components:", "breakdown": ["AWS SSO: No long-lived credentials, automatic rotation", "Permission Sets: Team-based access with session tags", "Okta SAML: Leverages existing identity provider", "CloudTrail + EventBridge: Real-time audit and alerting"], "otherOptions": "A) IAM users require long-lived access keys\nC) Cognito designed for application users, not AWS access\nD) Missing centralized management for 500 developers"}'),
       (138, 38, 'AWS Performance - Hybrid Networking', 'Application', 'Domain 2: Design High-Performing Architectures',
        'A media company needs to transfer 50TB of daily video content from on-premises editing stations to S3 for processing. Current internet upload takes 20 hours, causing production delays. The solution must complete transfers within 4 hours and support 100 concurrent editor workstations. Which approach best meets these performance requirements?',
        '[{"text": "A) Multiple Site-to-Site VPN connections with ECMP", "isCorrect": false}, {"text": "B) AWS Direct Connect with Virtual Interfaces and S3 Transfer Acceleration", "isCorrect": true}, {"text": "C) Storage Gateway File Gateway with local cache", "isCorrect": false}, {"text": "D) Daily AWS Snowball Edge shipments", "isCorrect": false}]',
        'B) AWS Direct Connect with Virtual Interfaces and S3 Transfer Acceleration',
        'Direct Connect provides dedicated bandwidth for consistent transfer speeds, VIFs enable private S3 connectivity, and Transfer Acceleration optimizes the upload path for concurrent workstations.',
        '{"summary": "High-performance hybrid transfer solution:", "breakdown": ["Direct Connect: 10Gbps dedicated bandwidth", "VIF to S3: Private connectivity, no internet congestion", "Transfer Acceleration: Optimizes for 100 concurrent uploads", "50TB in 4 hours: Requires ~3.5Gbps sustained throughput"], "otherOptions": "A) VPN limited to ~1.25Gbps aggregate bandwidth\nC) File Gateway adds latency and cache limitations\nD) Snowball shipping time exceeds 4-hour requirement"}'),
       (139, 39, 'AWS Architecture - Event-Driven Serverless', 'Expert',
        'Domain 2: Design High-Performing Architectures',
        'An e-commerce platform needs to process order events with multiple downstream systems: inventory (must process once), shipping (at-least-once), analytics (can tolerate duplicates), and email (exactly-once). Order volume varies from 100/minute to 10,000/minute during flash sales. Which event-driven architecture ensures correct delivery semantics for each consumer?',
        '[{"text": "A) SNS with SQS queues for all consumers", "isCorrect": false}, {"text": "B) EventBridge with rules routing to SQS FIFO for inventory, standard SQS for shipping, Kinesis for analytics, and Step Functions for email", "isCorrect": true}, {"text": "C) Kinesis Data Streams with Lambda consumers", "isCorrect": false}, {"text": "D) Direct Lambda invocations with error handling", "isCorrect": false}]',
        'B) EventBridge with rules routing to SQS FIFO for inventory, standard SQS for shipping, Kinesis for analytics, and Step Functions for email',
        'EventBridge provides content-based routing, SQS FIFO ensures exactly-once processing, standard SQS handles at-least-once, Kinesis supports analytics replay, and Step Functions manages email workflow state.',
        '{"summary": "Event delivery semantics per consumer:", "breakdown": ["SQS FIFO: Exactly-once for critical inventory updates", "Standard SQS: At-least-once with retry for shipping", "Kinesis: Replay capability for analytics reprocessing", "Step Functions: Manages email state to prevent duplicates"], "otherOptions": "A) SNS+SQS doesn''t provide exactly-once semantics\nC) Kinesis requires all consumers to process in order\nD) Direct invocations lack delivery guarantees"}'),
       (140, 40, 'AWS Cost Optimization - Modern Architectures', 'Expert',
        'Domain 4: Design Cost-Optimized Architectures',
        'A SaaS company runs 200 customer environments, each with identical architecture: ALB, 2-10 EC2 instances, RDS MySQL, and 50GB of S3 storage. Monthly costs are $400,000 with only 30% resource utilization. Customers require isolation for compliance. Which modernization approach provides maximum cost reduction while maintaining isolation?',
        '[{"text": "A) Containerize applications and run all customers on shared EKS cluster", "isCorrect": false}, {"text": "B) AWS App Runner with customer-specific environments, Aurora Serverless v2, and S3 bucket policies", "isCorrect": true}, {"text": "C) Lambda functions with RDS Proxy and separate VPCs", "isCorrect": false}, {"text": "D) Consolidate all customers into a single multi-tenant application", "isCorrect": false}]',
        'B) AWS App Runner with customer-specific environments, Aurora Serverless v2, and S3 bucket policies',
        'App Runner provides serverless compute with environment isolation, Aurora Serverless v2 scales database resources based on actual usage, eliminating overprovisioning while maintaining compliance isolation.',
        '{"summary": "Serverless multi-tenant cost optimization:", "breakdown": ["App Runner: Pay-per-request pricing, no idle EC2 costs", "Aurora Serverless v2: Scales down to 0.5 ACU when idle", "Environment isolation: Separate App Runner services per customer", "Estimated savings: 70% reduction through utilization-based pricing"], "otherOptions": "A) Shared cluster violates compliance isolation requirements\nC) Lambda cold starts problematic for web applications\nD) Multi-tenant architecture breaks compliance requirements"}'),
       (141, 41, 'AWS Storage Concepts', 'Knowledge', 'Domain 3: Design High-Performing Architectures',
        'A company needs to choose between different AWS storage types for various workloads. Which statement BEST describes the appropriate use cases for each storage type?',
        '[{"text": "A) Use object storage for databases, block storage for file shares, file storage for web content", "isCorrect": false}, {"text": "B) Use block storage for databases, file storage for shared access, object storage for web content", "isCorrect": true}, {"text": "C) Use file storage for databases, object storage for shared access, block storage for web content", "isCorrect": false}, {"text": "D) All storage types can be used interchangeably for any workload", "isCorrect": false}]',
        'B) Use block storage for databases, file storage for shared access, object storage for web content',
        'Block storage (EBS) provides high IOPS for databases, file storage (EFS) enables shared access, object storage (S3) scales for web content.',
        '{"summary": "AWS storage type characteristics:", "breakdown": ["Block storage (EBS): High IOPS, low latency, ideal for databases and boot volumes", "File storage (EFS): POSIX-compliant, shared access, good for content repositories", "Object storage (S3): Auto-scaling, REST API, perfect for web content and backups", "Each type optimized for specific access patterns and performance requirements"], "otherOptions": "A) Object storage not suitable for database IOPS requirements\nC) File storage lacks the IOPS performance needed for databases\nD) Each storage type has specific use cases and limitations"}'),
       (142, 42, 'AWS Storage Concepts', 'Application', 'Domain 2: Design Resilient Architectures',
        'An enterprise is migrating from on-premises and needs to maintain compatibility with existing storage protocols. They use NFS for Unix systems, SMB for Windows, and iSCSI for SAN storage. Which AWS services provide the BEST protocol compatibility?',
        '[{"text": "A) S3 for all protocols using third-party gateways", "isCorrect": false}, {"text": "B) EFS for NFS, FSx for Windows for SMB, Storage Gateway for iSCSI", "isCorrect": true}, {"text": "C) EBS for all protocols using EC2 instance configuration", "isCorrect": false}, {"text": "D) DataSync for protocol translation across all storage types", "isCorrect": false}]',
        'B) EFS for NFS, FSx for Windows for SMB, Storage Gateway for iSCSI',
        'AWS provides native protocol support: EFS (NFS), FSx for Windows (SMB), Storage Gateway (iSCSI) for seamless migration.',
        '{"summary": "Native AWS protocol support:", "breakdown": ["EFS: Native NFS v4.1, POSIX-compliant, Linux/Unix compatibility", "FSx for Windows: Native SMB, Active Directory integration", "Storage Gateway: iSCSI interface, hybrid cloud connectivity", "No application changes required for protocol compatibility"], "otherOptions": "A) S3 doesn''t natively support NFS/SMB/iSCSI protocols\nC) EBS provides block storage but not protocol-level compatibility\nD) DataSync is for data transfer, not protocol translation"}'),
       (143, 43, 'AWS S3 Deep Dive', 'Application', 'Domain 2: Design Resilient Architectures',
        'A media company requires 99.999999999% (11 9s) data durability for their video assets. They need to understand the difference between S3 durability and availability for their SLA reporting. Which statement is CORRECT about S3 durability vs availability?',
        '[{"text": "A) Durability and availability are identical metrics across all S3 storage classes", "isCorrect": false}, {"text": "B) Durability (11 9s) represents data loss protection, while availability varies by storage class", "isCorrect": true}, {"text": "C) Availability is always higher than durability in S3", "isCorrect": false}, {"text": "D) Only S3 Standard provides 11 9s durability", "isCorrect": false}]',
        'B) Durability (11 9s) represents data loss protection, while availability varies by storage class',
        'S3 provides 11 9s durability (data loss protection) across ALL storage classes, but availability (uptime) varies by class.',
        '{"summary": "S3 durability vs availability:", "breakdown": ["Durability (11 9s): Probability of NOT losing data over a year", "Availability varies: Standard (99.99%), IA (99.9%), Glacier (variable)", "All storage classes: Same durability guarantee", "Durability: Data integrity protection through redundancy"], "otherOptions": "A) Different metrics with different SLA percentages\nC) Availability percentages are typically lower than durability\nD) All S3 storage classes maintain 11 9s durability"}'),
       (144, 44, 'AWS S3 Deep Dive', 'Application', 'Domain 1: Design Secure Architectures',
        'A healthcare organization stores patient records in S3 and needs to maintain full control over encryption keys while ensuring HIPAA compliance. They want to minimize key management overhead but need complete audit trails. Which encryption approach provides the BEST balance?',
        '[{"text": "A) SSE-S3 with AWS-managed keys", "isCorrect": false}, {"text": "B) SSE-KMS with customer-managed keys", "isCorrect": true}, {"text": "C) SSE-C with customer-provided keys", "isCorrect": false}, {"text": "D) Client-side encryption with S3 Encryption Client", "isCorrect": false}]',
        'B) SSE-KMS with customer-managed keys',
        'SSE-KMS with customer-managed keys provides control over keys, audit trails via CloudTrail, and AWS handles key management infrastructure.',
        '{"summary": "SSE-KMS benefits for HIPAA compliance:", "breakdown": ["Customer control: Create, rotate, and manage encryption keys", "Audit trails: CloudTrail logs all key usage and access", "AWS management: Infrastructure and availability handled by AWS", "HIPAA compliant: Meets regulatory requirements for key control"], "otherOptions": "A) AWS-managed keys don''t provide sufficient customer control\nC) SSE-C requires managing key storage and availability\nD) Client-side encryption adds complexity without AWS integration benefits"}'),
       (145, 45, 'AWS S3 Deep Dive', 'Expert', 'Domain 4: Design Cost-Optimized Architectures',
        'A content delivery platform stores 200TB of video files with unpredictable access patterns. Analytics show some videos remain popular for months while others are rarely accessed after the first week. They need cost optimization without performance penalties for popular content. Which S3 storage strategy provides optimal cost savings?',
        '[{"text": "A) Lifecycle policy transitioning all content to S3 IA after 30 days", "isCorrect": false}, {"text": "B) S3 Intelligent-Tiering with Deep Archive Access tier enabled", "isCorrect": true}, {"text": "C) Keep popular content in Standard, move others to Glacier based on view counts", "isCorrect": false}, {"text": "D) Use S3 One Zone-IA for all content to reduce costs", "isCorrect": false}]',
        'B) S3 Intelligent-Tiering with Deep Archive Access tier enabled',
        'S3 Intelligent-Tiering automatically optimizes costs based on access patterns without retrieval fees for frequent/infrequent tiers, perfect for unpredictable patterns.',
        '{"summary": "Intelligent-Tiering benefits for unpredictable access:", "breakdown": ["Automatic optimization: No manual lifecycle policies needed", "No retrieval fees: For frequent and infrequent access tiers", "Deep Archive tier: Additional savings for rarely accessed content", "Performance maintained: Popular content stays in frequent tier"], "otherOptions": "A) Fixed lifecycle doesn''t handle unpredictable patterns\nC) Manual management based on view counts is complex and error-prone\nD) One Zone-IA lacks resilience for valuable content"}'),
       (146, 46, 'AWS EBS Deep Dive', 'Application', 'Domain 3: Design High-Performing Architectures',
        'A database application requires consistent 20,000 IOPS with low latency for transaction processing. The database size is 8TB and growing. Which EBS volume type and configuration provides the BEST performance while supporting future growth?',
        '[{"text": "A) gp3 volumes with 20,000 provisioned IOPS", "isCorrect": false}, {"text": "B) io2 Block Express volumes with 20,000 provisioned IOPS", "isCorrect": true}, {"text": "C) gp2 volumes in RAID 0 configuration", "isCorrect": false}, {"text": "D) st1 throughput optimized volumes for cost savings", "isCorrect": false}]',
        'B) io2 Block Express volumes with 20,000 provisioned IOPS',
        'io2 Block Express provides consistent IOPS up to 256,000, supports volumes up to 64TB, and offers sub-millisecond latency for databases.',
        '{"summary": "io2 Block Express advantages for databases:", "breakdown": ["Consistent IOPS: 20,000 IOPS guaranteed regardless of volume size", "Sub-millisecond latency: Optimal for transaction processing", "Scalability: Supports up to 64TB volumes for future growth", "Durability: 99.999% annual durability vs 99.9% for gp3"], "otherOptions": "A) gp3 IOPS can vary with volume size and burst credits\nC) RAID 0 increases failure risk and management complexity\nD) st1 optimized for throughput, not IOPS-intensive workloads"}'),
       (147, 47, 'AWS EBS Deep Dive', 'Application', 'Domain 2: Design Resilient Architectures',
        'A production database uses EBS volumes and requires point-in-time recovery capability with minimal data loss. The current snapshot strategy creates daily snapshots, but the RPO requirement is 4 hours. Which enhancement provides the BEST backup strategy improvement?',
        '[{"text": "A) Increase snapshot frequency to every 4 hours using scheduled snapshots", "isCorrect": true}, {"text": "B) Enable EBS Fast Snapshot Restore on existing daily snapshots", "isCorrect": false}, {"text": "C) Configure EBS Multi-Attach to create live replicas", "isCorrect": false}, {"text": "D) Use AWS Backup service with continuous backup enabled", "isCorrect": false}]',
        'A) Increase snapshot frequency to every 4 hours using scheduled snapshots',
        'Scheduling snapshots every 4 hours directly meets the RPO requirement by ensuring maximum data loss is limited to 4 hours.',
        '{"summary": "Snapshot strategy for 4-hour RPO:", "breakdown": ["Scheduled snapshots: Every 4 hours meets exact RPO requirement", "Incremental backups: Only changed blocks stored, cost-effective", "Point-in-time recovery: Can restore to any snapshot timestamp", "Automated lifecycle: Can retain snapshots based on retention policy"], "otherOptions": "B) Fast Snapshot Restore improves RTO, not RPO\nC) Multi-Attach doesn''t provide backup functionality\nD) Continuous backup overkill for 4-hour RPO requirement"}'),
       (148, 48, 'AWS EBS Deep Dive', 'Expert', 'Domain 1: Design Secure Architectures',
        'A financial application stores sensitive data on EBS volumes and requires encryption in transit and at rest. The application accesses data across multiple AZs and needs to ensure encryption keys remain under customer control with automatic rotation. Which configuration provides comprehensive encryption coverage?',
        '[{"text": "A) EBS encryption with AWS managed keys and HTTPS application protocols", "isCorrect": false}, {"text": "B) EBS encryption with customer-managed KMS keys, automatic rotation enabled, and application-level TLS", "isCorrect": true}, {"text": "C) Instance store volumes with client-side encryption", "isCorrect": false}, {"text": "D) EBS encryption with customer-provided keys (SSE-C)", "isCorrect": false}]',
        'B) EBS encryption with customer-managed KMS keys, automatic rotation enabled, and application-level TLS',
        'Customer-managed KMS keys provide control with automatic rotation, EBS encryption handles data at rest, and TLS provides encryption in transit.',
        '{"summary": "Comprehensive EBS encryption strategy:", "breakdown": ["Customer-managed KMS keys: Full control over encryption keys", "Automatic rotation: Annual key rotation without service interruption", "EBS encryption: All data at rest encrypted including snapshots", "Application TLS: Encrypts data in transit between instances"], "otherOptions": "A) AWS managed keys don''t provide customer control\nC) Instance store data is temporary, not suitable for persistent financial data\nD) Customer-provided keys require manual key management"}'),
       (149, 49, 'AWS File Systems Deep Dive', 'Application', 'Domain 3: Design High-Performing Architectures',
        'A video editing company needs shared storage accessible from 50 EC2 instances across multiple AZs for collaborative editing projects. Files range from 1GB to 50GB, and editors require low-latency access. Performance needs vary from 1 GB/s during off-hours to 10 GB/s during production. Which file system solution provides the BEST performance and scalability?',
        '[{"text": "A) Amazon EFS with General Purpose performance mode", "isCorrect": false}, {"text": "B) Amazon EFS with Max I/O performance mode and Provisioned Throughput", "isCorrect": true}, {"text": "C) Amazon FSx for Lustre with scratch file system", "isCorrect": false}, {"text": "D) Multiple EBS volumes shared using NFS on EC2", "isCorrect": false}]',
        'B) Amazon EFS with Max I/O performance mode and Provisioned Throughput',
        'EFS Max I/O mode supports higher IOPS for concurrent access, while Provisioned Throughput ensures consistent 10 GB/s performance regardless of file system size.',
        '{"summary": "EFS configuration for video editing workloads:", "breakdown": ["Max I/O mode: Higher IOPS limit for 50 concurrent clients", "Provisioned Throughput: Guarantees 10 GB/s during peak production", "Multi-AZ access: Native support across availability zones", "POSIX compliance: Standard file system semantics for editing software"], "otherOptions": "A) General Purpose mode has lower IOPS limits for concurrent access\nC) Lustre scratch file system designed for HPC, not collaborative editing\nD) EBS volumes can''t be natively shared between instances"}'),
       (150, 50, 'AWS File Systems Deep Dive', 'Expert', 'Domain 2: Design High-Performing Architectures',
        'A Windows-based application requires high-performance file storage with Active Directory integration, SMB protocol support, and sub-millisecond latencies. The workload involves intensive random I/O operations on small files. Current on-premises Windows File Server achieves 100,000 IOPS. Which AWS solution provides equivalent performance?',
        '[{"text": "A) Amazon FSx for Windows File Server with SSD storage", "isCorrect": true}, {"text": "B) Amazon EFS with Windows file gateway", "isCorrect": false}, {"text": "C) EC2 Windows instance with attached io2 EBS volumes", "isCorrect": false}, {"text": "D) Amazon FSx for Lustre with Windows connectivity", "isCorrect": false}]',
        'A) Amazon FSx for Windows File Server with SSD storage',
        'FSx for Windows File Server provides native SMB, Active Directory integration, SSD storage for high IOPS, and sub-millisecond latencies specifically optimized for Windows workloads.',
        '{"summary": "FSx for Windows File Server benefits:", "breakdown": ["Native SMB: Full Windows file system compatibility", "Active Directory: Seamless user authentication and authorization", "SSD storage: Up to 100,000+ IOPS for random I/O workloads", "Sub-millisecond latency: Optimized for Windows application performance"], "otherOptions": "B) EFS doesn''t natively support SMB or Active Directory\nC) Self-managed solution lacks native SMB optimization\nD) FSx for Lustre designed for Linux HPC workloads, not Windows"}'),
       (151, 51, 'AWS Hybrid Storage Migration', 'Application', 'Domain 1: Design Resilient Architectures',
        'A company needs to migrate 500TB of archival data from on-premises tape storage to AWS. The data is rarely accessed but must be retrievable within 12 hours when needed. The migration must complete within 3 months with minimal impact on existing network bandwidth. Which migration strategy is MOST efficient?',
        '[{"text": "A) AWS DataSync over Direct Connect for gradual migration", "isCorrect": false}, {"text": "B) Multiple AWS Snowball Edge devices shipped in batches to S3 Glacier Deep Archive", "isCorrect": true}, {"text": "C) Storage Gateway Tape Gateway for gradual cloud migration", "isCorrect": false}, {"text": "D) AWS Transfer Family for secure file transfer", "isCorrect": false}]',
        'B) Multiple AWS Snowball Edge devices shipped in batches to S3 Glacier Deep Archive',
        'Snowball Edge devices handle large data volumes efficiently without network impact, while Glacier Deep Archive provides cost-effective storage with 12-hour retrieval.',
        '{"summary": "Large-scale archival migration strategy:", "breakdown": ["Snowball Edge capacity: 80TB per device, 7 devices for 500TB", "No network impact: Physical data transfer bypasses internet", "Glacier Deep Archive: Lowest cost storage for rarely accessed data", "12-hour retrieval: Meets business requirement for archive access"], "otherOptions": "A) DataSync over network would take months and impact bandwidth\nC) Tape Gateway for ongoing hybrid, not one-time migration\nD) Transfer Family not optimized for 500TB bulk migration"}'),
       (152, 52, 'AWS Hybrid Storage Migration', 'Expert', 'Domain 3: Design High-Performing Architectures',
        'A media company operates a hybrid architecture where video editors work on-premises but need seamless access to cloud storage for rendering jobs. Local performance must remain unaffected while providing transparent cloud storage scalability. Frequently accessed files should be available locally. Which hybrid storage solution provides the BEST user experience?',
        '[{"text": "A) AWS Storage Gateway File Gateway with local cache", "isCorrect": true}, {"text": "B) AWS DataSync scheduled sync between on-premises and S3", "isCorrect": false}, {"text": "C) Direct Connect with EFS mounted on workstations", "isCorrect": false}, {"text": "D) S3 bucket mounted using third-party NFS gateway", "isCorrect": false}]',
        'A) AWS Storage Gateway File Gateway with local cache',
        'Storage Gateway File Gateway provides NFS interface with intelligent local caching, transparent S3 integration, and optimal performance for frequently accessed files.',
        '{"summary": "File Gateway hybrid benefits:", "breakdown": ["Local cache: Frequently accessed files available at local speeds", "Transparent scaling: Files automatically stored in S3", "NFS interface: Standard file system access for editing applications", "Intelligent caching: Recently and frequently used files cached locally"], "otherOptions": "B) DataSync requires scheduled sync, not transparent access\nC) EFS over Direct Connect has latency for large video files\nD) Third-party solutions lack AWS service integration and support"}'),
       (153, 53, 'AWS Data Security Concepts', 'Application', 'Domain 1: Design Secure Architectures',
        'A financial services company needs to automatically discover and protect sensitive data across 200 S3 buckets containing customer documents. They require automated classification, policy enforcement, and immediate alerts for data exposure risks. Which combination provides comprehensive data security automation?',
        '[{"text": "A) AWS Config rules for bucket policies and CloudWatch for monitoring", "isCorrect": false}, {"text": "B) Amazon Macie for discovery and classification, EventBridge for alerts, and S3 Block Public Access", "isCorrect": true}, {"text": "C) AWS Inspector for vulnerability scanning and GuardDuty for threats", "isCorrect": false}, {"text": "D) Custom Lambda functions with CloudTrail for access monitoring", "isCorrect": false}]',
        'B) Amazon Macie for discovery and classification, EventBridge for alerts, and S3 Block Public Access',
        'Macie uses ML to discover sensitive data, provides automatic classification, EventBridge enables real-time alerts, and Block Public Access prevents exposure.',
        '{"summary": "Automated data security components:", "breakdown": ["Amazon Macie: ML-powered sensitive data discovery and classification", "EventBridge integration: Real-time alerts for policy violations", "S3 Block Public Access: Prevents accidental public exposure", "Automated findings: Continuous monitoring across all 200 buckets"], "otherOptions": "A) Config and CloudWatch don''t provide sensitive data discovery\nC) Inspector and GuardDuty focus on infrastructure, not data classification\nD) Custom solutions require significant development and maintenance"}'),
       (154, 54, 'AWS Data Security Concepts', 'Expert', 'Domain 1: Design Secure Architectures',
        'A healthcare organization must implement data classification and protection for patient records across multiple AWS services (S3, RDS, DynamoDB). They need automated discovery of PHI, policy-based access controls, and audit trails meeting HIPAA requirements. Which architecture provides comprehensive data protection?',
        '[{"text": "A) Amazon Macie for S3, database activity streams for RDS, and VPC Flow Logs", "isCorrect": false}, {"text": "B) Amazon Macie for S3, AWS CloudTrail data events, and customer-managed KMS keys across all services", "isCorrect": true}, {"text": "C) AWS Config for compliance monitoring and GuardDuty for threat detection", "isCorrect": false}, {"text": "D) Custom data loss prevention (DLP) solution with third-party tools", "isCorrect": false}]',
        'B) Amazon Macie for S3, AWS CloudTrail data events, and customer-managed KMS keys across all services',
        'Macie discovers PHI in S3, CloudTrail data events provide comprehensive audit trails, and customer-managed KMS keys ensure encryption control across all services.',
        '{"summary": "HIPAA-compliant data protection architecture:", "breakdown": ["Macie: Automated PHI discovery and classification in S3", "CloudTrail data events: Complete audit trail of data access", "Customer-managed KMS: Encryption control across S3, RDS, DynamoDB", "Policy-based access: IAM policies with resource-based conditions"], "otherOptions": "A) Missing encryption control and comprehensive audit coverage\nC) Config and GuardDuty don''t provide PHI discovery and classification\nD) Custom solutions require extensive compliance validation"}'),
       (155, 55, 'AWS Encryption', 'Expert', 'Domain 1: Design Secure Architectures',
        'A multinational corporation needs end-to-end encryption for data stored across AWS services in multiple regions. They require customer control over encryption keys, automatic rotation, cross-region access, and integration with their existing HSM infrastructure. Which encryption strategy provides the MOST comprehensive solution?',
        '[{"text": "A) AWS KMS customer-managed keys with automatic rotation in each region", "isCorrect": false}, {"text": "B) AWS CloudHSM cluster with custom key management application", "isCorrect": true}, {"text": "C) Client-side encryption with application-managed keys", "isCorrect": false}, {"text": "D) AWS KMS with imported key material from on-premises HSM", "isCorrect": false}]',
        'B) AWS CloudHSM cluster with custom key management application',
        'CloudHSM provides dedicated hardware security modules with full customer control, integrates with existing HSM infrastructure, and supports cross-region key management.',
        '{"summary": "CloudHSM comprehensive encryption benefits:", "breakdown": ["Dedicated HSM: Full customer control over encryption operations", "FIPS 140-2 Level 3: Highest security certification available", "Cross-region clustering: Keys available across multiple regions", "HSM integration: Compatible with existing on-premises HSM infrastructure"], "otherOptions": "A) KMS keys are region-specific and don''t integrate with existing HSM\nC) Application-managed keys lack HSM security and are difficult to manage at scale\nD) Imported key material in KMS doesn''t provide full HSM integration"}'),
       (156, 56, 'AWS Encryption', 'Application', 'Domain 1: Design Secure Architectures',
        'A development team needs to encrypt application data before storing it in DynamoDB. They want to minimize performance impact while ensuring field-level encryption for sensitive columns (SSN, credit card numbers) but allow searching on encrypted email addresses. Which encryption approach provides the BEST balance of security and functionality?',
        '[{"text": "A) DynamoDB encryption at rest with AWS managed keys", "isCorrect": false}, {"text": "B) AWS DynamoDB Encryption Client with deterministic encryption for emails, probabilistic for sensitive fields", "isCorrect": true}, {"text": "C) Application-level AES encryption for all fields", "isCorrect": false}, {"text": "D) AWS KMS encryption for entire DynamoDB table", "isCorrect": false}]',
        'B) AWS DynamoDB Encryption Client with deterministic encryption for emails, probabilistic for sensitive fields',
        'DynamoDB Encryption Client provides field-level encryption with deterministic encryption for searchable fields and probabilistic encryption for maximum security on sensitive data.',
        '{"summary": "Field-level encryption strategy:", "breakdown": ["Deterministic encryption: Same email always produces same ciphertext (searchable)", "Probabilistic encryption: Different ciphertext each time (maximum security)", "Client-side encryption: Data encrypted before sending to DynamoDB", "Performance optimized: Only sensitive fields encrypted, not entire records"], "otherOptions": "A) Table-level encryption doesn''t provide field-level control\nC) Application encryption without library optimization impacts performance\nD) KMS table encryption encrypts all data, doesn''t enable selective searching"}'),
       (157, 57, 'AWS Governance Compliance', 'Expert', 'Domain 1: Design Secure Architectures',
        'A regulated financial institution must demonstrate continuous compliance with SOC 2 requirements for data storage. They need automated policy enforcement, compliance reporting, and evidence collection across 50 AWS accounts. Non-compliant resources must be automatically remediated. Which governance framework provides comprehensive compliance automation?',
        '[{"text": "A) AWS Config with custom rules and SNS notifications", "isCorrect": false}, {"text": "B) AWS Organizations with SCPs, Config Conformance Packs, and Systems Manager Automation", "isCorrect": true}, {"text": "C) AWS CloudFormation with compliance templates", "isCorrect": false}, {"text": "D) AWS Security Hub with manual remediation workflows", "isCorrect": false}]',
        'B) AWS Organizations with SCPs, Config Conformance Packs, and Systems Manager Automation',
        'Organizations provides account governance, Conformance Packs enable SOC 2 compliance checks, and Systems Manager Automation handles remediation across all accounts.',
        '{"summary": "Comprehensive compliance automation:", "breakdown": ["Organizations + SCPs: Account-level policy enforcement", "Config Conformance Packs: Pre-built SOC 2 compliance rules", "Systems Manager Automation: Automatic remediation of non-compliant resources", "Centralized reporting: Compliance status across all 50 accounts"], "otherOptions": "A) Config alone lacks account-level governance and automated remediation\nC) CloudFormation provides deployment compliance, not ongoing monitoring\nD) Security Hub requires manual remediation, not automated"}'),
       (158, 58, 'AWS Governance Compliance', 'Application', 'Domain 1: Design Secure Architectures',
        'A global company must ensure S3 buckets across all regions comply with data residency requirements. EU data must stay in EU regions, US data in US regions, with automatic policy enforcement and compliance reporting. Which solution provides the MOST effective governance?',
        '[{"text": "A) IAM policies restricting S3 actions based on user location", "isCorrect": false}, {"text": "B) S3 bucket policies with aws:RequestedRegion condition keys", "isCorrect": false}, {"text": "C) AWS Organizations SCPs with region restrictions based on data classification tags", "isCorrect": true}, {"text": "D) AWS Config rules monitoring bucket creation across regions", "isCorrect": false}]',
        'C) AWS Organizations SCPs with region restrictions based on data classification tags',
        'Organizations SCPs provide account-level enforcement that cannot be overridden, with tag-based conditions enabling automatic data residency compliance.',
        '{"summary": "Data residency governance with SCPs:", "breakdown": ["SCPs: Cannot be overridden by account-level permissions", "Tag-based conditions: Automatically enforce based on data classification", "Region restrictions: Prevent EU data creation in non-EU regions", "Organization-wide: Applies to all current and future accounts"], "otherOptions": "A) IAM policies can be overridden by account administrators\nB) Bucket policies only apply after bucket creation\nD) Config rules detect violations but don''t prevent them"}'),
       (159, 59, 'AWS Storage Resiliency Scalability', 'Expert', 'Domain 2: Design Resilient Architectures',
        'A video streaming platform stores 1PB of content with global distribution requirements. They need 99.99% availability, automatic failover, and the ability to handle 10x traffic spikes during major events. Current single-region S3 architecture shows latency issues for international users. Which architecture provides optimal resiliency and global performance?',
        '[{"text": "A) S3 Cross-Region Replication with CloudFront distribution", "isCorrect": true}, {"text": "B) Multi-region S3 buckets with Route 53 latency-based routing", "isCorrect": false}, {"text": "C) S3 Transfer Acceleration with single-region storage", "isCorrect": false}, {"text": "D) EFS with VPC peering across regions", "isCorrect": false}]',
        'A) S3 Cross-Region Replication with CloudFront distribution',
        'S3 CRR provides data resilience and regional availability, while CloudFront edge locations ensure global low-latency access and can handle traffic spikes automatically.',
        '{"summary": "Global content delivery architecture:", "breakdown": ["S3 CRR: Automatic replication across regions for resilience", "CloudFront: 400+ edge locations for global low-latency delivery", "Auto-scaling: CloudFront handles 10x traffic spikes automatically", "99.99% availability: S3 + CloudFront combined SLA exceeds requirement"], "otherOptions": "B) Route 53 doesn''t provide edge caching for content delivery\nC) Transfer Acceleration only helps uploads, not global content delivery\nD) EFS not designed for 1PB content delivery workloads"}'),
       (160, 60, 'AWS Storage Resiliency Scalability', 'Application', 'Domain 2: Design Resilient Architectures',
        'An e-commerce platform experiences seasonal traffic patterns with 50x increase during holiday sales. Their product catalog and user-generated content storage must scale automatically without performance degradation. Current EBS-based storage architecture requires manual intervention during peaks. Which storage architecture provides automatic scalability?',
        '[{"text": "A) Auto Scaling EBS volumes with CloudWatch metrics", "isCorrect": false}, {"text": "B) Amazon S3 with CloudFront for content delivery and Lambda for processing", "isCorrect": true}, {"text": "C) Amazon EFS with Provisioned Throughput mode", "isCorrect": false}, {"text": "D) Multiple EBS volumes in RAID configuration", "isCorrect": false}]',
        'B) Amazon S3 with CloudFront for content delivery and Lambda for processing',
        'S3 provides unlimited automatic scaling, CloudFront handles traffic spikes through edge caching, and Lambda scales processing automatically with demand.',
        '{"summary": "Auto-scaling storage architecture:", "breakdown": ["S3 unlimited scaling: Handles any amount of data and requests", "CloudFront edge caching: Reduces origin load during traffic spikes", "Lambda auto-scaling: Processing scales from 0 to thousands of concurrent executions", "No manual intervention: All components scale automatically"], "otherOptions": "A) EBS volumes cannot auto-scale capacity\nC) EFS requires pre-provisioned throughput planning\nD) RAID configuration still requires manual capacity planning"}'),
       (161, 61, 'AWS Data Lifecycle', 'Application', 'Domain 4: Design Cost-Optimized Architectures',
        'A research institution stores genomics data with the following access pattern: analyzed intensively for 30 days, occasionally referenced for 6 months, and archived for 10 years for compliance. Current costs are $50,000/month in S3 Standard. Which lifecycle policy provides maximum cost optimization while meeting access requirements?',
        '[{"text": "A) Standard for 30 days → IA at 30 days → Glacier at 180 days → Deep Archive at 1 year", "isCorrect": true}, {"text": "B) Intelligent-Tiering for all data with Deep Archive enabled", "isCorrect": false}, {"text": "C) Standard for 30 days → Glacier immediately at 30 days", "isCorrect": false}, {"text": "D) One Zone-IA for all data to reduce costs", "isCorrect": false}]',
        'A) Standard for 30 days → IA at 30 days → Glacier at 180 days → Deep Archive at 1 year',
        'Lifecycle transitions match access patterns: Standard for intensive analysis, IA for occasional access, Glacier for long-term storage, Deep Archive for compliance.',
        '{"summary": "Optimized lifecycle transitions for research data:", "breakdown": ["Days 1-30: S3 Standard for intensive analysis", "Days 31-180: S3 IA for occasional reference (68% cost reduction)", "Days 181-365: S3 Glacier for backup storage (77% cost reduction)", "Years 1-10: Deep Archive for compliance (80% cost reduction)"], "otherOptions": "B) Intelligent-Tiering adds overhead for predictable access patterns\nC) Early Glacier transition makes occasional access expensive\nD) One Zone-IA lacks resilience for valuable research data"}'),
       (162, 62, 'AWS Data Lifecycle', 'Expert', 'Domain 4: Design Cost-Optimized Architectures',
        'A media company has complex data lifecycle needs: news content (hot for 7 days, cold afterwards), evergreen content (unpredictable access), and archived footage (rare access but immediate retrieval when needed). They want automated optimization without management overhead. Which strategy handles all three patterns optimally?',
        '[{"text": "A) Separate buckets with different lifecycle policies for each content type", "isCorrect": false}, {"text": "B) S3 Intelligent-Tiering with object tagging and tag-based lifecycle policies", "isCorrect": true}, {"text": "C) Manual lifecycle management based on content metadata", "isCorrect": false}, {"text": "D) Single lifecycle policy with shortest transition times", "isCorrect": false}]',
        'B) S3 Intelligent-Tiering with object tagging and tag-based lifecycle policies',
        'Intelligent-Tiering with object tags enables different optimization strategies per content type while maintaining automation and handling unpredictable access patterns.',
        '{"summary": "Tag-based intelligent lifecycle management:", "breakdown": ["Object tagging: Classify content types (news, evergreen, archive)", "Intelligent-Tiering: Automatic optimization for unpredictable evergreen content", "Tag-based policies: Specific rules for news (7-day pattern) and archive content", "No management overhead: All transitions automated based on access patterns"], "otherOptions": "A) Multiple buckets increase management complexity\nC) Manual management doesn''t scale and introduces errors\nD) Single policy can''t optimize for different access patterns"}'),
       (163, 63, 'AWS Storage Cost Management', 'Expert', 'Domain 4: Design Cost-Optimized Architectures',
        'A data analytics company spends $100,000/month on storage across S3, EBS, and EFS for various workloads. They need comprehensive cost optimization while maintaining performance. Analysis shows: 40% rarely accessed, 30% predictable patterns, 20% unpredictable access, 10% high-performance needs. Which multi-service optimization strategy provides maximum savings?',
        '[{"text": "A) Move all data to cheapest storage classes regardless of access patterns", "isCorrect": false}, {"text": "B) S3 lifecycle policies for rarely accessed, Intelligent-Tiering for unpredictable, gp3 optimization for EBS, EFS IA for infrequent file access", "isCorrect": true}, {"text": "C) Consolidate all storage to S3 with single lifecycle policy", "isCorrect": false}, {"text": "D) Implement Reserved Capacity for all storage services", "isCorrect": false}]',
        'B) S3 lifecycle policies for rarely accessed, Intelligent-Tiering for unpredictable, gp3 optimization for EBS, EFS IA for infrequent file access',
        'Tailored optimization per access pattern: lifecycle for predictable patterns, Intelligent-Tiering for unpredictable, gp3 for cost-effective EBS performance, EFS IA for infrequent access.',
        '{"summary": "Multi-service cost optimization strategy:", "breakdown": ["40% rarely accessed: S3 lifecycle to Glacier/Deep Archive (75% savings)", "20% unpredictable: S3 Intelligent-Tiering (40% average savings)", "EBS optimization: gp3 volumes with right-sized IOPS (20% savings)", "EFS optimization: IA storage class for infrequent access (85% savings)"], "otherOptions": "A) Ignoring access patterns can cause performance issues and retrieval costs\nC) Not all workloads suit object storage architecture\nD) Reserved Capacity not available for all storage types and may not match usage patterns"}'),
       (164, 64, 'AWS Storage Cost Management', 'Application', 'Domain 4: Design Cost-Optimized Architectures',
        'A startup''s S3 storage costs have grown to $25,000/month with 80% of objects never accessed after 90 days. They also have high data transfer costs from direct client downloads. The application serves user-uploaded images and documents globally. Which combination provides the most comprehensive cost reduction?',
        '[{"text": "A) S3 lifecycle policy to Glacier and CloudFront distribution", "isCorrect": true}, {"text": "B) S3 Intelligent-Tiering and S3 Transfer Acceleration", "isCorrect": false}, {"text": "C) Move all data to S3 One Zone-IA with direct client access", "isCorrect": false}, {"text": "D) Compress all objects and implement client-side caching", "isCorrect": false}]',
        'A) S3 lifecycle policy to Glacier and CloudFront distribution',
        'Lifecycle policy to Glacier reduces storage costs by 68% for unused objects, while CloudFront eliminates data transfer costs and provides global acceleration.',
        '{"summary": "Comprehensive S3 cost optimization:", "breakdown": ["Lifecycle to Glacier: 68% storage cost reduction for 80% of objects", "CloudFront: Eliminates data transfer costs from S3 to internet", "Global caching: Improves performance while reducing origin costs", "Combined savings: Approximately 70% total cost reduction"], "otherOptions": "B) Intelligent-Tiering adds overhead for predictable 90-day pattern\nC) One Zone-IA lacks resilience and doesn''t address transfer costs\nD) Compression helps but doesn''t address core storage lifecycle and transfer costs"}'),
       (165, 65, 'AWS Storage Comprehensive', 'Expert', 'Domain 3: Design High-Performing Architectures',
        'A large enterprise needs a comprehensive storage strategy for their cloud migration. Requirements include: 500TB database storage with 50,000 IOPS, 2PB file shares for 1000 users, 10PB object storage with global access, and hybrid connectivity to on-premises. Which architecture provides optimal performance, cost, and integration?',
        '[{"text": "A) io2 Block Express EBS, EFS Max I/O, S3 with CloudFront, and Storage Gateway", "isCorrect": true}, {"text": "B) gp3 EBS, FSx for Lustre, S3 Intelligent-Tiering, and Direct Connect", "isCorrect": false}, {"text": "C) Aurora storage, EFS General Purpose, S3 Standard, and VPN connections", "isCorrect": false}, {"text": "D) Instance store, FSx for Windows, S3 Glacier, and AWS Outposts", "isCorrect": false}]',
        'A) io2 Block Express EBS, EFS Max I/O, S3 with CloudFront, and Storage Gateway',
        'io2 Block Express provides required IOPS, EFS Max I/O handles concurrent users, S3+CloudFront scales globally, Storage Gateway enables hybrid integration.',
        '{"summary": "Enterprise storage architecture components:", "breakdown": ["io2 Block Express: 50,000 IOPS with sub-millisecond latency for databases", "EFS Max I/O: Scales to 1000+ concurrent users with higher IOPS", "S3 + CloudFront: Unlimited object storage with global low-latency access", "Storage Gateway: Seamless hybrid connectivity with caching"], "otherOptions": "B) gp3 may not provide consistent 50,000 IOPS, FSx Lustre not ideal for general file shares\nC) Aurora storage limits database choice, VPN insufficient for 500TB+ workloads\nD) Instance store temporary, FSx Windows not mentioned as requirement, Glacier too slow"}'),
       (166, 66, 'AWS Storage Comprehensive', 'Expert', 'Domain 1: Design Secure Architectures',
        'A financial services firm requires end-to-end security for their storage infrastructure handling sensitive customer data across S3, EBS, and EFS. Requirements include: customer-controlled encryption, automated compliance monitoring, data classification, and audit trails meeting regulatory standards. Which comprehensive security architecture meets all requirements?',
        '[{"text": "A) AWS managed encryption, GuardDuty monitoring, and CloudTrail logging", "isCorrect": false}, {"text": "B) Customer-managed KMS keys, Amazon Macie, Config Conformance Packs, and CloudTrail data events", "isCorrect": true}, {"text": "C) CloudHSM encryption, Security Hub, and VPC Flow Logs", "isCorrect": false}, {"text": "D) Client-side encryption, Inspector scanning, and AWS Config rules", "isCorrect": false}]',
        'B) Customer-managed KMS keys, Amazon Macie, Config Conformance Packs, and CloudTrail data events',
        'Customer-managed KMS provides encryption control, Macie classifies sensitive data, Conformance Packs ensure compliance, CloudTrail data events provide complete audit trails.',
        '{"summary": "Comprehensive security architecture for financial services:", "breakdown": ["Customer-managed KMS: Full control over encryption keys across all storage services", "Amazon Macie: Automated discovery and classification of sensitive financial data", "Config Conformance Packs: Continuous compliance monitoring for financial regulations", "CloudTrail data events: Complete audit trail of all data access activities"], "otherOptions": "A) AWS managed encryption lacks customer control required for financial services\nC) CloudHSM overkill, Security Hub doesn''t provide data classification\nD) Client-side encryption complex to manage, Inspector focuses on vulnerabilities not data classification"}');

-- Create the schema for the prepper application
-- CREATE SCHEMA IF NOT EXISTS prepper;

-- Set the search path to the prepper schema
SET
search_path TO prepper;

-- Drop the old table if it exists to apply the new schema
-- DROP TABLE IF EXISTS prepper.comptia_cloud_plus_questions;
-- DROP TABLE IF EXISTS prepper.aws_certified_architect_associate_questions;



-- Create the updated table for AWS Certified Architect Associate questions
CREATE TABLE IF NOT EXISTS prepper.aws_certified_architect_associate_questions
(
    id
    SERIAL
    PRIMARY
    KEY,
    question_id
    INTEGER
    UNIQUE
    NOT
    NULL,
    question_number
    INTEGER
    NOT
    NULL,
    category
    TEXT,
    difficulty
    TEXT,
    domain
    TEXT,
    question_text
    TEXT
    NOT
    NULL,
    options
    JSONB
    NOT
    NULL,
    correct_answer
    TEXT
    NOT
    NULL,
    explanation
    TEXT,
    explanation_details
    JSONB
);

-- Create the updated table for AWS Certified Architect Associate questions
CREATE TABLE IF NOT EXISTS prepper.Comptia_cloud_plus_questions
(
    id
    SERIAL
    PRIMARY
    KEY,
    question_id
    INTEGER
    UNIQUE
    NOT
    NULL,
    question_number
    INTEGER
    NOT
    NULL,
    category
    TEXT,
    difficulty
    TEXT,
    domain
    TEXT,
    question_text
    TEXT
    NOT
    NULL,
    options
    JSONB
    NOT
    NULL,
    correct_answer
    TEXT
    NOT
    NULL,
    explanation
    TEXT,
    explanation_details
    JSONB
);


-- Insert all AWS Certified Architect Associate questions
INSERT INTO prepper.aws_certified_architect_associate_questions (question_id, question_number, category, difficulty,
                                                                 domain, question_text, options, correct_answer,
                                                                 explanation, explanation_details)
VALUES (101, 1, 'AWS Architecture - High Availability', 'Application', 'Domain 1: Design Resilient Architectures',
        'A company runs a critical e-commerce application on a single EC2 instance in us-east-1a. The application must achieve 99.99% uptime and automatically recover from AZ failures. Which solution provides the MOST cost-effective approach to meet these requirements?',
        '[{"text": "A) Deploy a larger EC2 instance type in the same AZ", "isCorrect": false}, {"text": "B) Create an AMI and manually launch instances in other AZs when needed", "isCorrect": false}, {"text": "C) Use Auto Scaling Group with instances across multiple AZs behind an ALB", "isCorrect": true}, {"text": "D) Use Elastic Beanstalk with a single instance", "isCorrect": false}]',
        'C) Use Auto Scaling Group with instances across multiple AZs behind an ALB',
        'Auto Scaling Groups with multiple AZs provide automatic failure detection and recovery, while ALB distributes traffic and handles health checks.',
        '{"summary": "Multi-AZ Auto Scaling provides automated resilience:", "breakdown": ["Auto Scaling Group automatically replaces failed instances", "Multiple AZs protect against entire AZ failures", "ALB provides health checks and traffic distribution", "Most cost-effective as it only runs minimum required instances"], "otherOptions": "A) Larger instance doesn''t solve AZ failure\nB) Manual process doesn''t meet automation requirement\nD) Single instance still has single point of failure"}'),
       (102, 2, 'AWS Architecture - Data Resilience', 'Application', 'Domain 1: Design Resilient Architectures',
        'A financial services company stores critical transaction data in RDS MySQL. They need to ensure data can be recovered with RPO of 5 minutes and RTO of 15 minutes in case of primary database failure. Which combination of solutions meets these requirements? (Choose TWO)',
        '[{"text": "A) Enable automated backups with 5-minute backup frequency", "isCorrect": false}, {"text": "B) Configure RDS Multi-AZ deployment", "isCorrect": true}, {"text": "C) Create hourly manual snapshots", "isCorrect": false}, {"text": "D) Enable point-in-time recovery", "isCorrect": true}]',
        'B) Configure RDS Multi-AZ deployment',
        'Multi-AZ provides automatic failover within minutes, while point-in-time recovery enables restoration to any second within the backup retention period.',
        '{"summary": "Meeting RPO/RTO requirements:", "breakdown": ["Multi-AZ: Automatic failover typically completes in 60-120 seconds", "Point-in-time recovery: Can restore to any second (meets 5-min RPO)", "Automated backups: Only daily, can''t meet 5-minute RPO", "Manual snapshots: Too infrequent for RPO requirement"], "otherOptions": "A) Automated backups are daily, not every 5 minutes\nC) Hourly snapshots exceed 5-minute RPO requirement"}'),
       (103, 3, 'AWS Architecture - Cross-Region Resilience', 'Expert', 'Domain 1: Design Resilient Architectures',
        'A media company streams video content globally and needs to ensure service availability even if an entire AWS region becomes unavailable. The application uses ALB, EC2 Auto Scaling, and RDS MySQL. Which design provides the MOST comprehensive disaster recovery solution?',
        '[{"text": "A) Create read replicas in multiple regions with manual failover", "isCorrect": false}, {"text": "B) Use Route 53 health checks with cross-region ALBs and RDS cross-region automated backups", "isCorrect": false}, {"text": "C) Deploy identical infrastructure in two regions with Route 53 failover routing and RDS cross-region read replicas with promotion capability", "isCorrect": true}, {"text": "D) Use CloudFormation to deploy infrastructure in multiple regions manually when needed", "isCorrect": false}]',
        'C) Deploy identical infrastructure in two regions with Route 53 failover routing and RDS cross-region read replicas with promotion capability',
        'Full cross-region deployment with automated DNS failover and database replica promotion provides the fastest recovery from regional failures.',
        '{"summary": "Comprehensive multi-region DR strategy:", "breakdown": ["Active-passive setup in multiple regions", "Route 53 automatic failover based on health checks", "RDS read replicas can be promoted to standalone databases", "Infrastructure already running in secondary region"], "otherOptions": "A) Manual failover too slow for video streaming\nB) Cross-region backups take too long to restore\nD) Manual deployment during disaster is too slow"}'),
       (104, 4, 'AWS Storage - Backup Strategy', 'Application', 'Domain 1: Design Resilient Architectures',
        'A company needs to backup 50TB of data from on-premises to AWS. The data must be available within 4 hours during a disaster, and they want to minimize ongoing costs. The initial backup can take up to 2 weeks. Which solution is MOST appropriate?',
        '[{"text": "A) AWS Storage Gateway with Tape Gateway to S3 Glacier", "isCorrect": false}, {"text": "B) AWS Snowball Edge to S3, then lifecycle policy to S3 Intelligent-Tiering", "isCorrect": true}, {"text": "C) Direct upload to S3 Standard over internet connection", "isCorrect": false}, {"text": "D) AWS DataSync to S3 Glacier Deep Archive", "isCorrect": false}]',
        'B) AWS Snowball Edge to S3, then lifecycle policy to S3 Intelligent-Tiering',
        'Snowball Edge handles the initial 50TB transfer efficiently, while S3 Intelligent-Tiering automatically optimizes costs and meets the 4-hour recovery requirement.',
        '{"summary": "Large data backup strategy considerations:", "breakdown": ["Snowball Edge: Cost-effective for 50TB initial transfer", "S3 Intelligent-Tiering: Automatic cost optimization", "Standard tier availability: Immediate access for 4-hour RTO", "2-week initial timeline: Perfect for Snowball shipping"], "otherOptions": "A) Tape Gateway too slow for 4-hour recovery\nC) 50TB over internet too slow and expensive\nD) Glacier Deep Archive takes 12+ hours to retrieve"}'),
       (105, 5, 'AWS Performance - Database Optimization', 'Application',
        'Domain 2: Design High-Performing Architectures',
        'An application experiences slow database performance during peak hours. The RDS MySQL instance shows 95% CPU utilization, and 80% of queries are read operations. The application cannot tolerate any downtime. Which solution provides the BEST performance improvement with minimal operational overhead?',
        '[{"text": "A) Create a larger RDS instance and schedule maintenance window for upgrade", "isCorrect": false}, {"text": "B) Enable RDS Performance Insights to identify slow queries", "isCorrect": false}, {"text": "C) Create RDS read replicas and modify application to use them for read queries", "isCorrect": true}, {"text": "D) Migrate to Amazon Aurora MySQL with automatic scaling", "isCorrect": false}]',
        'C) Create RDS read replicas and modify application to use them for read queries',
        'Read replicas directly address the 80% read workload without downtime, providing immediate performance relief for read-heavy applications.',
        '{"summary": "Read replica benefits for read-heavy workloads:", "breakdown": ["80% read queries can be offloaded to replicas", "No downtime required for setup", "Immediate performance improvement", "Cost-effective solution for read scaling"], "otherOptions": "A) Instance upgrade requires downtime\nB) Performance Insights only identifies issues, doesn''t solve them\nD) Migration requires significant effort and testing"}'),
       (106, 6, 'AWS Performance - Caching Strategy', 'Expert', 'Domain 2: Design High-Performing Architectures',
        'A social media application serves user profiles stored in DynamoDB. The same profiles are accessed frequently by millions of users, causing high read costs and occasional throttling. The application requires sub-millisecond response times. Which caching strategy provides the BEST performance and cost optimization?',
        '[{"text": "A) Implement application-level caching with Redis ElastiCache", "isCorrect": false}, {"text": "B) Enable DynamoDB DAX (DynamoDB Accelerator)", "isCorrect": true}, {"text": "C) Use CloudFront to cache DynamoDB API responses", "isCorrect": false}, {"text": "D) Increase DynamoDB provisioned read capacity", "isCorrect": false}]',
        'B) Enable DynamoDB DAX (DynamoDB Accelerator)',
        'DAX provides microsecond-level response times specifically designed for DynamoDB, with automatic cache management and no application changes required.',
        '{"summary": "DAX advantages for DynamoDB caching:", "breakdown": ["Microsecond response times (sub-millisecond requirement)", "Transparent to application (no code changes)", "Automatic cache invalidation and management", "Reduces DynamoDB read costs significantly"], "otherOptions": "A) Redis requires application code changes and cache management\nC) CloudFront can''t cache DynamoDB API calls\nD) Increasing capacity doesn''t solve latency, only costs more"}'),
       (107, 7, 'AWS Performance - Content Delivery', 'Comprehension', 'Domain 2: Design High-Performing Architectures',
        'A global news website serves static content (images, videos, articles) to users worldwide. Users in Asia report slow page load times, while users in the US experience good performance. The origin servers are located in us-east-1. Which solution provides the MOST effective performance improvement for Asian users?',
        '[{"text": "A) Deploy additional EC2 instances in ap-southeast-1", "isCorrect": false}, {"text": "B) Implement CloudFront distribution with edge locations in Asia", "isCorrect": true}, {"text": "C) Use Transfer Acceleration for S3 uploads", "isCorrect": false}, {"text": "D) Create Route 53 latency-based routing to multiple regions", "isCorrect": false}]',
        'B) Implement CloudFront distribution with edge locations in Asia',
        'CloudFront edge locations cache static content closer to Asian users, dramatically reducing latency for static content delivery.',
        '{"summary": "CloudFront benefits for global content delivery:", "breakdown": ["Edge locations in Asia cache content locally", "Reduces latency from us-east-1 to Asia", "Handles static content (images, videos, articles) efficiently", "Automatic cache management and optimization"], "otherOptions": "A) Additional instances help dynamic content, not static\nC) Transfer Acceleration for uploads, not downloads\nD) Latency routing helps with dynamic content, not static"}'),
       (108, 8, 'AWS Security - IAM Policies', 'Expert', 'Domain 3: Design Secure Applications',
        'A company has multiple AWS accounts and needs to grant developers access to specific S3 buckets based on their team assignment. Developers should only access buckets prefixed with their team name (e.g., team-alpha-*). Which approach provides the MOST secure and scalable solution?',
        '[{"text": "A) Create individual IAM users in each account with specific S3 bucket policies", "isCorrect": false}, {"text": "B) Use AWS Organizations with SCPs and cross-account IAM roles with dynamic policy conditions", "isCorrect": true}, {"text": "C) Create shared IAM users with different access keys for each team", "isCorrect": false}, {"text": "D) Use S3 bucket policies with IAM user conditions for each team", "isCorrect": false}]',
        'B) Use AWS Organizations with SCPs and cross-account IAM roles with dynamic policy conditions',
        'Organizations with SCPs provide account-level governance, while cross-account roles with dynamic conditions enable secure, scalable team-based access.',
        '{"summary": "Multi-account secure access strategy:", "breakdown": ["SCPs provide organization-wide security boundaries", "Cross-account roles eliminate need for multiple IAM users", "Dynamic conditions based on user attributes (team tags)", "Centralized access management across accounts"], "otherOptions": "A) Individual users don''t scale across multiple accounts\nC) Shared users violate security best practices\nD) Bucket policies alone can''t handle multi-account scenarios efficiently"}'),
       (109, 9, 'AWS Security - Data Encryption', 'Application', 'Domain 3: Design Secure Applications',
        'A healthcare company must encrypt all data at rest and in transit. They need to maintain full control over encryption keys and meet HIPAA compliance requirements. The solution must integrate with RDS, S3, and EBS. Which key management approach is MOST appropriate?',
        '[{"text": "A) Use AWS managed keys (SSE-S3, default RDS encryption)", "isCorrect": false}, {"text": "B) Use AWS KMS customer managed keys with CloudTrail logging", "isCorrect": true}, {"text": "C) Use client-side encryption with application-managed keys", "isCorrect": false}, {"text": "D) Use AWS KMS AWS managed keys with detailed monitoring", "isCorrect": false}]',
        'B) Use AWS KMS customer managed keys with CloudTrail logging',
        'Customer managed KMS keys provide full control over key policies, rotation, and access, while CloudTrail provides complete audit trails required for HIPAA compliance.',
        '{"summary": "HIPAA-compliant key management requirements:", "breakdown": ["Customer managed keys: Full control over key policies and access", "CloudTrail: Complete audit trail of key usage", "Cross-service integration: Works with RDS, S3, EBS", "HIPAA compliance: Meets regulatory requirements for key control"], "otherOptions": "A) AWS managed keys don''t provide sufficient control for HIPAA\nC) Client-side encryption complex and doesn''t integrate well\nD) AWS managed keys still don''t give customer full control"}'),
       (110, 10, 'AWS Security - Network Security', 'Application', 'Domain 3: Design Secure Applications',
        'A three-tier web application (web, app, database) needs to be secured according to the principle of least privilege. The web tier must be accessible from the internet, app tier only from web tier, and database tier only from app tier. Which security group configuration is CORRECT?',
        '[{"text": "A) Web SG: 0.0.0.0/0:80,443 | App SG: Web-SG:8080 | DB SG: App-SG:3306", "isCorrect": true}, {"text": "B) Web SG: 0.0.0.0/0:80,443 | App SG: 0.0.0.0/0:8080 | DB SG: 0.0.0.0/0:3306", "isCorrect": false}, {"text": "C) All tiers: VPC CIDR:All ports for internal communication", "isCorrect": false}, {"text": "D) Web SG: 0.0.0.0/0:All | App SG: VPC CIDR:All | DB SG: VPC CIDR:All", "isCorrect": false}]',
        'A) Web SG: 0.0.0.0/0:80,443 | App SG: Web-SG:8080 | DB SG: App-SG:3306',
        'Security groups should reference other security groups for internal communication, implementing true least privilege access between tiers.',
        '{"summary": "Proper three-tier security group design:", "breakdown": ["Web tier: Internet access on standard web ports (80, 443)", "App tier: Only accepts traffic from web security group", "Database tier: Only accepts traffic from app security group", "Security group references: More secure than IP-based rules"], "otherOptions": "B) App and DB exposed to internet\nC) Too broad access within VPC\nD) Overly permissive on all tiers"}'),
       (111, 11, 'AWS Security - Advanced Threats', 'Expert', 'Domain 3: Design Secure Applications',
        'A company''s web application experiences DDoS attacks and SQL injection attempts. They need comprehensive protection that automatically adapts to new attack patterns. The solution must integrate with CloudFront and ALB. Which combination provides the MOST comprehensive protection?',
        '[{"text": "A) AWS Shield Standard + CloudFront + Security Groups", "isCorrect": false}, {"text": "B) AWS Shield Advanced + AWS WAF + AWS Firewall Manager", "isCorrect": true}, {"text": "C) AWS Shield Standard + AWS WAF + VPC Flow Logs", "isCorrect": false}, {"text": "D) Network ACLs + AWS Inspector + CloudWatch", "isCorrect": false}]',
        'B) AWS Shield Advanced + AWS WAF + AWS Firewall Manager',
        'Shield Advanced provides enhanced DDoS protection, WAF blocks application-layer attacks, and Firewall Manager centrally manages security policies across resources.',
        '{"summary": "Comprehensive web application protection stack:", "breakdown": ["Shield Advanced: Enhanced DDoS protection with attack response team", "WAF: Blocks SQL injection, XSS, and other application attacks", "Firewall Manager: Centralized security policy management", "Auto-adaptation: Machine learning capabilities for new threats"], "otherOptions": "A) Shield Standard insufficient for advanced DDoS\nC) VPC Flow Logs don''t provide active protection\nD) Inspector scans instances, doesn''t provide runtime protection"}'),
       (112, 12, 'AWS Cost Optimization - Instance Selection', 'Application',
        'Domain 4: Design Cost-Optimized Architectures',
        'A batch processing workload runs predictably every night from 2 AM to 6 AM for data analytics. The workload can tolerate interruptions and can resume from checkpoints. Current costs using On-Demand instances are $500/month. Which pricing model combination provides the MAXIMUM cost savings?',
        '[{"text": "A) Reserved Instances for consistent baseline capacity", "isCorrect": false}, {"text": "B) Spot Instances with Auto Scaling groups across multiple AZs", "isCorrect": true}, {"text": "C) Savings Plans for compute usage commitment", "isCorrect": false}, {"text": "D) Dedicated Hosts for workload isolation", "isCorrect": false}]',
        'B) Spot Instances with Auto Scaling groups across multiple AZs',
        'Spot Instances can provide up to 90% savings for fault-tolerant batch workloads, and multiple AZs reduce interruption risk.',
        '{"summary": "Spot Instances optimal for batch processing:", "breakdown": ["Up to 90% cost savings vs On-Demand", "Workload tolerates interruptions (checkpoint resume)", "Predictable 4-hour window reduces interruption risk", "Multiple AZs provide capacity diversification"], "otherOptions": "A) Reserved Instances only ~75% savings, overkill for 4-hour workload\nC) Savings Plans better for consistent 24/7 usage\nD) Dedicated Hosts most expensive option"}'),
       (113, 13, 'AWS Cost Optimization - Storage Lifecycle', 'Application',
        'Domain 4: Design Cost-Optimized Architectures',
        'A company stores application logs in S3. Logs are actively analyzed for 30 days, occasionally accessed for 90 days, and must be retained for 7 years for compliance. Current storage costs are $1000/month in S3 Standard. Which lifecycle policy provides the BEST cost optimization?',
        '[{"text": "A) Transition to S3 IA after 30 days, then S3 Glacier after 90 days, then Deep Archive after 1 year", "isCorrect": true}, {"text": "B) Keep all data in S3 Standard for quick access when needed", "isCorrect": false}, {"text": "C) Move everything to S3 Glacier immediately after upload", "isCorrect": false}, {"text": "D) Use S3 Intelligent-Tiering for automatic optimization", "isCorrect": false}]',
        'A) Transition to S3 IA after 30 days, then S3 Glacier after 90 days, then Deep Archive after 1 year',
        'Lifecycle policy matching access patterns: S3 Standard for active use, S3 IA for occasional access, Glacier for long-term retention, Deep Archive for compliance.',
        '{"summary": "Optimal lifecycle transitions for log data:", "breakdown": ["Days 1-30: S3 Standard for active analysis", "Days 31-90: S3 IA for occasional access (50% savings)", "Days 91-365: S3 Glacier for backup (68% savings)", "Years 1-7: Deep Archive for compliance (77% savings)"], "otherOptions": "B) Standard storage most expensive for long-term retention\nC) Glacier immediate transition prevents active analysis\nD) Intelligent-Tiering adds overhead for predictable patterns"}'),
       (114, 14, 'AWS Cost Optimization - Database Costs', 'Expert', 'Domain 4: Design Cost-Optimized Architectures',
        'A startup''s MySQL database experiences unpredictable traffic patterns ranging from 5% to 95% CPU utilization throughout the day. Current RDS costs are $800/month with frequent over-provisioning during low-traffic periods. Which solution provides the BEST cost optimization while maintaining performance?',
        '[{"text": "A) Migrate to Aurora Serverless v2 with automatic scaling", "isCorrect": true}, {"text": "B) Use RDS with scheduled Auto Scaling based on time patterns", "isCorrect": false}, {"text": "C) Switch to smaller RDS instance and add read replicas", "isCorrect": false}, {"text": "D) Migrate to DynamoDB with on-demand billing", "isCorrect": false}]',
        'A) Migrate to Aurora Serverless v2 with automatic scaling',
        'Aurora Serverless v2 automatically scales compute capacity based on actual demand, eliminating over-provisioning costs during low-traffic periods.',
        '{"summary": "Aurora Serverless v2 benefits for variable workloads:", "breakdown": ["Automatic scaling from 0.5 to 128 ACUs based on demand", "Pay only for actual compute used (per-second billing)", "No over-provisioning during low-traffic periods", "MySQL compatibility maintains application compatibility"], "otherOptions": "B) Scheduled scaling can''t handle unpredictable patterns\nC) Smaller instance may cause performance issues during peaks\nD) DynamoDB requires application rewrite from MySQL"}'),
       (115, 15, 'AWS Cost Optimization - Data Transfer', 'Application',
        'Domain 4: Design Cost-Optimized Architectures',
        'A company transfers 100TB of data monthly from EC2 instances to users worldwide. Current data transfer costs are $9,000/month. The data consists of software downloads that rarely change. Which solution provides the MOST significant cost reduction?',
        '[{"text": "A) Use VPC endpoints to reduce data transfer charges", "isCorrect": false}, {"text": "B) Implement CloudFront CDN with S3 origin", "isCorrect": true}, {"text": "C) Move EC2 instances to regions with lower data transfer costs", "isCorrect": false}, {"text": "D) Compress all data before transmission", "isCorrect": false}]',
        'B) Implement CloudFront CDN with S3 origin',
        'CloudFront eliminates most internet data transfer charges and caches static content globally, dramatically reducing costs for software downloads.',
        '{"summary": "CloudFront cost optimization for static content:", "breakdown": ["No data transfer charges between S3 and CloudFront", "Reduced CloudFront to internet rates vs EC2 to internet", "Caching reduces origin data transfer for repeated downloads", "Global edge locations improve performance and reduce costs"], "otherOptions": "A) VPC endpoints help AWS service costs, not internet transfer\nC) Regional pricing differences minimal for internet transfer\nD) Compression helps but doesn''t address core transfer costs"}'),
       (116, 16, 'AWS Architecture - Decoupling', 'Application', 'Domain 1: Design Resilient Architectures',
        'A photo processing application receives images via API, processes them, and stores results. During peak times, the processing queue becomes backlogged and API requests start failing. Which decoupling strategy provides the BEST resilience?',
        '[{"text": "A) Increase EC2 instance size for faster processing", "isCorrect": false}, {"text": "B) Use SQS queue between API and processing with Auto Scaling based on queue depth", "isCorrect": true}, {"text": "C) Add more API Gateway throttling to reduce incoming requests", "isCorrect": false}, {"text": "D) Use Lambda functions instead of EC2 for processing", "isCorrect": false}]',
        'B) Use SQS queue between API and processing with Auto Scaling based on queue depth',
        'SQS decouples components allowing the API to accept requests even when processing is slow, with Auto Scaling adding capacity based on queue backlog.',
        '{"summary": "SQS decoupling benefits:", "breakdown": ["API remains responsive during processing delays", "Queue acts as buffer for variable processing times", "Auto Scaling responds to actual demand (queue depth)", "No lost requests during traffic spikes"], "otherOptions": "A) Larger instances don''t solve architecture coupling\nC) Throttling causes request failures\nD) Lambda has 15-minute timeout limit for processing"}'),
       (117, 17, 'AWS Performance - Auto Scaling', 'Expert', 'Domain 2: Design High-Performing Architectures',
        'An application experiences sudden traffic spikes that cause Auto Scaling to launch instances, but by the time instances are ready, traffic has decreased. This causes unnecessary costs. Which scaling strategy provides the BEST optimization?',
        '[{"text": "A) Use predictive scaling with machine learning to anticipate traffic patterns", "isCorrect": true}, {"text": "B) Decrease Auto Scaling cooldown periods for faster scaling", "isCorrect": false}, {"text": "C) Use larger instance types that can handle traffic spikes", "isCorrect": false}, {"text": "D) Implement step scaling with smaller scaling increments", "isCorrect": false}]',
        'A) Use predictive scaling with machine learning to anticipate traffic patterns',
        'Predictive scaling uses machine learning to forecast traffic and pre-scale capacity, avoiding the lag time of reactive scaling.',
        '{"summary": "Predictive scaling advantages:", "breakdown": ["ML forecasts traffic patterns to pre-scale capacity", "Reduces lag time between traffic spike and available capacity", "Prevents over-scaling by understanding traffic duration", "Works with historical patterns and scheduled events"], "otherOptions": "B) Faster scaling still reactive, doesn''t solve core timing issue\nC) Over-provisioning increases costs unnecessarily\nD) Step scaling still reactive to traffic changes"}'),
       (118, 18, 'AWS Security - Secrets Management', 'Application', 'Domain 3: Design Secure Applications',
        'A microservices application needs to securely store and rotate database passwords, API keys, and certificates. The solution must integrate with RDS automatic rotation and provide audit trails. Which service combination is MOST appropriate?',
        '[{"text": "A) Store secrets in S3 with bucket encryption and versioning", "isCorrect": false}, {"text": "B) Use AWS Secrets Manager with CloudTrail logging", "isCorrect": true}, {"text": "C) Store secrets in Systems Manager Parameter Store with SecureString", "isCorrect": false}, {"text": "D) Use AWS KMS to encrypt secrets stored in DynamoDB", "isCorrect": false}]',
        'B) Use AWS Secrets Manager with CloudTrail logging',
        'Secrets Manager provides automatic rotation for RDS, integration with AWS services, and complete audit trails through CloudTrail.',
        '{"summary": "Secrets Manager comprehensive benefits:", "breakdown": ["Automatic rotation for RDS, Redshift, DocumentDB", "Native integration with AWS services", "Complete audit trail through CloudTrail", "Secure retrieval with fine-grained IAM permissions"], "otherOptions": "A) S3 not designed for secrets management\nC) Parameter Store lacks automatic rotation for databases\nD) Custom solution requires building rotation logic"}'),
       (119, 19, 'AWS Architecture - Service Limits', 'Expert', 'Domain 2: Design High-Performing Architectures',
        'An application needs to upload files up to 500GB to S3. Users report upload failures and timeouts. The application currently uses single PUT requests. Which solution addresses the root cause of the upload failures?',
        '[{"text": "A) Enable S3 Transfer Acceleration for faster uploads", "isCorrect": false}, {"text": "B) Implement S3 multipart upload for large files", "isCorrect": true}, {"text": "C) Use CloudFront for upload optimization", "isCorrect": false}, {"text": "D) Increase application timeout settings", "isCorrect": false}]',
        'B) Implement S3 multipart upload for large files',
        'S3 has a 5GB limit for single PUT operations. Files larger than 5GB require multipart upload, which is recommended for files over 100MB.',
        '{"summary": "S3 upload limits and best practices:", "breakdown": ["Single PUT limit: 5GB maximum file size", "Multipart upload: Required for files >5GB, recommended >100MB", "500GB files: Must use multipart upload", "Improves reliability with retry capability per part"], "otherOptions": "A) Transfer Acceleration helps speed, doesn''t solve size limit\nC) CloudFront not designed for large file uploads\nD) Timeouts don''t solve the 5GB PUT limit"}'),
       (120, 20, 'AWS Architecture - Lambda Limitations', 'Expert', 'Domain 1: Design Resilient Architectures',
        'A data processing function needs to run for 20 minutes to process large datasets. The current Lambda function times out after 15 minutes. Which solution provides the BEST architecture for this requirement?',
        '[{"text": "A) Increase Lambda timeout to maximum 15 minutes and optimize code", "isCorrect": false}, {"text": "B) Break processing into smaller Lambda functions using Step Functions", "isCorrect": true}, {"text": "C) Use Lambda with SQS to process data in smaller chunks", "isCorrect": false}, {"text": "D) Migrate to ECS Fargate for longer-running tasks", "isCorrect": false}]',
        'B) Break processing into smaller Lambda functions using Step Functions',
        'Lambda has a hard 15-minute timeout limit. Step Functions can orchestrate multiple Lambda functions to handle longer workflows.',
        '{"summary": "Lambda timeout limits and solutions:", "breakdown": ["Lambda maximum timeout: 15 minutes (hard limit)", "Step Functions: Can orchestrate multi-step workflows", "Break down processing: Multiple Lambda functions in sequence", "State management: Step Functions handles workflow state"], "otherOptions": "A) 15 minutes is the maximum timeout\nC) SQS helps with async processing but doesn''t solve timeout\nD) ECS Fargate valid but Step Functions more serverless-native"}'),
       (121, 21, 'AWS Cost Optimization - Compute Savings', 'Expert', 'Domain 4: Design Cost-Optimized Architectures',
        'A company runs a web application with the following pattern: 2 instances minimum 24/7, scales to 10 instances during business hours (9 AM-5 PM weekdays), and spikes to 50 instances during monthly sales (first 3 days). Current monthly On-Demand costs are $15,000. Which purchasing strategy provides MAXIMUM cost savings while maintaining performance?',
        '[{"text": "A) All-Upfront Reserved Instances for 10 instances, On-Demand for the rest", "isCorrect": false}, {"text": "B) Reserved Instances for 2 instances, Savings Plans for business hours capacity, Spot for sales spikes", "isCorrect": true}, {"text": "C) Compute Savings Plans for all capacity needs", "isCorrect": false}, {"text": "D) Scheduled Reserved Instances for business hours, On-Demand for spikes", "isCorrect": false}]',
        'B) Reserved Instances for 2 instances, Savings Plans for business hours capacity, Spot for sales spikes',
        'Layered purchasing strategy: RIs for baseline (70% savings), Savings Plans for predictable scaling (66% savings), Spot for spikes (up to 90% savings).',
        '{"summary": "Optimal cost strategy for variable workloads:", "breakdown": ["Reserved Instances: 2 baseline instances (24/7) - 70% savings", "Savings Plans: 8 additional instances for business hours - 66% savings", "Spot Instances: 40 instances for monthly sales - up to 90% savings", "Total savings: Approximately 75% reduction from $15,000"], "otherOptions": "A) Over-provisioning RIs for 10 instances wastes money nights/weekends\nC) Savings Plans less discount than RIs for baseline\nD) Scheduled RIs discontinued, wouldn''t cover spikes"}'),
       (122, 22, 'AWS Cost Optimization - Data Processing', 'Application',
        'Domain 4: Design Cost-Optimized Architectures',
        'A data analytics company processes 10TB of log files daily using EMR clusters. Processing takes 4 hours and runs once daily at 2 AM. The current approach uses On-Demand instances costing $8,000/month. Which architecture change provides the BEST cost optimization?',
        '[{"text": "A) Migrate to AWS Glue for serverless ETL processing", "isCorrect": false}, {"text": "B) Use EMR on Spot Instances with diverse instance types across multiple AZs", "isCorrect": true}, {"text": "C) Pre-process data with Lambda before EMR to reduce cluster size", "isCorrect": false}, {"text": "D) Use smaller EMR cluster running 24/7 with Reserved Instances", "isCorrect": false}]',
        'B) Use EMR on Spot Instances with diverse instance types across multiple AZs',
        'EMR on Spot Instances can provide 50-80% cost savings for batch processing workloads that can tolerate interruptions.',
        '{"summary": "EMR Spot Instance optimization:", "breakdown": ["Spot savings: 50-80% reduction from On-Demand pricing", "Instance diversity: Reduces interruption risk", "Multiple AZs: Ensures capacity availability", "4-hour processing window: Perfect for Spot reliability"], "otherOptions": "A) Glue more expensive for 10TB daily processing\nC) Lambda timeout and cost limitations for TB-scale data\nD) 24/7 cluster wasteful for 4-hour daily job"}'),
       (123, 23, 'AWS Cost Optimization - Storage Optimization', 'Expert',
        'Domain 4: Design Cost-Optimized Architectures',
        'A media company stores 500TB of video content in S3 Standard. Analytics show: 10% accessed daily (hot), 30% accessed weekly (warm), 60% accessed rarely but unpredictably. Retrieval must complete within 12 hours. Current cost is $11,500/month. Which storage strategy provides optimal cost reduction?',
        '[{"text": "A) S3 Intelligent-Tiering for all content with archive configuration", "isCorrect": true}, {"text": "B) S3 Standard for hot, S3 IA for warm, S3 Glacier for cold data", "isCorrect": false}, {"text": "C) S3 One Zone-IA for all content to reduce costs", "isCorrect": false}, {"text": "D) S3 Standard for hot, S3 Glacier Instant Retrieval for all other content", "isCorrect": false}]',
        'A) S3 Intelligent-Tiering for all content with archive configuration',
        'S3 Intelligent-Tiering automatically optimizes storage costs for unpredictable access patterns without retrieval fees, perfect for this use case.',
        '{"summary": "Intelligent-Tiering benefits for unpredictable access:", "breakdown": ["Automatic tiering: No manual lifecycle policies needed", "No retrieval fees: Critical for unpredictable access", "Archive configuration: Moves rarely accessed objects to archive tiers", "Cost savings: Up to 70% reduction while maintaining performance"], "otherOptions": "B) Manual lifecycle rules don''t handle unpredictable access well\nC) One Zone-IA risky for 500TB of valuable content\nD) Glacier Instant Retrieval has retrieval fees for 60% of content"}'),
       (124, 24, 'AWS Security - ML Data Protection', 'Application', 'Domain 3: Design Secure Applications',
        'A healthcare company uses Amazon Comprehend Medical to analyze patient records and Amazon Textract for insurance form processing. They must ensure HIPAA compliance and data isolation. Which security configuration is MOST appropriate?',
        '[{"text": "A) Use default service encryption and enable CloudTrail logging", "isCorrect": false}, {"text": "B) Enable VPC endpoints, use customer-managed KMS keys, and create separate IAM roles per service", "isCorrect": true}, {"text": "C) Process all data in a single private subnet with network ACLs", "isCorrect": false}, {"text": "D) Use AWS managed keys and restrict access with S3 bucket policies", "isCorrect": false}]',
        'B) Enable VPC endpoints, use customer-managed KMS keys, and create separate IAM roles per service',
        'VPC endpoints ensure private connectivity, customer-managed KMS keys provide encryption control, and separate IAM roles enable least privilege access for HIPAA compliance.',
        '{"summary": "HIPAA-compliant ML service configuration:", "breakdown": ["VPC endpoints: Keep data traffic within AWS network", "Customer-managed KMS: Full control over encryption keys", "Separate IAM roles: Least privilege per service", "Audit trail: CloudTrail logs all API calls for compliance"], "otherOptions": "A) Default encryption insufficient for HIPAA requirements\nC) Network ACLs don''t provide service-level isolation\nD) AWS managed keys don''t provide sufficient control"}'),
       (125, 25, 'AWS Security - Container Security', 'Expert', 'Domain 3: Design Secure Applications',
        'A fintech application runs on EKS with sensitive payment processing workloads. The security team requires pod-level network isolation, secrets rotation every 30 days, and runtime threat detection. Which solution meets ALL requirements?',
        '[{"text": "A) Calico network policies, AWS Secrets Manager, and Amazon Inspector", "isCorrect": false}, {"text": "B) AWS App Mesh, Systems Manager Parameter Store, and CloudWatch Container Insights", "isCorrect": false}, {"text": "C) EKS security groups for pods, AWS Secrets Manager with rotation, and Amazon GuardDuty EKS protection", "isCorrect": true}, {"text": "D) Network ACLs, HashiCorp Vault, and AWS Config rules", "isCorrect": false}]',
        'C) EKS security groups for pods, AWS Secrets Manager with rotation, and Amazon GuardDuty EKS protection',
        'EKS security groups provide pod-level isolation, Secrets Manager handles automatic rotation, and GuardDuty EKS protection offers runtime threat detection.',
        '{"summary": "Comprehensive EKS security architecture:", "breakdown": ["Security groups for pods: Fine-grained network isolation at pod level", "Secrets Manager: Automatic 30-day rotation with EKS integration", "GuardDuty EKS: Runtime threat detection and anomaly detection", "Native AWS integration: Simplified management and compliance"], "otherOptions": "A) Inspector scans vulnerabilities, not runtime threats\nB) App Mesh is service mesh, not security focused\nD) Network ACLs too coarse for pod-level control"}'),
       (126, 26, 'AWS Architecture - Event-Driven Resilience', 'Expert', 'Domain 1: Design Resilient Architectures',
        'An e-commerce platform must process orders from multiple channels (web, mobile, partners) with different formats. The system must handle 50,000 orders/hour during sales, ensure no order loss, and enable new channel integration without affecting existing ones. Which architecture provides the BEST resilience and scalability?',
        '[{"text": "A) API Gateway with Lambda functions writing directly to DynamoDB", "isCorrect": false}, {"text": "B) Amazon EventBridge with channel-specific rules routing to SQS queues and containerized processors", "isCorrect": true}, {"text": "C) Kinesis Data Streams with Lambda consumers for each channel", "isCorrect": false}, {"text": "D) Application Load Balancer with Auto Scaling EC2 instances", "isCorrect": false}]',
        'B) Amazon EventBridge with channel-specific rules routing to SQS queues and containerized processors',
        'EventBridge provides event routing with schema registry, SQS ensures message durability, and containerized processors enable independent scaling per channel.',
        '{"summary": "Event-driven architecture benefits:", "breakdown": ["EventBridge: Central event router with content-based filtering", "Schema Registry: Validates events and enables discovery", "SQS queues: Guaranteed message delivery and buffering", "Container isolation: Each channel processor scales independently"], "otherOptions": "A) Direct writes risk data loss during high load\nC) Kinesis overkill for transactional data, complex scaling\nD) Monolithic approach, difficult to add new channels"}'),
       (127, 27, 'AWS Architecture - Disaster Recovery', 'Application', 'Domain 1: Design Resilient Architectures',
        'A financial services application requires RPO of 1 hour and RTO of 4 hours. The application uses Aurora PostgreSQL, EC2 instances with custom AMIs, and stores documents in S3. The DR solution must be cost-effective. Which approach meets these requirements?',
        '[{"text": "A) Aurora Global Database with standby EC2 instances in secondary region", "isCorrect": false}, {"text": "B) Aurora backups to secondary region, AMIs copied cross-region, S3 cross-region replication, and AWS Backup", "isCorrect": true}, {"text": "C) Multi-region active-active deployment with Route 53 failover", "isCorrect": false}, {"text": "D) Database snapshots and EC2 snapshots copied hourly to secondary region", "isCorrect": false}]',
        'B) Aurora backups to secondary region, AMIs copied cross-region, S3 cross-region replication, and AWS Backup',
        'Backup-based DR meets RPO/RTO requirements cost-effectively: Aurora automated backups (continuous), AMIs ready for quick launch, S3 CRR for documents.',
        '{"summary": "Cost-effective DR strategy components:", "breakdown": ["Aurora backups: Point-in-time recovery within 5 minutes (meets 1-hour RPO)", "Cross-region AMI copies: Ready for quick EC2 launch", "S3 CRR: Near real-time replication for documents", "AWS Backup: Centralized backup management and compliance"], "otherOptions": "A) Global Database expensive for 4-hour RTO requirement\nC) Active-active overkill and complex for these requirements\nD) Manual snapshots miss continuous data changes"}'),
       (128, 28, 'AWS Performance - API Optimization', 'Application', 'Domain 2: Design High-Performing Architectures',
        'A mobile app backend serves personalized content to 10 million users globally. The API Gateway shows high latency for data aggregation from multiple microservices. Each request makes 5-7 service calls with 200ms average latency per call. Which solution provides the BEST performance improvement?',
        '[{"text": "A) Implement API Gateway caching for all endpoints", "isCorrect": false}, {"text": "B) Use AWS AppSync with GraphQL to batch and parallelize service calls", "isCorrect": true}, {"text": "C) Add ElastiCache in front of each microservice", "isCorrect": false}, {"text": "D) Increase Lambda memory allocation for faster processing", "isCorrect": false}]',
        'B) Use AWS AppSync with GraphQL to batch and parallelize service calls',
        'AppSync with GraphQL reduces API calls through query batching and parallel resolution, dramatically reducing overall latency for aggregated data.',
        '{"summary": "AppSync optimization benefits:", "breakdown": ["Query batching: Single request instead of 5-7 calls", "Parallel resolution: Service calls execute simultaneously", "Reduced latency: From 1000-1400ms to 200-300ms total", "Caching built-in: Further performance improvements"], "otherOptions": "A) Caching doesn''t help with personalized content\nC) Still requires serial service calls\nD) Lambda memory doesn''t solve aggregation latency"}'),
       (129, 29, 'AWS Performance - ML Inference', 'Expert', 'Domain 2: Design High-Performing Architectures',
        'A video streaming platform needs real-time content moderation using computer vision ML models. They process 1,000 concurrent streams with <100ms inference latency requirement. Current SageMaker endpoint shows 300ms latency. Which optimization provides the required performance?',
        '[{"text": "A) Use larger SageMaker instance types with more GPUs", "isCorrect": false}, {"text": "B) Deploy models to Lambda with Provisioned Concurrency", "isCorrect": false}, {"text": "C) Implement SageMaker multi-model endpoints with edge deployment using AWS IoT Greengrass", "isCorrect": false}, {"text": "D) Use Amazon EC2 Inf1 instances with AWS Neuron for optimized inference", "isCorrect": true}]',
        'D) Use Amazon EC2 Inf1 instances with AWS Neuron for optimized inference',
        'EC2 Inf1 instances with AWS Inferentia chips provide the lowest latency and highest throughput for real-time ML inference at scale.',
        '{"summary": "Inf1 instance optimization benefits:", "breakdown": ["AWS Inferentia chips: Purpose-built for ML inference", "Sub-100ms latency: Achievable with optimized models", "High throughput: Handle 1,000 concurrent streams", "Cost-effective: Up to 70% lower cost than GPU instances"], "otherOptions": "A) Larger instances don''t guarantee latency reduction\nB) Lambda cold starts and limits prevent consistent <100ms\nC) Edge deployment adds complexity without meeting core requirement"}'),
       (130, 30, 'AWS Cost Optimization - Hybrid Architecture', 'Expert',
        'Domain 4: Design Cost-Optimized Architectures',
        'A company wants to modernize their on-premises data warehouse (50TB) to AWS. They need to maintain some on-premises reporting tools while enabling cloud analytics. Budget is limited to $5,000/month. Query patterns: 20% hot data (daily), 30% warm (weekly), 50% cold (monthly). Which architecture provides the BEST cost optimization?',
        '[{"text": "A) Migrate everything to Redshift with Reserved Instances", "isCorrect": false}, {"text": "B) Use Redshift for hot data, Redshift Spectrum for warm/cold data in S3, with AWS Direct Connect", "isCorrect": true}, {"text": "C) Keep all data on-premises and use Athena federated queries", "isCorrect": false}, {"text": "D) Use RDS PostgreSQL with read replicas for all data", "isCorrect": false}]',
        'B) Use Redshift for hot data, Redshift Spectrum for warm/cold data in S3, with AWS Direct Connect',
        'Tiered approach with Redshift for hot data and Spectrum for S3-based warm/cold data provides optimal price-performance within budget constraints.',
        '{"summary": "Hybrid data warehouse optimization:", "breakdown": ["Redshift (10TB): ~$2,500/month for hot data performance", "S3 + Spectrum (40TB): ~$1,000/month for warm/cold storage", "Direct Connect: ~$1,500/month for reliable hybrid connectivity", "Total: ~$5,000/month within budget with optimal performance"], "otherOptions": "A) Full Redshift for 50TB exceeds budget significantly\nC) Athena federated queries too slow for hot data needs\nD) RDS not optimized for analytics workloads, would exceed budget"}'),
       (131, 31, 'AWS Cost Optimization - Serverless Economics', 'Expert',
        'Domain 4: Design Cost-Optimized Architectures',
        'A news aggregation service processes 50 million articles monthly with unpredictable spikes during breaking news (up to 10x normal load). Current EC2-based architecture costs $12,000/month and struggles with spike handling. Articles average 5KB, require text analysis, and must be searchable within 1 minute. Which serverless architecture provides the best cost optimization while meeting requirements?',
        '[{"text": "A) API Gateway → Lambda → DynamoDB with ElasticSearch", "isCorrect": false}, {"text": "B) EventBridge → SQS → Lambda with concurrent execution limits → S3 → Athena with Glue crawlers", "isCorrect": true}, {"text": "C) Kinesis Data Streams → Kinesis Analytics → RDS", "isCorrect": false}, {"text": "D) Step Functions orchestrating multiple Lambda functions → Aurora Serverless", "isCorrect": false}]',
        'B) EventBridge → SQS → Lambda with concurrent execution limits → S3 → Athena with Glue crawlers',
        'EventBridge with SQS provides event buffering for spikes, Lambda with concurrency limits controls costs, S3 offers cheap storage, and Athena enables SQL searches without database costs.',
        '{"summary": "Serverless architecture cost optimization:", "breakdown": ["EventBridge + SQS: Handles 10x spikes without over-provisioning", "Lambda concurrency limits: Prevents runaway costs during spikes", "S3 storage: $0.023/GB vs database storage at $0.10/GB", "Athena queries: Pay-per-query ($5/TB scanned) vs always-on database"], "otherOptions": "A) ElasticSearch cluster costs $3000+/month minimum\nC) Kinesis Analytics expensive for sporadic spikes\nD) Aurora Serverless still has minimum capacity costs"}'),
       (132, 32, 'AWS Cost Optimization - Container Right-Sizing', 'Application',
        'Domain 4: Design Cost-Optimized Architectures',
        'A microservices application runs 40 containers on ECS with varying resource needs: 10 containers need 4 vCPU/8GB (API servers), 20 need 1 vCPU/2GB (workers), and 10 need 0.5 vCPU/1GB (sidecars). Current costs using m5.4xlarge instances are $8,000/month with 35% CPU utilization. Which optimization strategy provides maximum cost savings?',
        '[{"text": "A) Switch to larger instances for better CPU:memory ratio", "isCorrect": false}, {"text": "B) Use ECS capacity providers with mixed instance types and Spot for workers", "isCorrect": true}, {"text": "C) Migrate all containers to Fargate with right-sized task definitions", "isCorrect": false}, {"text": "D) Implement cluster auto-scaling based on CPU utilization", "isCorrect": false}]',
        'B) Use ECS capacity providers with mixed instance types and Spot for workers',
        'Capacity providers enable mixing instance types optimized for different workloads, while Spot instances for stateless workers can reduce costs by 70-90%.',
        '{"summary": "Container cost optimization strategy:", "breakdown": ["Mixed instances: c5 for CPU-intensive, r5 for memory-intensive", "Spot for workers: 70% savings on 20 containers (stateless)", "Better bin packing: Increases utilization from 35% to 75%", "Total savings: Approximately 60% reduction from $8,000"], "otherOptions": "A) Larger instances worsen utilization problems\nC) Fargate more expensive for predictable workloads\nD) Auto-scaling doesn''t address instance type mismatch"}'),
       (133, 33, 'AWS ML Services Integration', 'Application', 'Domain 3: Design Secure Applications',
        'A social media platform needs to detect inappropriate content in user posts (text and images) in real-time. They process 1 million posts daily and must comply with COPPA regulations for users under 13. Which combination of AWS services provides comprehensive content moderation with compliance controls?',
        '[{"text": "A) Amazon Comprehend for text and custom SageMaker model for images", "isCorrect": false}, {"text": "B) Amazon Rekognition Content Moderation, Amazon Comprehend toxicity detection, with Lambda age-gating logic", "isCorrect": true}, {"text": "C) Amazon Textract for text extraction and Amazon Personalize for content filtering", "isCorrect": false}, {"text": "D) AWS Glue for data preparation and Amazon Forecast for predicting inappropriate content", "isCorrect": false}]',
        'B) Amazon Rekognition Content Moderation, Amazon Comprehend toxicity detection, with Lambda age-gating logic',
        'Rekognition provides pre-trained image moderation, Comprehend detects toxic text, and Lambda implements age-appropriate filtering logic for COPPA compliance.',
        '{"summary": "Content moderation architecture components:", "breakdown": ["Rekognition: Detects inappropriate images (violence, adult content)", "Comprehend: Identifies toxic text, PII, and sentiment", "Lambda age-gating: Applies stricter rules for users under 13", "Real-time processing: Sub-second moderation decisions"], "otherOptions": "A) Custom models require training data and maintenance\nC) Textract is for document processing, not moderation\nD) Forecast predicts time-series data, not content issues"}'),
       (134, 34, 'AWS Architecture - IoT and Edge', 'Expert', 'Domain 1: Design Resilient Architectures',
        'A manufacturing company operates 500 factories with 100 IoT sensors each, generating 1KB messages every second. They need local anomaly detection with sub-second response, cloud-based analytics, and must operate during internet outages. Which architecture provides the required edge computing capabilities?',
        '[{"text": "A) Direct IoT Core ingestion with Kinesis Analytics for anomaly detection", "isCorrect": false}, {"text": "B) AWS IoT Greengrass with Lambda functions, local message routing, and intermittent cloud sync", "isCorrect": true}, {"text": "C) EC2 instances in each factory with VPN connections to cloud", "isCorrect": false}, {"text": "D) AWS Outposts in each factory for local processing", "isCorrect": false}]',
        'B) AWS IoT Greengrass with Lambda functions, local message routing, and intermittent cloud sync',
        'IoT Greengrass enables local Lambda execution for sub-second anomaly detection, continues operating offline, and synchronizes with cloud when connected.',
        '{"summary": "Edge computing with IoT Greengrass benefits:", "breakdown": ["Local Lambda: Sub-second anomaly detection at edge", "Offline operation: Continues processing during outages", "Efficient sync: Batches data for cloud analytics when connected", "Cost-effective: No need for servers in 500 locations"], "otherOptions": "A) Requires constant internet, no local processing\nC) Managing 500 EC2 instances operationally complex\nD) Outposts overkill and expensive for IoT workloads"}'),
       (135, 35, 'AWS Cost Optimization - Streaming Data', 'Application',
        'Domain 4: Design Cost-Optimized Architectures',
        'A ride-sharing app ingests 100,000 location updates per second during peak hours (6-9 AM, 5-8 PM) but only 5,000 per second during off-peak. Current Kinesis Data Streams with provisioned shards costs $15,000/month. Real-time processing is required only during peak hours; batch processing is acceptable off-peak. Which architecture reduces costs while meeting requirements?',
        '[{"text": "A) Kinesis Data Streams with auto-scaling shards based on traffic", "isCorrect": false}, {"text": "B) Kinesis Data Streams On-Demand for peaks, Kinesis Firehose to S3 for off-peak batch", "isCorrect": true}, {"text": "C) Replace with SQS queues and Lambda functions", "isCorrect": false}, {"text": "D) DynamoDB Streams with Lambda triggers", "isCorrect": false}]',
        'B) Kinesis Data Streams On-Demand for peaks, Kinesis Firehose to S3 for off-peak batch',
        'Kinesis On-Demand eliminates over-provisioning costs while Firehose to S3 provides cost-effective batch processing during off-peak hours.',
        '{"summary": "Streaming cost optimization strategy:", "breakdown": ["Kinesis On-Demand: Pay only for actual throughput", "Peak hours: Real-time processing as required", "Off-peak Firehose: 90% cheaper than real-time streams", "Estimated savings: 65% reduction from $15,000/month"], "otherOptions": "A) Auto-scaling still requires minimum shards\nC) SQS lacks streaming semantics for real-time\nD) DynamoDB Streams tied to table operations"}'),
       (136, 36, 'AWS Architecture - Multi-Region', 'Expert', 'Domain 1: Design Resilient Architectures',
        'A global trading platform requires <10ms latency for 99% of users across US, EU, and APAC regions. The platform processes 1 million trades daily with strict consistency requirements. Each trade must be reflected globally within 100ms. Current single-region deployment shows 150ms+ latency for distant users. Which multi-region architecture best meets these requirements?',
        '[{"text": "A) Deploy full stack in each region with eventual consistency", "isCorrect": false}, {"text": "B) Aurora Global Database with local read replicas, Route 53 geolocation routing, and DynamoDB Global Tables for session data", "isCorrect": true}, {"text": "C) Single region with CloudFront caching for static content", "isCorrect": false}, {"text": "D) Active-active regions with asynchronous replication", "isCorrect": false}]',
        'B) Aurora Global Database with local read replicas, Route 53 geolocation routing, and DynamoDB Global Tables for session data',
        'Aurora Global Database provides <1 second replication with strong consistency, local read replicas ensure <10ms reads, while DynamoDB Global Tables handle session state with automatic conflict resolution.',
        '{"summary": "Multi-region architecture for low latency trading:", "breakdown": ["Aurora Global: <1 second cross-region replication", "Local read replicas: <10ms read latency per region", "Write forwarding: Maintains consistency for trades", "DynamoDB Global Tables: Multi-master for session data"], "otherOptions": "A) Eventual consistency violates trade requirements\nC) CloudFront doesn''t help with dynamic trade data\nD) Async replication can''t guarantee 100ms consistency"}'),
       (137, 37, 'AWS Security - Zero Trust Architecture', 'Expert', 'Domain 3: Design Secure Applications',
        'A financial services company needs to implement zero-trust access to their AWS environment for 500 developers across 50 teams. Requirements include: no long-lived credentials, audit trail of all actions, team-based access boundaries, and integration with existing Okta SSO. Which solution provides the most comprehensive zero-trust implementation?',
        '[{"text": "A) IAM users with MFA and IP restrictions for each developer", "isCorrect": false}, {"text": "B) AWS SSO with Okta integration, Permission Sets with session tags, and CloudTrail with EventBridge rules", "isCorrect": true}, {"text": "C) Cognito user pools with custom authorizers", "isCorrect": false}, {"text": "D) IAM roles with external ID and session policies", "isCorrect": false}]',
        'B) AWS SSO with Okta integration, Permission Sets with session tags, and CloudTrail with EventBridge rules',
        'AWS SSO provides temporary credentials, Okta integration enables SAML-based authentication, Permission Sets with session tags enforce team boundaries, while CloudTrail ensures complete audit trails.',
        '{"summary": "Zero-trust implementation components:", "breakdown": ["AWS SSO: No long-lived credentials, automatic rotation", "Permission Sets: Team-based access with session tags", "Okta SAML: Leverages existing identity provider", "CloudTrail + EventBridge: Real-time audit and alerting"], "otherOptions": "A) IAM users require long-lived access keys\nC) Cognito designed for application users, not AWS access\nD) Missing centralized management for 500 developers"}'),
       (138, 38, 'AWS Performance - Hybrid Networking', 'Application', 'Domain 2: Design High-Performing Architectures',
        'A media company needs to transfer 50TB of daily video content from on-premises editing stations to S3 for processing. Current internet upload takes 20 hours, causing production delays. The solution must complete transfers within 4 hours and support 100 concurrent editor workstations. Which approach best meets these performance requirements?',
        '[{"text": "A) Multiple Site-to-Site VPN connections with ECMP", "isCorrect": false}, {"text": "B) AWS Direct Connect with Virtual Interfaces and S3 Transfer Acceleration", "isCorrect": true}, {"text": "C) Storage Gateway File Gateway with local cache", "isCorrect": false}, {"text": "D) Daily AWS Snowball Edge shipments", "isCorrect": false}]',
        'B) AWS Direct Connect with Virtual Interfaces and S3 Transfer Acceleration',
        'Direct Connect provides dedicated bandwidth for consistent transfer speeds, VIFs enable private S3 connectivity, and Transfer Acceleration optimizes the upload path for concurrent workstations.',
        '{"summary": "High-performance hybrid transfer solution:", "breakdown": ["Direct Connect: 10Gbps dedicated bandwidth", "VIF to S3: Private connectivity, no internet congestion", "Transfer Acceleration: Optimizes for 100 concurrent uploads", "50TB in 4 hours: Requires ~3.5Gbps sustained throughput"], "otherOptions": "A) VPN limited to ~1.25Gbps aggregate bandwidth\nC) File Gateway adds latency and cache limitations\nD) Snowball shipping time exceeds 4-hour requirement"}'),
       (139, 39, 'AWS Architecture - Event-Driven Serverless', 'Expert',
        'Domain 2: Design High-Performing Architectures',
        'An e-commerce platform needs to process order events with multiple downstream systems: inventory (must process once), shipping (at-least-once), analytics (can tolerate duplicates), and email (exactly-once). Order volume varies from 100/minute to 10,000/minute during flash sales. Which event-driven architecture ensures correct delivery semantics for each consumer?',
        '[{"text": "A) SNS with SQS queues for all consumers", "isCorrect": false}, {"text": "B) EventBridge with rules routing to SQS FIFO for inventory, standard SQS for shipping, Kinesis for analytics, and Step Functions for email", "isCorrect": true}, {"text": "C) Kinesis Data Streams with Lambda consumers", "isCorrect": false}, {"text": "D) Direct Lambda invocations with error handling", "isCorrect": false}]',
        'B) EventBridge with rules routing to SQS FIFO for inventory, standard SQS for shipping, Kinesis for analytics, and Step Functions for email',
        'EventBridge provides content-based routing, SQS FIFO ensures exactly-once processing, standard SQS handles at-least-once, Kinesis supports analytics replay, and Step Functions manages email workflow state.',
        '{"summary": "Event delivery semantics per consumer:", "breakdown": ["SQS FIFO: Exactly-once for critical inventory updates", "Standard SQS: At-least-once with retry for shipping", "Kinesis: Replay capability for analytics reprocessing", "Step Functions: Manages email state to prevent duplicates"], "otherOptions": "A) SNS+SQS doesn''t provide exactly-once semantics\nC) Kinesis requires all consumers to process in order\nD) Direct invocations lack delivery guarantees"}'),
       (140, 40, 'AWS Cost Optimization - Modern Architectures', 'Expert',
        'Domain 4: Design Cost-Optimized Architectures',
        'A SaaS company runs 200 customer environments, each with identical architecture: ALB, 2-10 EC2 instances, RDS MySQL, and 50GB of S3 storage. Monthly costs are $400,000 with only 30% resource utilization. Customers require isolation for compliance. Which modernization approach provides maximum cost reduction while maintaining isolation?',
        '[{"text": "A) Containerize applications and run all customers on shared EKS cluster", "isCorrect": false}, {"text": "B) AWS App Runner with customer-specific environments, Aurora Serverless v2, and S3 bucket policies", "isCorrect": true}, {"text": "C) Lambda functions with RDS Proxy and separate VPCs", "isCorrect": false}, {"text": "D) Consolidate all customers into a single multi-tenant application", "isCorrect": false}]',
        'B) AWS App Runner with customer-specific environments, Aurora Serverless v2, and S3 bucket policies',
        'App Runner provides serverless compute with environment isolation, Aurora Serverless v2 scales database resources based on actual usage, eliminating overprovisioning while maintaining compliance isolation.',
        '{"summary": "Serverless multi-tenant cost optimization:", "breakdown": ["App Runner: Pay-per-request pricing, no idle EC2 costs", "Aurora Serverless v2: Scales down to 0.5 ACU when idle", "Environment isolation: Separate App Runner services per customer", "Estimated savings: 70% reduction through utilization-based pricing"], "otherOptions": "A) Shared cluster violates compliance isolation requirements\nC) Lambda cold starts problematic for web applications\nD) Multi-tenant architecture breaks compliance requirements"}'),
       (141, 41, 'AWS Storage Concepts', 'Knowledge', 'Domain 3: Design High-Performing Architectures',
        'A company needs to choose between different AWS storage types for various workloads. Which statement BEST describes the appropriate use cases for each storage type?',
        '[{"text": "A) Use object storage for databases, block storage for file shares, file storage for web content", "isCorrect": false}, {"text": "B) Use block storage for databases, file storage for shared access, object storage for web content", "isCorrect": true}, {"text": "C) Use file storage for databases, object storage for shared access, block storage for web content", "isCorrect": false}, {"text": "D) All storage types can be used interchangeably for any workload", "isCorrect": false}]',
        'B) Use block storage for databases, file storage for shared access, object storage for web content',
        'Block storage (EBS) provides high IOPS for databases, file storage (EFS) enables shared access, object storage (S3) scales for web content.',
        '{"summary": "AWS storage type characteristics:", "breakdown": ["Block storage (EBS): High IOPS, low latency, ideal for databases and boot volumes", "File storage (EFS): POSIX-compliant, shared access, good for content repositories", "Object storage (S3): Auto-scaling, REST API, perfect for web content and backups", "Each type optimized for specific access patterns and performance requirements"], "otherOptions": "A) Object storage not suitable for database IOPS requirements\nC) File storage lacks the IOPS performance needed for databases\nD) Each storage type has specific use cases and limitations"}'),
       (142, 42, 'AWS Storage Concepts', 'Application', 'Domain 2: Design Resilient Architectures',
        'An enterprise is migrating from on-premises and needs to maintain compatibility with existing storage protocols. They use NFS for Unix systems, SMB for Windows, and iSCSI for SAN storage. Which AWS services provide the BEST protocol compatibility?',
        '[{"text": "A) S3 for all protocols using third-party gateways", "isCorrect": false}, {"text": "B) EFS for NFS, FSx for Windows for SMB, Storage Gateway for iSCSI", "isCorrect": true}, {"text": "C) EBS for all protocols using EC2 instance configuration", "isCorrect": false}, {"text": "D) DataSync for protocol translation across all storage types", "isCorrect": false}]',
        'B) EFS for NFS, FSx for Windows for SMB, Storage Gateway for iSCSI',
        'AWS provides native protocol support: EFS (NFS), FSx for Windows (SMB), Storage Gateway (iSCSI) for seamless migration.',
        '{"summary": "Native AWS protocol support:", "breakdown": ["EFS: Native NFS v4.1, POSIX-compliant, Linux/Unix compatibility", "FSx for Windows: Native SMB, Active Directory integration", "Storage Gateway: iSCSI interface, hybrid cloud connectivity", "No application changes required for protocol compatibility"], "otherOptions": "A) S3 doesn''t natively support NFS/SMB/iSCSI protocols\nC) EBS provides block storage but not protocol-level compatibility\nD) DataSync is for data transfer, not protocol translation"}'),
       (143, 43, 'AWS S3 Deep Dive', 'Application', 'Domain 2: Design Resilient Architectures',
        'A media company requires 99.999999999% (11 9s) data durability for their video assets. They need to understand the difference between S3 durability and availability for their SLA reporting. Which statement is CORRECT about S3 durability vs availability?',
        '[{"text": "A) Durability and availability are identical metrics across all S3 storage classes", "isCorrect": false}, {"text": "B) Durability (11 9s) represents data loss protection, while availability varies by storage class", "isCorrect": true}, {"text": "C) Availability is always higher than durability in S3", "isCorrect": false}, {"text": "D) Only S3 Standard provides 11 9s durability", "isCorrect": false}]',
        'B) Durability (11 9s) represents data loss protection, while availability varies by storage class',
        'S3 provides 11 9s durability (data loss protection) across ALL storage classes, but availability (uptime) varies by class.',
        '{"summary": "S3 durability vs availability:", "breakdown": ["Durability (11 9s): Probability of NOT losing data over a year", "Availability varies: Standard (99.99%), IA (99.9%), Glacier (variable)", "All storage classes: Same durability guarantee", "Durability: Data integrity protection through redundancy"], "otherOptions": "A) Different metrics with different SLA percentages\nC) Availability percentages are typically lower than durability\nD) All S3 storage classes maintain 11 9s durability"}'),
       (144, 44, 'AWS S3 Deep Dive', 'Application', 'Domain 1: Design Secure Architectures',
        'A healthcare organization stores patient records in S3 and needs to maintain full control over encryption keys while ensuring HIPAA compliance. They want to minimize key management overhead but need complete audit trails. Which encryption approach provides the BEST balance?',
        '[{"text": "A) SSE-S3 with AWS-managed keys", "isCorrect": false}, {"text": "B) SSE-KMS with customer-managed keys", "isCorrect": true}, {"text": "C) SSE-C with customer-provided keys", "isCorrect": false}, {"text": "D) Client-side encryption with S3 Encryption Client", "isCorrect": false}]',
        'B) SSE-KMS with customer-managed keys',
        'SSE-KMS with customer-managed keys provides control over keys, audit trails via CloudTrail, and AWS handles key management infrastructure.',
        '{"summary": "SSE-KMS benefits for HIPAA compliance:", "breakdown": ["Customer control: Create, rotate, and manage encryption keys", "Audit trails: CloudTrail logs all key usage and access", "AWS management: Infrastructure and availability handled by AWS", "HIPAA compliant: Meets regulatory requirements for key control"], "otherOptions": "A) AWS-managed keys don''t provide sufficient customer control\nC) SSE-C requires managing key storage and availability\nD) Client-side encryption adds complexity without AWS integration benefits"}'),
       (145, 45, 'AWS S3 Deep Dive', 'Expert', 'Domain 4: Design Cost-Optimized Architectures',
        'A content delivery platform stores 200TB of video files with unpredictable access patterns. Analytics show some videos remain popular for months while others are rarely accessed after the first week. They need cost optimization without performance penalties for popular content. Which S3 storage strategy provides optimal cost savings?',
        '[{"text": "A) Lifecycle policy transitioning all content to S3 IA after 30 days", "isCorrect": false}, {"text": "B) S3 Intelligent-Tiering with Deep Archive Access tier enabled", "isCorrect": true}, {"text": "C) Keep popular content in Standard, move others to Glacier based on view counts", "isCorrect": false}, {"text": "D) Use S3 One Zone-IA for all content to reduce costs", "isCorrect": false}]',
        'B) S3 Intelligent-Tiering with Deep Archive Access tier enabled',
        'S3 Intelligent-Tiering automatically optimizes costs based on access patterns without retrieval fees for frequent/infrequent tiers, perfect for unpredictable patterns.',
        '{"summary": "Intelligent-Tiering benefits for unpredictable access:", "breakdown": ["Automatic optimization: No manual lifecycle policies needed", "No retrieval fees: For frequent and infrequent access tiers", "Deep Archive tier: Additional savings for rarely accessed content", "Performance maintained: Popular content stays in frequent tier"], "otherOptions": "A) Fixed lifecycle doesn''t handle unpredictable patterns\nC) Manual management based on view counts is complex and error-prone\nD) One Zone-IA lacks resilience for valuable content"}'),
       (146, 46, 'AWS EBS Deep Dive', 'Application', 'Domain 3: Design High-Performing Architectures',
        'A database application requires consistent 20,000 IOPS with low latency for transaction processing. The database size is 8TB and growing. Which EBS volume type and configuration provides the BEST performance while supporting future growth?',
        '[{"text": "A) gp3 volumes with 20,000 provisioned IOPS", "isCorrect": false}, {"text": "B) io2 Block Express volumes with 20,000 provisioned IOPS", "isCorrect": true}, {"text": "C) gp2 volumes in RAID 0 configuration", "isCorrect": false}, {"text": "D) st1 throughput optimized volumes for cost savings", "isCorrect": false}]',
        'B) io2 Block Express volumes with 20,000 provisioned IOPS',
        'io2 Block Express provides consistent IOPS up to 256,000, supports volumes up to 64TB, and offers sub-millisecond latency for databases.',
        '{"summary": "io2 Block Express advantages for databases:", "breakdown": ["Consistent IOPS: 20,000 IOPS guaranteed regardless of volume size", "Sub-millisecond latency: Optimal for transaction processing", "Scalability: Supports up to 64TB volumes for future growth", "Durability: 99.999% annual durability vs 99.9% for gp3"], "otherOptions": "A) gp3 IOPS can vary with volume size and burst credits\nC) RAID 0 increases failure risk and management complexity\nD) st1 optimized for throughput, not IOPS-intensive workloads"}'),
       (147, 47, 'AWS EBS Deep Dive', 'Application', 'Domain 2: Design Resilient Architectures',
        'A production database uses EBS volumes and requires point-in-time recovery capability with minimal data loss. The current snapshot strategy creates daily snapshots, but the RPO requirement is 4 hours. Which enhancement provides the BEST backup strategy improvement?',
        '[{"text": "A) Increase snapshot frequency to every 4 hours using scheduled snapshots", "isCorrect": true}, {"text": "B) Enable EBS Fast Snapshot Restore on existing daily snapshots", "isCorrect": false}, {"text": "C) Configure EBS Multi-Attach to create live replicas", "isCorrect": false}, {"text": "D) Use AWS Backup service with continuous backup enabled", "isCorrect": false}]',
        'A) Increase snapshot frequency to every 4 hours using scheduled snapshots',
        'Scheduling snapshots every 4 hours directly meets the RPO requirement by ensuring maximum data loss is limited to 4 hours.',
        '{"summary": "Snapshot strategy for 4-hour RPO:", "breakdown": ["Scheduled snapshots: Every 4 hours meets exact RPO requirement", "Incremental backups: Only changed blocks stored, cost-effective", "Point-in-time recovery: Can restore to any snapshot timestamp", "Automated lifecycle: Can retain snapshots based on retention policy"], "otherOptions": "B) Fast Snapshot Restore improves RTO, not RPO\nC) Multi-Attach doesn''t provide backup functionality\nD) Continuous backup overkill for 4-hour RPO requirement"}'),
       (148, 48, 'AWS EBS Deep Dive', 'Expert', 'Domain 1: Design Secure Architectures',
        'A financial application stores sensitive data on EBS volumes and requires encryption in transit and at rest. The application accesses data across multiple AZs and needs to ensure encryption keys remain under customer control with automatic rotation. Which configuration provides comprehensive encryption coverage?',
        '[{"text": "A) EBS encryption with AWS managed keys and HTTPS application protocols", "isCorrect": false}, {"text": "B) EBS encryption with customer-managed KMS keys, automatic rotation enabled, and application-level TLS", "isCorrect": true}, {"text": "C) Instance store volumes with client-side encryption", "isCorrect": false}, {"text": "D) EBS encryption with customer-provided keys (SSE-C)", "isCorrect": false}]',
        'B) EBS encryption with customer-managed KMS keys, automatic rotation enabled, and application-level TLS',
        'Customer-managed KMS keys provide control with automatic rotation, EBS encryption handles data at rest, and TLS provides encryption in transit.',
        '{"summary": "Comprehensive EBS encryption strategy:", "breakdown": ["Customer-managed KMS keys: Full control over encryption keys", "Automatic rotation: Annual key rotation without service interruption", "EBS encryption: All data at rest encrypted including snapshots", "Application TLS: Encrypts data in transit between instances"], "otherOptions": "A) AWS managed keys don''t provide customer control\nC) Instance store data is temporary, not suitable for persistent financial data\nD) Customer-provided keys require manual key management"}'),
       (149, 49, 'AWS File Systems Deep Dive', 'Application', 'Domain 3: Design High-Performing Architectures',
        'A video editing company needs shared storage accessible from 50 EC2 instances across multiple AZs for collaborative editing projects. Files range from 1GB to 50GB, and editors require low-latency access. Performance needs vary from 1 GB/s during off-hours to 10 GB/s during production. Which file system solution provides the BEST performance and scalability?',
        '[{"text": "A) Amazon EFS with General Purpose performance mode", "isCorrect": false}, {"text": "B) Amazon EFS with Max I/O performance mode and Provisioned Throughput", "isCorrect": true}, {"text": "C) Amazon FSx for Lustre with scratch file system", "isCorrect": false}, {"text": "D) Multiple EBS volumes shared using NFS on EC2", "isCorrect": false}]',
        'B) Amazon EFS with Max I/O performance mode and Provisioned Throughput',
        'EFS Max I/O mode supports higher IOPS for concurrent access, while Provisioned Throughput ensures consistent 10 GB/s performance regardless of file system size.',
        '{"summary": "EFS configuration for video editing workloads:", "breakdown": ["Max I/O mode: Higher IOPS limit for 50 concurrent clients", "Provisioned Throughput: Guarantees 10 GB/s during peak production", "Multi-AZ access: Native support across availability zones", "POSIX compliance: Standard file system semantics for editing software"], "otherOptions": "A) General Purpose mode has lower IOPS limits for concurrent access\nC) Lustre scratch file system designed for HPC, not collaborative editing\nD) EBS volumes can''t be natively shared between instances"}'),
       (150, 50, 'AWS File Systems Deep Dive', 'Expert', 'Domain 2: Design High-Performing Architectures',
        'A Windows-based application requires high-performance file storage with Active Directory integration, SMB protocol support, and sub-millisecond latencies. The workload involves intensive random I/O operations on small files. Current on-premises Windows File Server achieves 100,000 IOPS. Which AWS solution provides equivalent performance?',
        '[{"text": "A) Amazon FSx for Windows File Server with SSD storage", "isCorrect": true}, {"text": "B) Amazon EFS with Windows file gateway", "isCorrect": false}, {"text": "C) EC2 Windows instance with attached io2 EBS volumes", "isCorrect": false}, {"text": "D) Amazon FSx for Lustre with Windows connectivity", "isCorrect": false}]',
        'A) Amazon FSx for Windows File Server with SSD storage',
        'FSx for Windows File Server provides native SMB, Active Directory integration, SSD storage for high IOPS, and sub-millisecond latencies specifically optimized for Windows workloads.',
        '{"summary": "FSx for Windows File Server benefits:", "breakdown": ["Native SMB: Full Windows file system compatibility", "Active Directory: Seamless user authentication and authorization", "SSD storage: Up to 100,000+ IOPS for random I/O workloads", "Sub-millisecond latency: Optimized for Windows application performance"], "otherOptions": "B) EFS doesn''t natively support SMB or Active Directory\nC) Self-managed solution lacks native SMB optimization\nD) FSx for Lustre designed for Linux HPC workloads, not Windows"}'),
       (151, 51, 'AWS Hybrid Storage Migration', 'Application', 'Domain 1: Design Resilient Architectures',
        'A company needs to migrate 500TB of archival data from on-premises tape storage to AWS. The data is rarely accessed but must be retrievable within 12 hours when needed. The migration must complete within 3 months with minimal impact on existing network bandwidth. Which migration strategy is MOST efficient?',
        '[{"text": "A) AWS DataSync over Direct Connect for gradual migration", "isCorrect": false}, {"text": "B) Multiple AWS Snowball Edge devices shipped in batches to S3 Glacier Deep Archive", "isCorrect": true}, {"text": "C) Storage Gateway Tape Gateway for gradual cloud migration", "isCorrect": false}, {"text": "D) AWS Transfer Family for secure file transfer", "isCorrect": false}]',
        'B) Multiple AWS Snowball Edge devices shipped in batches to S3 Glacier Deep Archive',
        'Snowball Edge devices handle large data volumes efficiently without network impact, while Glacier Deep Archive provides cost-effective storage with 12-hour retrieval.',
        '{"summary": "Large-scale archival migration strategy:", "breakdown": ["Snowball Edge capacity: 80TB per device, 7 devices for 500TB", "No network impact: Physical data transfer bypasses internet", "Glacier Deep Archive: Lowest cost storage for rarely accessed data", "12-hour retrieval: Meets business requirement for archive access"], "otherOptions": "A) DataSync over network would take months and impact bandwidth\nC) Tape Gateway for ongoing hybrid, not one-time migration\nD) Transfer Family not optimized for 500TB bulk migration"}'),
       (152, 52, 'AWS Hybrid Storage Migration', 'Expert', 'Domain 3: Design High-Performing Architectures',
        'A media company operates a hybrid architecture where video editors work on-premises but need seamless access to cloud storage for rendering jobs. Local performance must remain unaffected while providing transparent cloud storage scalability. Frequently accessed files should be available locally. Which hybrid storage solution provides the BEST user experience?',
        '[{"text": "A) AWS Storage Gateway File Gateway with local cache", "isCorrect": true}, {"text": "B) AWS DataSync scheduled sync between on-premises and S3", "isCorrect": false}, {"text": "C) Direct Connect with EFS mounted on workstations", "isCorrect": false}, {"text": "D) S3 bucket mounted using third-party NFS gateway", "isCorrect": false}]',
        'A) AWS Storage Gateway File Gateway with local cache',
        'Storage Gateway File Gateway provides NFS interface with intelligent local caching, transparent S3 integration, and optimal performance for frequently accessed files.',
        '{"summary": "File Gateway hybrid benefits:", "breakdown": ["Local cache: Frequently accessed files available at local speeds", "Transparent scaling: Files automatically stored in S3", "NFS interface: Standard file system access for editing applications", "Intelligent caching: Recently and frequently used files cached locally"], "otherOptions": "B) DataSync requires scheduled sync, not transparent access\nC) EFS over Direct Connect has latency for large video files\nD) Third-party solutions lack AWS service integration and support"}'),
       (153, 53, 'AWS Data Security Concepts', 'Application', 'Domain 1: Design Secure Architectures',
        'A financial services company needs to automatically discover and protect sensitive data across 200 S3 buckets containing customer documents. They require automated classification, policy enforcement, and immediate alerts for data exposure risks. Which combination provides comprehensive data security automation?',
        '[{"text": "A) AWS Config rules for bucket policies and CloudWatch for monitoring", "isCorrect": false}, {"text": "B) Amazon Macie for discovery and classification, EventBridge for alerts, and S3 Block Public Access", "isCorrect": true}, {"text": "C) AWS Inspector for vulnerability scanning and GuardDuty for threats", "isCorrect": false}, {"text": "D) Custom Lambda functions with CloudTrail for access monitoring", "isCorrect": false}]',
        'B) Amazon Macie for discovery and classification, EventBridge for alerts, and S3 Block Public Access',
        'Macie uses ML to discover sensitive data, provides automatic classification, EventBridge enables real-time alerts, and Block Public Access prevents exposure.',
        '{"summary": "Automated data security components:", "breakdown": ["Amazon Macie: ML-powered sensitive data discovery and classification", "EventBridge integration: Real-time alerts for policy violations", "S3 Block Public Access: Prevents accidental public exposure", "Automated findings: Continuous monitoring across all 200 buckets"], "otherOptions": "A) Config and CloudWatch don''t provide sensitive data discovery\nC) Inspector and GuardDuty focus on infrastructure, not data classification\nD) Custom solutions require significant development and maintenance"}'),
       (154, 54, 'AWS Data Security Concepts', 'Expert', 'Domain 1: Design Secure Architectures',
        'A healthcare organization must implement data classification and protection for patient records across multiple AWS services (S3, RDS, DynamoDB). They need automated discovery of PHI, policy-based access controls, and audit trails meeting HIPAA requirements. Which architecture provides comprehensive data protection?',
        '[{"text": "A) Amazon Macie for S3, database activity streams for RDS, and VPC Flow Logs", "isCorrect": false}, {"text": "B) Amazon Macie for S3, AWS CloudTrail data events, and customer-managed KMS keys across all services", "isCorrect": true}, {"text": "C) AWS Config for compliance monitoring and GuardDuty for threat detection", "isCorrect": false}, {"text": "D) Custom data loss prevention (DLP) solution with third-party tools", "isCorrect": false}]',
        'B) Amazon Macie for S3, AWS CloudTrail data events, and customer-managed KMS keys across all services',
        'Macie discovers PHI in S3, CloudTrail data events provide comprehensive audit trails, and customer-managed KMS keys ensure encryption control across all services.',
        '{"summary": "HIPAA-compliant data protection architecture:", "breakdown": ["Macie: Automated PHI discovery and classification in S3", "CloudTrail data events: Complete audit trail of data access", "Customer-managed KMS: Encryption control across S3, RDS, DynamoDB", "Policy-based access: IAM policies with resource-based conditions"], "otherOptions": "A) Missing encryption control and comprehensive audit coverage\nC) Config and GuardDuty don''t provide PHI discovery and classification\nD) Custom solutions require extensive compliance validation"}'),
       (155, 55, 'AWS Encryption', 'Expert', 'Domain 1: Design Secure Architectures',
        'A multinational corporation needs end-to-end encryption for data stored across AWS services in multiple regions. They require customer control over encryption keys, automatic rotation, cross-region access, and integration with their existing HSM infrastructure. Which encryption strategy provides the MOST comprehensive solution?',
        '[{"text": "A) AWS KMS customer-managed keys with automatic rotation in each region", "isCorrect": false}, {"text": "B) AWS CloudHSM cluster with custom key management application", "isCorrect": true}, {"text": "C) Client-side encryption with application-managed keys", "isCorrect": false}, {"text": "D) AWS KMS with imported key material from on-premises HSM", "isCorrect": false}]',
        'B) AWS CloudHSM cluster with custom key management application',
        'CloudHSM provides dedicated hardware security modules with full customer control, integrates with existing HSM infrastructure, and supports cross-region key management.',
        '{"summary": "CloudHSM comprehensive encryption benefits:", "breakdown": ["Dedicated HSM: Full customer control over encryption operations", "FIPS 140-2 Level 3: Highest security certification available", "Cross-region clustering: Keys available across multiple regions", "HSM integration: Compatible with existing on-premises HSM infrastructure"], "otherOptions": "A) KMS keys are region-specific and don''t integrate with existing HSM\nC) Application-managed keys lack HSM security and are difficult to manage at scale\nD) Imported key material in KMS doesn''t provide full HSM integration"}'),
       (156, 56, 'AWS Encryption', 'Application', 'Domain 1: Design Secure Architectures',
        'A development team needs to encrypt application data before storing it in DynamoDB. They want to minimize performance impact while ensuring field-level encryption for sensitive columns (SSN, credit card numbers) but allow searching on encrypted email addresses. Which encryption approach provides the BEST balance of security and functionality?',
        '[{"text": "A) DynamoDB encryption at rest with AWS managed keys", "isCorrect": false}, {"text": "B) AWS DynamoDB Encryption Client with deterministic encryption for emails, probabilistic for sensitive fields", "isCorrect": true}, {"text": "C) Application-level AES encryption for all fields", "isCorrect": false}, {"text": "D) AWS KMS encryption for entire DynamoDB table", "isCorrect": false}]',
        'B) AWS DynamoDB Encryption Client with deterministic encryption for emails, probabilistic for sensitive fields',
        'DynamoDB Encryption Client provides field-level encryption with deterministic encryption for searchable fields and probabilistic encryption for maximum security on sensitive data.',
        '{"summary": "Field-level encryption strategy:", "breakdown": ["Deterministic encryption: Same email always produces same ciphertext (searchable)", "Probabilistic encryption: Different ciphertext each time (maximum security)", "Client-side encryption: Data encrypted before sending to DynamoDB", "Performance optimized: Only sensitive fields encrypted, not entire records"], "otherOptions": "A) Table-level encryption doesn''t provide field-level control\nC) Application encryption without library optimization impacts performance\nD) KMS table encryption encrypts all data, doesn''t enable selective searching"}'),
       (157, 57, 'AWS Governance Compliance', 'Expert', 'Domain 1: Design Secure Architectures',
        'A regulated financial institution must demonstrate continuous compliance with SOC 2 requirements for data storage. They need automated policy enforcement, compliance reporting, and evidence collection across 50 AWS accounts. Non-compliant resources must be automatically remediated. Which governance framework provides comprehensive compliance automation?',
        '[{"text": "A) AWS Config with custom rules and SNS notifications", "isCorrect": false}, {"text": "B) AWS Organizations with SCPs, Config Conformance Packs, and Systems Manager Automation", "isCorrect": true}, {"text": "C) AWS CloudFormation with compliance templates", "isCorrect": false}, {"text": "D) AWS Security Hub with manual remediation workflows", "isCorrect": false}]',
        'B) AWS Organizations with SCPs, Config Conformance Packs, and Systems Manager Automation',
        'Organizations provides account governance, Conformance Packs enable SOC 2 compliance checks, and Systems Manager Automation handles remediation across all accounts.',
        '{"summary": "Comprehensive compliance automation:", "breakdown": ["Organizations + SCPs: Account-level policy enforcement", "Config Conformance Packs: Pre-built SOC 2 compliance rules", "Systems Manager Automation: Automatic remediation of non-compliant resources", "Centralized reporting: Compliance status across all 50 accounts"], "otherOptions": "A) Config alone lacks account-level governance and automated remediation\nC) CloudFormation provides deployment compliance, not ongoing monitoring\nD) Security Hub requires manual remediation, not automated"}'),
       (158, 58, 'AWS Governance Compliance', 'Application', 'Domain 1: Design Secure Architectures',
        'A global company must ensure S3 buckets across all regions comply with data residency requirements. EU data must stay in EU regions, US data in US regions, with automatic policy enforcement and compliance reporting. Which solution provides the MOST effective governance?',
        '[{"text": "A) IAM policies restricting S3 actions based on user location", "isCorrect": false}, {"text": "B) S3 bucket policies with aws:RequestedRegion condition keys", "isCorrect": false}, {"text": "C) AWS Organizations SCPs with region restrictions based on data classification tags", "isCorrect": true}, {"text": "D) AWS Config rules monitoring bucket creation across regions", "isCorrect": false}]',
        'C) AWS Organizations SCPs with region restrictions based on data classification tags',
        'Organizations SCPs provide account-level enforcement that cannot be overridden, with tag-based conditions enabling automatic data residency compliance.',
        '{"summary": "Data residency governance with SCPs:", "breakdown": ["SCPs: Cannot be overridden by account-level permissions", "Tag-based conditions: Automatically enforce based on data classification", "Region restrictions: Prevent EU data creation in non-EU regions", "Organization-wide: Applies to all current and future accounts"], "otherOptions": "A) IAM policies can be overridden by account administrators\nB) Bucket policies only apply after bucket creation\nD) Config rules detect violations but don''t prevent them"}'),
       (159, 59, 'AWS Storage Resiliency Scalability', 'Expert', 'Domain 2: Design Resilient Architectures',
        'A video streaming platform stores 1PB of content with global distribution requirements. They need 99.99% availability, automatic failover, and the ability to handle 10x traffic spikes during major events. Current single-region S3 architecture shows latency issues for international users. Which architecture provides optimal resiliency and global performance?',
        '[{"text": "A) S3 Cross-Region Replication with CloudFront distribution", "isCorrect": true}, {"text": "B) Multi-region S3 buckets with Route 53 latency-based routing", "isCorrect": false}, {"text": "C) S3 Transfer Acceleration with single-region storage", "isCorrect": false}, {"text": "D) EFS with VPC peering across regions", "isCorrect": false}]',
        'A) S3 Cross-Region Replication with CloudFront distribution',
        'S3 CRR provides data resilience and regional availability, while CloudFront edge locations ensure global low-latency access and can handle traffic spikes automatically.',
        '{"summary": "Global content delivery architecture:", "breakdown": ["S3 CRR: Automatic replication across regions for resilience", "CloudFront: 400+ edge locations for global low-latency delivery", "Auto-scaling: CloudFront handles 10x traffic spikes automatically", "99.99% availability: S3 + CloudFront combined SLA exceeds requirement"], "otherOptions": "B) Route 53 doesn''t provide edge caching for content delivery\nC) Transfer Acceleration only helps uploads, not global content delivery\nD) EFS not designed for 1PB content delivery workloads"}'),
       (160, 60, 'AWS Storage Resiliency Scalability', 'Application', 'Domain 2: Design Resilient Architectures',
        'An e-commerce platform experiences seasonal traffic patterns with 50x increase during holiday sales. Their product catalog and user-generated content storage must scale automatically without performance degradation. Current EBS-based storage architecture requires manual intervention during peaks. Which storage architecture provides automatic scalability?',
        '[{"text": "A) Auto Scaling EBS volumes with CloudWatch metrics", "isCorrect": false}, {"text": "B) Amazon S3 with CloudFront for content delivery and Lambda for processing", "isCorrect": true}, {"text": "C) Amazon EFS with Provisioned Throughput mode", "isCorrect": false}, {"text": "D) Multiple EBS volumes in RAID configuration", "isCorrect": false}]',
        'B) Amazon S3 with CloudFront for content delivery and Lambda for processing',
        'S3 provides unlimited automatic scaling, CloudFront handles traffic spikes through edge caching, and Lambda scales processing automatically with demand.',
        '{"summary": "Auto-scaling storage architecture:", "breakdown": ["S3 unlimited scaling: Handles any amount of data and requests", "CloudFront edge caching: Reduces origin load during traffic spikes", "Lambda auto-scaling: Processing scales from 0 to thousands of concurrent executions", "No manual intervention: All components scale automatically"], "otherOptions": "A) EBS volumes cannot auto-scale capacity\nC) EFS requires pre-provisioned throughput planning\nD) RAID configuration still requires manual capacity planning"}'),
       (161, 61, 'AWS Data Lifecycle', 'Application', 'Domain 4: Design Cost-Optimized Architectures',
        'A research institution stores genomics data with the following access pattern: analyzed intensively for 30 days, occasionally referenced for 6 months, and archived for 10 years for compliance. Current costs are $50,000/month in S3 Standard. Which lifecycle policy provides maximum cost optimization while meeting access requirements?',
        '[{"text": "A) Standard for 30 days → IA at 30 days → Glacier at 180 days → Deep Archive at 1 year", "isCorrect": true}, {"text": "B) Intelligent-Tiering for all data with Deep Archive enabled", "isCorrect": false}, {"text": "C) Standard for 30 days → Glacier immediately at 30 days", "isCorrect": false}, {"text": "D) One Zone-IA for all data to reduce costs", "isCorrect": false}]',
        'A) Standard for 30 days → IA at 30 days → Glacier at 180 days → Deep Archive at 1 year',
        'Lifecycle transitions match access patterns: Standard for intensive analysis, IA for occasional access, Glacier for long-term storage, Deep Archive for compliance.',
        '{"summary": "Optimized lifecycle transitions for research data:", "breakdown": ["Days 1-30: S3 Standard for intensive analysis", "Days 31-180: S3 IA for occasional reference (68% cost reduction)", "Days 181-365: S3 Glacier for backup storage (77% cost reduction)", "Years 1-10: Deep Archive for compliance (80% cost reduction)"], "otherOptions": "B) Intelligent-Tiering adds overhead for predictable access patterns\nC) Early Glacier transition makes occasional access expensive\nD) One Zone-IA lacks resilience for valuable research data"}'),
       (162, 62, 'AWS Data Lifecycle', 'Expert', 'Domain 4: Design Cost-Optimized Architectures',
        'A media company has complex data lifecycle needs: news content (hot for 7 days, cold afterwards), evergreen content (unpredictable access), and archived footage (rare access but immediate retrieval when needed). They want automated optimization without management overhead. Which strategy handles all three patterns optimally?',
        '[{"text": "A) Separate buckets with different lifecycle policies for each content type", "isCorrect": false}, {"text": "B) S3 Intelligent-Tiering with object tagging and tag-based lifecycle policies", "isCorrect": true}, {"text": "C) Manual lifecycle management based on content metadata", "isCorrect": false}, {"text": "D) Single lifecycle policy with shortest transition times", "isCorrect": false}]',
        'B) S3 Intelligent-Tiering with object tagging and tag-based lifecycle policies',
        'Intelligent-Tiering with object tags enables different optimization strategies per content type while maintaining automation and handling unpredictable access patterns.',
        '{"summary": "Tag-based intelligent lifecycle management:", "breakdown": ["Object tagging: Classify content types (news, evergreen, archive)", "Intelligent-Tiering: Automatic optimization for unpredictable evergreen content", "Tag-based policies: Specific rules for news (7-day pattern) and archive content", "No management overhead: All transitions automated based on access patterns"], "otherOptions": "A) Multiple buckets increase management complexity\nC) Manual management doesn''t scale and introduces errors\nD) Single policy can''t optimize for different access patterns"}'),
       (163, 63, 'AWS Storage Cost Management', 'Expert', 'Domain 4: Design Cost-Optimized Architectures',
        'A data analytics company spends $100,000/month on storage across S3, EBS, and EFS for various workloads. They need comprehensive cost optimization while maintaining performance. Analysis shows: 40% rarely accessed, 30% predictable patterns, 20% unpredictable access, 10% high-performance needs. Which multi-service optimization strategy provides maximum savings?',
        '[{"text": "A) Move all data to cheapest storage classes regardless of access patterns", "isCorrect": false}, {"text": "B) S3 lifecycle policies for rarely accessed, Intelligent-Tiering for unpredictable, gp3 optimization for EBS, EFS IA for infrequent file access", "isCorrect": true}, {"text": "C) Consolidate all storage to S3 with single lifecycle policy", "isCorrect": false}, {"text": "D) Implement Reserved Capacity for all storage services", "isCorrect": false}]',
        'B) S3 lifecycle policies for rarely accessed, Intelligent-Tiering for unpredictable, gp3 optimization for EBS, EFS IA for infrequent file access',
        'Tailored optimization per access pattern: lifecycle for predictable patterns, Intelligent-Tiering for unpredictable, gp3 for cost-effective EBS performance, EFS IA for infrequent access.',
        '{"summary": "Multi-service cost optimization strategy:", "breakdown": ["40% rarely accessed: S3 lifecycle to Glacier/Deep Archive (75% savings)", "20% unpredictable: S3 Intelligent-Tiering (40% average savings)", "EBS optimization: gp3 volumes with right-sized IOPS (20% savings)", "EFS optimization: IA storage class for infrequent access (85% savings)"], "otherOptions": "A) Ignoring access patterns can cause performance issues and retrieval costs\nC) Not all workloads suit object storage architecture\nD) Reserved Capacity not available for all storage types and may not match usage patterns"}'),
       (164, 64, 'AWS Storage Cost Management', 'Application', 'Domain 4: Design Cost-Optimized Architectures',
        'A startup''s S3 storage costs have grown to $25,000/month with 80% of objects never accessed after 90 days. They also have high data transfer costs from direct client downloads. The application serves user-uploaded images and documents globally. Which combination provides the most comprehensive cost reduction?',
        '[{"text": "A) S3 lifecycle policy to Glacier and CloudFront distribution", "isCorrect": true}, {"text": "B) S3 Intelligent-Tiering and S3 Transfer Acceleration", "isCorrect": false}, {"text": "C) Move all data to S3 One Zone-IA with direct client access", "isCorrect": false}, {"text": "D) Compress all objects and implement client-side caching", "isCorrect": false}]',
        'A) S3 lifecycle policy to Glacier and CloudFront distribution',
        'Lifecycle policy to Glacier reduces storage costs by 68% for unused objects, while CloudFront eliminates data transfer costs and provides global acceleration.',
        '{"summary": "Comprehensive S3 cost optimization:", "breakdown": ["Lifecycle to Glacier: 68% storage cost reduction for 80% of objects", "CloudFront: Eliminates data transfer costs from S3 to internet", "Global caching: Improves performance while reducing origin costs", "Combined savings: Approximately 70% total cost reduction"], "otherOptions": "B) Intelligent-Tiering adds overhead for predictable 90-day pattern\nC) One Zone-IA lacks resilience and doesn''t address transfer costs\nD) Compression helps but doesn''t address core storage lifecycle and transfer costs"}'),
       (165, 65, 'AWS Storage Comprehensive', 'Expert', 'Domain 3: Design High-Performing Architectures',
        'A large enterprise needs a comprehensive storage strategy for their cloud migration. Requirements include: 500TB database storage with 50,000 IOPS, 2PB file shares for 1000 users, 10PB object storage with global access, and hybrid connectivity to on-premises. Which architecture provides optimal performance, cost, and integration?',
        '[{"text": "A) io2 Block Express EBS, EFS Max I/O, S3 with CloudFront, and Storage Gateway", "isCorrect": true}, {"text": "B) gp3 EBS, FSx for Lustre, S3 Intelligent-Tiering, and Direct Connect", "isCorrect": false}, {"text": "C) Aurora storage, EFS General Purpose, S3 Standard, and VPN connections", "isCorrect": false}, {"text": "D) Instance store, FSx for Windows, S3 Glacier, and AWS Outposts", "isCorrect": false}]',
        'A) io2 Block Express EBS, EFS Max I/O, S3 with CloudFront, and Storage Gateway',
        'io2 Block Express provides required IOPS, EFS Max I/O handles concurrent users, S3+CloudFront scales globally, Storage Gateway enables hybrid integration.',
        '{"summary": "Enterprise storage architecture components:", "breakdown": ["io2 Block Express: 50,000 IOPS with sub-millisecond latency for databases", "EFS Max I/O: Scales to 1000+ concurrent users with higher IOPS", "S3 + CloudFront: Unlimited object storage with global low-latency access", "Storage Gateway: Seamless hybrid connectivity with caching"], "otherOptions": "B) gp3 may not provide consistent 50,000 IOPS, FSx Lustre not ideal for general file shares\nC) Aurora storage limits database choice, VPN insufficient for 500TB+ workloads\nD) Instance store temporary, FSx Windows not mentioned as requirement, Glacier too slow"}'),
       (166, 66, 'AWS Storage Comprehensive', 'Expert', 'Domain 1: Design Secure Architectures',
        'A financial services firm requires end-to-end security for their storage infrastructure handling sensitive customer data across S3, EBS, and EFS. Requirements include: customer-controlled encryption, automated compliance monitoring, data classification, and audit trails meeting regulatory standards. Which comprehensive security architecture meets all requirements?',
        '[{"text": "A) AWS managed encryption, GuardDuty monitoring, and CloudTrail logging", "isCorrect": false}, {"text": "B) Customer-managed KMS keys, Amazon Macie, Config Conformance Packs, and CloudTrail data events", "isCorrect": true}, {"text": "C) CloudHSM encryption, Security Hub, and VPC Flow Logs", "isCorrect": false}, {"text": "D) Client-side encryption, Inspector scanning, and AWS Config rules", "isCorrect": false}]',
        'B) Customer-managed KMS keys, Amazon Macie, Config Conformance Packs, and CloudTrail data events',
        'Customer-managed KMS provides encryption control, Macie classifies sensitive data, Conformance Packs ensure compliance, CloudTrail data events provide complete audit trails.',
        '{"summary": "Comprehensive security architecture for financial services:", "breakdown": ["Customer-managed KMS: Full control over encryption keys across all storage services", "Amazon Macie: Automated discovery and classification of sensitive financial data", "Config Conformance Packs: Continuous compliance monitoring for financial regulations", "CloudTrail data events: Complete audit trail of all data access activities"], "otherOptions": "A) AWS managed encryption lacks customer control required for financial services\nC) CloudHSM overkill, Security Hub doesn''t provide data classification\nD) Client-side encryption complex to manage, Inspector focuses on vulnerabilities not data classification"}');


-- Insert all CompTIA Cloud+ questions
INSERT INTO prepper.comptia_cloud_plus_questions (question_id, question_number, category, difficulty, domain,
                                                  question_text, options, correct_answer, explanation,
                                                  explanation_details)
VALUES (1, 1, 'Cloud Architecture - Service Models', 'Knowledge', 'Domain 1',
        'Which cloud service model provides virtualized computing resources over the internet, allowing users to rent servers, storage, and networking without purchasing physical hardware?',
        '[{"text": "A) Software as a Service (SaaS)", "isCorrect": false}, {"text": "B) Platform as a Service (PaaS)", "isCorrect": false}, {"text": "C) Infrastructure as a Service (IaaS)", "isCorrect": true}, {"text": "D) Function as a Service (FaaS)", "isCorrect": false}]',
        'C) Infrastructure as a Service (IaaS)',
        'IaaS provides virtualized computing resources including servers, storage, and networking infrastructure.',
        '{"summary": "IaaS characteristics include:", "breakdown": ["Virtualized computing resources over the internet", "Users rent infrastructure components rather than buying hardware", "Control over operating systems and applications", "Examples: AWS EC2, Azure VMs, Google Compute Engine"], "otherOptions": "A) SaaS delivers complete software applications\nB) PaaS provides development platforms\nD) FaaS provides serverless function execution"}'),
       (2, 2, 'Cloud Architecture - Service Models', 'Comprehension', 'Domain 1',
        'Your company wants to deploy a web application without managing the underlying servers, operating systems, or runtime environments. Which service model best fits this requirement?',
        '[{"text": "A) Infrastructure as a Service (IaaS)", "isCorrect": false}, {"text": "B) Platform as a Service (PaaS)", "isCorrect": true}, {"text": "C) Software as a Service (SaaS)", "isCorrect": false}, {"text": "D) Function as a Service (FaaS)", "isCorrect": false}]',
        'B) Platform as a Service (PaaS)',
        'PaaS abstracts away infrastructure management while providing a platform for application development and deployment.',
        '{"summary": "PaaS characteristics for this scenario:", "breakdown": ["Eliminates server and OS management overhead", "Provides development tools and runtime environments", "Allows focus on application code and business logic", "Examples: Azure App Service, Google App Engine, Heroku"], "otherOptions": "A) IaaS requires managing servers and OS\nC) SaaS provides complete applications\nD) FaaS is for event-driven functions"}'),
       (3, 3, 'Cloud Architecture - Service Models', 'Application', 'Domain 1',
        'A developer needs to execute code in response to events without managing any infrastructure. The code should automatically scale and only run when triggered. Which service model is most appropriate?',
        '[{"text": "A) Infrastructure as a Service (IaaS)", "isCorrect": false}, {"text": "B) Platform as a Service (PaaS)", "isCorrect": false}, {"text": "C) Software as a Service (SaaS)", "isCorrect": false}, {"text": "D) Function as a Service (FaaS)", "isCorrect": true}]',
        'D) Function as a Service (FaaS)',
        'FaaS allows developers to execute code in response to events without managing infrastructure, with automatic scaling.',
        '{"summary": "FaaS characteristics for this scenario:", "breakdown": ["Event-driven execution model", "No server management required", "Automatic scaling based on demand", "Pay-per-execution pricing model"], "otherOptions": "A) IaaS requires infrastructure management\nB) PaaS still requires some platform management\nC) SaaS provides complete applications, not custom code execution"}'),
       (4, 4, 'Cloud Architecture - Service Models', 'Knowledge', 'Domain 1',
        'Which service model is represented by applications like Salesforce, Office 365, and Gmail?',
        '[{"text": "A) Infrastructure as a Service (IaaS)", "isCorrect": false}, {"text": "B) Platform as a Service (PaaS)", "isCorrect": false}, {"text": "C) Software as a Service (SaaS)", "isCorrect": true}, {"text": "D) Function as a Service (FaaS)", "isCorrect": false}]',
        'C) Software as a Service (SaaS)',
        'SaaS delivers complete software applications over the internet that users access through web browsers.',
        '{"summary": "SaaS characteristics:", "breakdown": ["Complete software applications delivered over internet", "No software installation or maintenance required", "Subscription-based pricing model", "Multi-tenant architecture"], "otherOptions": "A) IaaS provides infrastructure components\nB) PaaS provides development platforms\nD) FaaS provides serverless function execution"}'),
       (5, 5, 'Cloud Architecture - Shared Responsibility', 'Comprehension', 'Domain 1',
        'In the shared responsibility model, a customer using Amazon RDS (managed database service) is responsible for which of the following?',
        '[{"text": "A) Database engine patching and updates", "isCorrect": false}, {"text": "B) Physical security of the data center", "isCorrect": false}, {"text": "C) Data encryption and access control configuration", "isCorrect": true}, {"text": "D) Hardware maintenance and replacement", "isCorrect": false}]',
        'C) Data encryption and access control configuration',
        'In managed services like RDS, customers are responsible for data security, access controls, and encryption configuration.',
        '{"summary": "Customer responsibilities in managed database services:", "breakdown": ["Data encryption at rest and in transit", "User access management and IAM policies", "Network security groups and firewall rules", "Backup retention and recovery testing"], "otherOptions": "A) AWS manages engine patching\nB) AWS handles physical security\nD) AWS manages hardware infrastructure"}'),
       (6, 6, 'Cloud Architecture - Shared Responsibility', 'Application', 'Domain 1',
        'Your organization is using IaaS virtual machines. According to the shared responsibility model, which security aspect is the customer responsible for?',
        '[{"text": "A) Physical security of the data center", "isCorrect": false}, {"text": "B) Hypervisor security and maintenance", "isCorrect": false}, {"text": "C) Operating system patches and configuration", "isCorrect": true}, {"text": "D) Network infrastructure hardware security", "isCorrect": false}]',
        'C) Operating system patches and configuration',
        'In IaaS, customers are responsible for securing the guest operating system, including patching and configuration.',
        '{"summary": "Customer responsibilities in IaaS:", "breakdown": ["Guest operating system security and patching", "Application security and configuration", "Network traffic protection (encryption)", "Identity and access management"], "otherOptions": "A) Physical security is provider responsibility\nB) Hypervisor is managed by cloud provider\nD) Network hardware is provider responsibility"}'),
       (7, 7, 'Cloud Architecture - Availability', 'Application', 'Domain 1',
        'Your application requires 99.99% uptime and must survive the failure of an entire data center. Which architecture approach best meets these requirements?',
        '[{"text": "A) Deploy in a single availability zone with multiple instances", "isCorrect": false}, {"text": "B) Deploy across multiple availability zones in the same region", "isCorrect": true}, {"text": "C) Use larger instance types for better reliability", "isCorrect": false}, {"text": "D) Implement only vertical scaling", "isCorrect": false}]',
        'B) Deploy across multiple availability zones in the same region',
        'Multi-AZ deployment provides data center-level fault tolerance while maintaining low latency.',
        '{"summary": "Multi-AZ deployment benefits:", "breakdown": ["Survives entire data center failures", "Maintains low latency within region", "Automatic failover capabilities", "Meets high availability requirements (99.99%)"], "otherOptions": "A) Single AZ cannot survive data center failure\nC) Instance size doesn''t address availability zones\nD) Vertical scaling doesn''t provide fault tolerance"}'),
       (8, 8, 'Cloud Architecture - Availability', 'Knowledge', 'Domain 1',
        'What is the primary purpose of availability zones in cloud computing?',
        '[{"text": "A) To provide different pricing tiers", "isCorrect": false}, {"text": "B) To separate different cloud services", "isCorrect": false}, {"text": "C) To provide redundancy and fault tolerance", "isCorrect": true}, {"text": "D) To comply with data sovereignty requirements", "isCorrect": false}]',
        'C) To provide redundancy and fault tolerance',
        'Availability zones provide redundancy and fault tolerance by isolating failures to specific geographic locations.',
        '{"summary": "Availability zone characteristics:", "breakdown": ["Isolated data center locations within a region", "Independent power, cooling, and networking", "Designed to prevent cascading failures", "Enable high availability architecture design"], "otherOptions": "A) Pricing is not determined by AZ\nB) Services can span multiple AZs\nD) Data sovereignty is handled at region level"}'),
       (9, 9, 'Cloud Architecture - Availability', 'Comprehension', 'Domain 1',
        'Which cloud strategy allows an organization to handle sudden traffic spikes by temporarily using public cloud resources while maintaining their private cloud for normal operations?',
        '[{"text": "A) Multi-cloud deployment", "isCorrect": false}, {"text": "B) Hybrid cloud architecture", "isCorrect": false}, {"text": "C) Cloud bursting", "isCorrect": true}, {"text": "D) Edge computing", "isCorrect": false}]',
        'C) Cloud bursting',
        'Cloud bursting allows organizations to scale from private to public cloud during peak demand periods.',
        '{"summary": "Cloud bursting characteristics:", "breakdown": ["Temporary use of public cloud resources", "Handles unexpected traffic spikes", "Cost-effective scaling approach", "Maintains private cloud for normal operations"], "otherOptions": "A) Multi-cloud uses multiple providers simultaneously\nB) Hybrid cloud is permanent architecture\nD) Edge computing brings processing closer to users"}'),
       (10, 10, 'Cloud Architecture - Storage', 'Knowledge', 'Domain 1',
        'Which storage type is best suited for frequently accessed data requiring low latency and high throughput?',
        '[{"text": "A) Cold storage tier", "isCorrect": false}, {"text": "B) Archive storage tier", "isCorrect": false}, {"text": "C) Hot storage tier", "isCorrect": true}, {"text": "D) Warm storage tier", "isCorrect": false}]',
        'C) Hot storage tier',
        'Hot storage tier is optimized for frequently accessed data with low latency requirements.',
        '{"summary": "Hot storage characteristics:", "breakdown": ["Optimized for frequent access patterns", "Provides low latency data retrieval", "Higher cost but better performance", "Ideal for production application data"], "otherOptions": "A) Cold storage for infrequent access\nB) Archive for long-term retention\nD) Warm storage for moderate access"}'),
       (11, 11, 'Cloud Architecture - Storage', 'Comprehension', 'Domain 1',
        'An application requires high IOPS for database operations and low-level access to storage blocks. Which storage combination is most appropriate?',
        '[{"text": "A) Object storage with HDD", "isCorrect": false}, {"text": "B) File storage with SSD", "isCorrect": false}, {"text": "C) Block storage with SSD", "isCorrect": true}, {"text": "D) Object storage with SSD", "isCorrect": false}]',
        'C) Block storage with SSD',
        'Block storage provides low-level access ideal for databases, while SSDs deliver the high IOPS required.',
        '{"summary": "Block storage with SSD advantages:", "breakdown": ["Block-level access optimal for database workloads", "SSD provides high IOPS and low latency", "Direct attachment to compute instances", "Suitable for transactional applications"], "otherOptions": "A) Object storage lacks low-level access; HDD has lower IOPS\nB) File storage not optimal for databases\nD) Object storage doesn''t provide block-level access"}'),
       (12, 12, 'Cloud Architecture - Storage', 'Application', 'Domain 1',
        'Your organization needs to store 100TB of archived data that is accessed once per year for compliance purposes. Which storage solution offers the best cost optimization?',
        '[{"text": "A) Hot storage tier", "isCorrect": false}, {"text": "B) Warm storage tier", "isCorrect": false}, {"text": "C) Cold storage tier", "isCorrect": false}, {"text": "D) Archive storage tier", "isCorrect": true}]',
        'D) Archive storage tier',
        'Archive storage tier is designed for long-term retention of rarely accessed data with lowest cost.',
        '{"summary": "Archive storage characteristics:", "breakdown": ["Lowest cost storage option", "Designed for long-term retention", "Higher retrieval times and costs", "Ideal for compliance and backup data"], "otherOptions": "A) Hot storage too expensive for rarely accessed data\nB) Warm storage still higher cost than needed\nC) Cold storage more expensive than archive"}'),
       (13, 13, 'Cloud Architecture - Network Components', 'Knowledge', 'Domain 1',
        'Which network component helps reduce latency for global users by caching content at edge locations closest to them?',
        '[{"text": "A) Application load balancer", "isCorrect": false}, {"text": "B) Network load balancer", "isCorrect": false}, {"text": "C) Content Delivery Network (CDN)", "isCorrect": true}, {"text": "D) Application gateway", "isCorrect": false}]',
        'C) Content Delivery Network (CDN)',
        'CDNs distribute content across geographically dispersed servers to minimize latency for end users.',
        '{"summary": "CDN functionality:", "breakdown": ["Caches static content at edge locations globally", "Routes requests to nearest geographic server", "Reduces bandwidth usage and server load", "Improves website performance and user experience"], "otherOptions": "A) ALB distributes traffic to backend servers\nB) NLB handles network layer traffic\nD) Application gateway provides secure access"}'),
       (14, 14, 'Cloud Architecture - Network Components', 'Comprehension', 'Domain 1',
        'What is the primary difference between an application load balancer and a network load balancer?',
        '[{"text": "A) Application load balancer works at Layer 7, network load balancer at Layer 4", "isCorrect": true}, {"text": "B) Network load balancer is more expensive than application load balancer", "isCorrect": false}, {"text": "C) Application load balancer only works with HTTP, network load balancer with HTTPS", "isCorrect": false}, {"text": "D) Network load balancer provides SSL termination, application load balancer does not", "isCorrect": false}]',
        'A) Application load balancer works at Layer 7, network load balancer at Layer 4',
        'Application load balancers operate at Layer 7 (application layer) while network load balancers operate at Layer 4 (transport layer).',
        '{"summary": "Load balancer layer differences:", "breakdown": ["ALB: Layer 7 - can route based on HTTP headers, URLs, cookies", "NLB: Layer 4 - routes based on IP protocol data", "ALB: Content-based routing capabilities", "NLB: Higher performance, lower latency"], "otherOptions": "B) Pricing varies by provider and usage\nC) Both can handle HTTP and HTTPS\nD) Both can provide SSL termination"}'),
       (15, 15, 'Cloud Architecture - Network Components', 'Application', 'Domain 1',
        'Your web application needs to route traffic based on URL paths (/api/* to API servers, /images/* to image servers). Which load balancer type should you use?',
        '[{"text": "A) Network load balancer", "isCorrect": false}, {"text": "B) Application load balancer", "isCorrect": true}, {"text": "C) Classic load balancer", "isCorrect": false}, {"text": "D) Gateway load balancer", "isCorrect": false}]',
        'B) Application load balancer',
        'Application load balancers can route traffic based on URL paths, headers, and other application-layer information.',
        '{"summary": "Application load balancer routing capabilities:", "breakdown": ["Path-based routing (/api, /images, etc.)", "Header-based routing", "Query parameter-based routing", "Cookie-based routing"], "otherOptions": "A) Network load balancer routes at Layer 4, cannot inspect URLs\nC) Classic load balancer has limited routing capabilities\nD) Gateway load balancer is for traffic inspection"}'),
       (16, 16, 'Cloud Architecture - Disaster Recovery', 'Knowledge', 'Domain 1',
        'What does RTO (Recovery Time Objective) represent in disaster recovery planning?',
        '[{"text": "A) The amount of data that can be lost during a disaster", "isCorrect": false}, {"text": "B) The maximum acceptable downtime after a disaster", "isCorrect": true}, {"text": "C) The cost of implementing disaster recovery", "isCorrect": false}, {"text": "D) The frequency of disaster recovery testing", "isCorrect": false}]',
        'B) The maximum acceptable downtime after a disaster',
        'RTO defines the maximum acceptable duration within which a system must be restored after a disruption.',
        '{"summary": "RTO characteristics:", "breakdown": ["Maximum acceptable downtime duration", "Measured in hours, minutes, or seconds", "Drives disaster recovery strategy selection", "Affects cost and complexity of DR solutions"], "otherOptions": "A) That describes RPO (Recovery Point Objective)\nC) Cost is a factor but not what RTO measures\nD) Testing frequency is separate from RTO"}'),
       (17, 17, 'Cloud Architecture - Disaster Recovery', 'Comprehension', 'Domain 1',
        'What is the difference between RPO and RTO in disaster recovery?',
        '[{"text": "A) RPO is about data loss, RTO is about downtime", "isCorrect": true}, {"text": "B) RPO is about downtime, RTO is about data loss", "isCorrect": false}, {"text": "C) RPO and RTO both measure downtime but in different units", "isCorrect": false}, {"text": "D) RPO is for applications, RTO is for infrastructure", "isCorrect": false}]',
        'A) RPO is about data loss, RTO is about downtime',
        'RPO (Recovery Point Objective) measures acceptable data loss, while RTO (Recovery Time Objective) measures acceptable downtime.',
        '{"summary": "RPO vs RTO:", "breakdown": ["RPO: Maximum tolerable data loss (measured in time)", "RTO: Maximum tolerable downtime", "RPO drives backup frequency requirements", "RTO drives disaster recovery strategy selection"], "otherOptions": "B) This reverses the definitions\nC) They measure different aspects of recovery\nD) Both apply to applications and infrastructure"}'),
       (18, 18, 'Cloud Architecture - Disaster Recovery', 'Application', 'Domain 1',
        'Your organization has an RTO of 2 hours and RPO of 30 minutes for a critical application. Which disaster recovery strategy best meets these requirements?',
        '[{"text": "A) Cold site with daily backups", "isCorrect": false}, {"text": "B) Warm site with automated failover", "isCorrect": true}, {"text": "C) Hot site with real-time replication", "isCorrect": false}, {"text": "D) Backup and restore only", "isCorrect": false}]',
        'B) Warm site with automated failover',
        'Warm site provides the right balance of cost and recovery time to meet 2-hour RTO requirements.',
        '{"summary": "Warm site characteristics for this scenario:", "breakdown": ["Can achieve 2-hour RTO with quick startup", "30-minute RPO achievable with frequent backups", "More cost-effective than hot site", "Automated failover reduces manual intervention"], "otherOptions": "A) Cold site takes too long for 2-hour RTO\nC) Hot site exceeds requirements and increases cost\nD) Backup/restore cannot meet 2-hour RTO"}'),
       (19, 19, 'Cloud Architecture - Multicloud', 'Application', 'Domain 1',
        'Your organization wants to avoid vendor lock-in while leveraging best-of-breed services from multiple cloud providers. What strategy should you implement?',
        '[{"text": "A) Single cloud deployment with multiple regions", "isCorrect": false}, {"text": "B) Multicloud tenancy strategy", "isCorrect": true}, {"text": "C) Hybrid cloud with on-premises integration", "isCorrect": false}, {"text": "D) Private cloud deployment only", "isCorrect": false}]',
        'B) Multicloud tenancy strategy',
        'Multicloud tenancy allows using services from multiple providers, avoiding vendor lock-in while accessing optimal services.',
        '{"summary": "Multicloud tenancy benefits:", "breakdown": ["Avoids dependency on a single cloud provider", "Enables selection of best services from each provider", "Provides redundancy and improved reliability", "Allows cost optimization through competitive pricing"], "otherOptions": "A) Single cloud still creates vendor dependency\nC) Hybrid focuses on on-premises integration\nD) Private cloud limits service options"}'),
       (20, 20, 'Cloud Architecture - Multicloud', 'Comprehension', 'Domain 1',
        'Which scenario best represents a valid use case for multicloud strategy?',
        '[{"text": "A) Using AWS for compute and Azure for AI/ML services", "isCorrect": true}, {"text": "B) Using the same cloud provider in multiple regions", "isCorrect": false}, {"text": "C) Using private cloud for all applications", "isCorrect": false}, {"text": "D) Using on-premises servers with cloud storage", "isCorrect": false}]',
        'A) Using AWS for compute and Azure for AI/ML services',
        'Multicloud involves using different cloud providers for different services based on their strengths.',
        '{"summary": "Multicloud strategy benefits:", "breakdown": ["Best-of-breed service selection", "Avoid vendor lock-in", "Improved negotiating position", "Reduced single point of failure risk"], "otherOptions": "B) Multiple regions with same provider is not multicloud\nC) Private cloud only is not multicloud\nD) Hybrid cloud, not multicloud"}'),
       (21, 21, 'Deployments - Migration Types', 'Comprehension', 'Domain 2',
        'Which migration strategy involves moving applications to the cloud with minimal changes, often called "lift and shift"?',
        '[{"text": "A) Rehosting", "isCorrect": true}, {"text": "B) Refactoring", "isCorrect": false}, {"text": "C) Rearchitecting", "isCorrect": false}, {"text": "D) Rebuilding", "isCorrect": false}]',
        'A) Rehosting', 'Rehosting (lift and shift) moves applications to cloud with minimal modification.',
        '{"summary": "Rehosting characteristics:", "breakdown": ["Minimal application changes required", "Fastest migration approach", "Lower initial cost and complexity", "May not fully utilize cloud benefits"], "otherOptions": "B) Refactoring modifies applications for cloud\nC) Rearchitecting redesigns for cloud-native\nD) Rebuilding creates new applications"}'),
       (22, 22, 'Deployments - Migration Types', 'Application', 'Domain 2',
        'Your legacy application needs significant changes to take advantage of cloud-native features like auto-scaling and managed services. Which migration approach is most appropriate?',
        '[{"text": "A) Rehosting", "isCorrect": false}, {"text": "B) Refactoring", "isCorrect": true}, {"text": "C) Retiring", "isCorrect": false}, {"text": "D) Retaining", "isCorrect": false}]',
        'B) Refactoring',
        'Refactoring involves modifying applications to take advantage of cloud-native features and services.',
        '{"summary": "Refactoring characteristics:", "breakdown": ["Modifies applications for cloud optimization", "Enables use of managed services", "Improves scalability and performance", "Higher effort than rehosting but better ROI"], "otherOptions": "A) Rehosting doesn''t modify applications\nC) Retiring eliminates the application\nD) Retaining keeps app on-premises"}'),
       (23, 23, 'Deployments - Migration Types', 'Knowledge', 'Domain 2',
        'Which migration strategy involves completely redesigning an application to be cloud-native from the ground up?',
        '[{"text": "A) Rehosting", "isCorrect": false}, {"text": "B) Refactoring", "isCorrect": false}, {"text": "C) Rearchitecting", "isCorrect": true}, {"text": "D) Replacing", "isCorrect": false}]',
        'C) Rearchitecting',
        'Rearchitecting involves completely redesigning applications to be cloud-native and take full advantage of cloud capabilities.',
        '{"summary": "Rearchitecting characteristics:", "breakdown": ["Complete application redesign", "Full utilization of cloud-native features", "Highest development effort and cost", "Maximum long-term benefits and flexibility"], "otherOptions": "A) Rehosting moves with minimal changes\nB) Refactoring makes modifications but not complete redesign\nD) Replacing uses different software"}'),
       (24, 24, 'Deployments - Infrastructure as Code', 'Application', 'Domain 2',
        'Your team needs to deploy identical environments across development, staging, and production. Which approach ensures consistency and reduces manual errors?',
        '[{"text": "A) Manual deployment through cloud console", "isCorrect": false}, {"text": "B) Infrastructure as Code (IaC) templates", "isCorrect": true}, {"text": "C) Copying virtual machine images", "isCorrect": false}, {"text": "D) Using different configurations for each environment", "isCorrect": false}]',
        'B) Infrastructure as Code (IaC) templates',
        'IaC templates ensure consistent, repeatable, and version-controlled infrastructure deployments.',
        '{"summary": "IaC benefits for environment consistency:", "breakdown": ["Version-controlled infrastructure definitions", "Automated and repeatable deployments", "Eliminates configuration drift", "Enables testing of infrastructure changes"], "otherOptions": "A) Manual processes are error-prone\nC) VM images don''t cover full infrastructure\nD) Different configs create inconsistency"}'),
       (25, 25, 'Deployments - Infrastructure as Code', 'Comprehension', 'Domain 2',
        'What is the primary benefit of using declarative Infrastructure as Code templates?',
        '[{"text": "A) Faster execution than imperative scripts", "isCorrect": false}, {"text": "B) Describes desired end state rather than step-by-step instructions", "isCorrect": true}, {"text": "C) Requires less storage space than imperative code", "isCorrect": false}, {"text": "D) Works only with specific cloud providers", "isCorrect": false}]',
        'B) Describes desired end state rather than step-by-step instructions',
        'Declarative IaC describes the desired end state, allowing the system to determine how to achieve it.',
        '{"summary": "Declarative IaC benefits:", "breakdown": ["Describes desired infrastructure state", "System determines implementation steps", "Idempotent operations (safe to run multiple times)", "Easier to understand and maintain"], "otherOptions": "A) Execution speed depends on implementation\nC) Storage size not a primary consideration\nD) Many tools work across multiple providers"}'),
       (26, 26, 'Deployments - Infrastructure as Code', 'Knowledge', 'Domain 2',
        'Which of the following is a key characteristic of Infrastructure as Code (IaC)?',
        '[{"text": "A) Infrastructure is managed manually through web consoles", "isCorrect": false}, {"text": "B) Infrastructure is defined in code and version controlled", "isCorrect": true}, {"text": "C) Infrastructure changes require physical hardware installation", "isCorrect": false}, {"text": "D) Infrastructure is managed by third-party vendors only", "isCorrect": false}]',
        'B) Infrastructure is defined in code and version controlled',
        'IaC treats infrastructure as code that can be version controlled, tested, and deployed programmatically.',
        '{"summary": "IaC key characteristics:", "breakdown": ["Infrastructure defined in code files", "Version control for infrastructure changes", "Automated deployment and provisioning", "Repeatable and consistent deployments"], "otherOptions": "A) IaC eliminates manual console management\nC) IaC works with virtual/cloud infrastructure\nD) IaC can be managed by internal teams"}'),
       (27, 27, 'Deployments - Deployment Strategies', 'Application', 'Domain 2',
        'Your application needs zero-downtime deployment with the ability to quickly rollback if issues occur. Which deployment strategy is most appropriate?',
        '[{"text": "A) Blue-green deployment", "isCorrect": true}, {"text": "B) In-place deployment", "isCorrect": false}, {"text": "C) Rolling deployment", "isCorrect": false}, {"text": "D) Recreate deployment", "isCorrect": false}]',
        'A) Blue-green deployment',
        'Blue-green deployment provides zero downtime and instant rollback capabilities by maintaining two identical environments.',
        '{"summary": "Blue-green deployment characteristics:", "breakdown": ["Two identical production environments", "Instant traffic switching between environments", "Zero downtime during deployment", "Quick rollback by switching traffic back"], "otherOptions": "B) In-place deployment causes downtime\nC) Rolling deployment has slower rollback\nD) Recreate deployment causes downtime"}'),
       (28, 28, 'Deployments - Deployment Strategies', 'Comprehension', 'Domain 2',
        'What is the primary advantage of canary deployment strategy?',
        '[{"text": "A) Fastest deployment method", "isCorrect": false}, {"text": "B) Lowest resource requirements", "isCorrect": false}, {"text": "C) Limits impact of issues to small user subset", "isCorrect": true}, {"text": "D) Simplest to implement", "isCorrect": false}]',
        'C) Limits impact of issues to small user subset',
        'Canary deployment gradually releases changes to small user groups, limiting the impact of potential issues.',
        '{"summary": "Canary deployment benefits:", "breakdown": ["Gradual rollout to subset of users", "Early detection of issues with limited impact", "Ability to monitor and validate changes", "Reduced risk of widespread problems"], "otherOptions": "A) Not the fastest method\nB) Requires additional monitoring infrastructure\nD) More complex than simple deployments"}'),
       (29, 29, 'Deployments - Deployment Strategies', 'Knowledge', 'Domain 2',
        'In a rolling deployment strategy, how are application instances updated?',
        '[{"text": "A) All instances are updated simultaneously", "isCorrect": false}, {"text": "B) Instances are updated one at a time or in small batches", "isCorrect": true}, {"text": "C) New instances are created while old ones remain", "isCorrect": false}, {"text": "D) All instances are shut down before updates", "isCorrect": false}]',
        'B) Instances are updated one at a time or in small batches',
        'Rolling deployment updates instances gradually, one at a time or in small batches, maintaining service availability.',
        '{"summary": "Rolling deployment characteristics:", "breakdown": ["Gradual instance updates", "Maintains service availability", "Lower resource requirements than blue-green", "Slower rollback process"], "otherOptions": "A) That describes in-place deployment\nC) That describes blue-green deployment\nD) That would cause service interruption"}'),
       (30, 30, 'Deployments - CI/CD', 'Application', 'Domain 2',
        'Your development team wants to automatically deploy code changes to production after passing all tests in the staging environment. Which CI/CD practice should be implemented?',
        '[{"text": "A) Continuous Integration only", "isCorrect": false}, {"text": "B) Continuous Delivery only", "isCorrect": false}, {"text": "C) Continuous Deployment", "isCorrect": true}, {"text": "D) Manual deployment with CI", "isCorrect": false}]',
        'C) Continuous Deployment',
        'Continuous Deployment automatically releases code changes to production after passing all pipeline stages.',
        '{"summary": "Continuous Deployment characteristics:", "breakdown": ["Fully automated pipeline from code to production", "No manual intervention required for deployment", "Requires robust testing and monitoring", "Enables rapid feature delivery to users"], "otherOptions": "A) CI only handles code integration\nB) CD deploys to staging but requires manual production release\nD) Manual deployment contradicts automation goals"}'),
       (31, 31, 'Operations - Scaling', 'Application', 'Domain 3',
        'Your web application experiences predictable traffic spikes every weekday from 9 AM to 5 PM. Which scaling approach would be most cost-effective?',
        '[{"text": "A) Vertical scaling with larger instances", "isCorrect": false}, {"text": "B) Horizontal auto-scaling based on schedule and metrics", "isCorrect": true}, {"text": "C) Manual scaling during business hours", "isCorrect": false}, {"text": "D) Keeping maximum capacity running at all times", "isCorrect": false}]',
        'B) Horizontal auto-scaling based on schedule and metrics',
        'Scheduled auto-scaling with metric triggers optimizes both cost and performance for predictable patterns.',
        '{"summary": "Auto-scaling advantages for predictable traffic:", "breakdown": ["Proactive scaling before traffic spikes", "Automatic scale-down during off-hours", "Combines scheduled and reactive scaling", "Optimizes cost while maintaining performance"], "otherOptions": "A) Vertical scaling requires downtime\nC) Manual scaling is reactive and error-prone\nD) Maximum capacity wastes money during off-hours"}'),
       (32, 32, 'Operations - Scaling', 'Comprehension', 'Domain 3',
        'What is the primary difference between horizontal and vertical scaling?',
        '[{"text": "A) Horizontal adds more instances, vertical increases instance size", "isCorrect": true}, {"text": "B) Horizontal is cheaper, vertical is more expensive", "isCorrect": false}, {"text": "C) Horizontal is automatic, vertical is manual", "isCorrect": false}, {"text": "D) Horizontal works with databases, vertical works with web servers", "isCorrect": false}]',
        'A) Horizontal adds more instances, vertical increases instance size',
        'Horizontal scaling adds more instances (scale out), while vertical scaling increases the capacity of existing instances (scale up).',
        '{"summary": "Scaling approach differences:", "breakdown": ["Horizontal: Scale out - add more instances", "Vertical: Scale up - increase instance capacity", "Horizontal: Better for distributed applications", "Vertical: Simpler but has hardware limits"], "otherOptions": "B) Cost depends on specific implementation\nC) Both can be automated\nD) Both work with various application types"}'),
       (33, 33, 'Operations - Scaling', 'Knowledge', 'Domain 3',
        'Which scaling trigger would be most appropriate for a CPU-intensive application?',
        '[{"text": "A) Memory utilization", "isCorrect": false}, {"text": "B) Network bandwidth", "isCorrect": false}, {"text": "C) CPU utilization", "isCorrect": true}, {"text": "D) Storage capacity", "isCorrect": false}]',
        'C) CPU utilization',
        'CPU-intensive applications should scale based on CPU utilization metrics to ensure adequate processing power.',
        '{"summary": "Scaling trigger selection:", "breakdown": ["CPU utilization for compute-intensive workloads", "Memory utilization for memory-intensive applications", "Network bandwidth for high-throughput applications", "Custom metrics for application-specific needs"], "otherOptions": "A) Memory not primary bottleneck for CPU-intensive apps\nB) Network may not be bottleneck\nD) Storage not relevant for CPU-intensive scaling"}'),
       (34, 34, 'Operations - Monitoring', 'Knowledge', 'Domain 3',
        'Which type of monitoring provides insights into application performance and user experience?',
        '[{"text": "A) Infrastructure monitoring", "isCorrect": false}, {"text": "B) Application Performance Monitoring (APM)", "isCorrect": true}, {"text": "C) Network monitoring", "isCorrect": false}, {"text": "D) Security monitoring", "isCorrect": false}]',
        'B) Application Performance Monitoring (APM)',
        'APM focuses on application behavior, response times, and user experience metrics.',
        '{"summary": "APM monitoring includes:", "breakdown": ["Application response times and throughput", "Error rates and exception tracking", "User experience and transaction traces", "Code-level performance insights"], "otherOptions": "A) Infrastructure monitors servers and resources\nC) Network monitoring focuses on connectivity\nD) Security monitoring tracks threats and vulnerabilities"}'),
       (35, 35, 'Operations - Monitoring', 'Comprehension', 'Domain 3',
        'What is the primary purpose of distributed tracing in microservices architecture?',
        '[{"text": "A) Monitor individual service performance", "isCorrect": false}, {"text": "B) Track requests across multiple services", "isCorrect": true}, {"text": "C) Collect application logs", "isCorrect": false}, {"text": "D) Measure network latency", "isCorrect": false}]',
        'B) Track requests across multiple services',
        'Distributed tracing tracks request paths through multiple services, identifying bottlenecks and failures.',
        '{"summary": "Distributed tracing benefits:", "breakdown": ["Visualizes request journey across microservices", "Identifies latency bottlenecks in service chain", "Correlates spans across distributed components", "Enables root cause analysis for performance issues"], "otherOptions": "A) That''s service-level monitoring\nC) That''s log aggregation\nD) That''s network monitoring"}'),
       (36, 36, 'Operations - Monitoring', 'Application', 'Domain 3',
        'Your application is experiencing intermittent performance issues. Which observability practice would best help trace the request flow through your microservices architecture?',
        '[{"text": "A) Log aggregation", "isCorrect": false}, {"text": "B) Metrics monitoring", "isCorrect": false}, {"text": "C) Distributed tracing", "isCorrect": true}, {"text": "D) Alert configuration", "isCorrect": false}]',
        'C) Distributed tracing',
        'Distributed tracing tracks request paths through multiple services, identifying bottlenecks and failures.',
        '{"summary": "Distributed tracing advantages:", "breakdown": ["Visualizes request journey across microservices", "Identifies latency bottlenecks in service chain", "Correlates spans across distributed components", "Enables root cause analysis for performance issues"], "otherOptions": "A) Logs provide events but not request flow\nB) Metrics show performance but not trace paths\nD) Alerts notify of issues but don''t trace flow"}'),
       (37, 37, 'Operations - Backup Strategies', 'Knowledge', 'Domain 3',
        'Which backup type only captures changes made since the last full backup?',
        '[{"text": "A) Full backup", "isCorrect": false}, {"text": "B) Incremental backup", "isCorrect": false}, {"text": "C) Differential backup", "isCorrect": true}, {"text": "D) Snapshot backup", "isCorrect": false}]',
        'C) Differential backup',
        'Differential backups capture all changes since the last full backup, not since the last backup of any type.',
        '{"summary": "Differential backup characteristics:", "breakdown": ["Captures changes since last full backup", "Faster than full backup, slower than incremental", "Requires only full backup + latest differential for restore", "Size grows until next full backup"], "otherOptions": "A) Full backup captures everything\nB) Incremental captures changes since last backup\nD) Snapshot is point-in-time image"}'),
       (38, 38, 'Operations - Backup Strategies', 'Application', 'Domain 3',
        'Your organization has an RPO of 4 hours for a critical database. The database receives constant updates throughout business hours. Which backup strategy best meets this requirement?',
        '[{"text": "A) Daily full backups at midnight", "isCorrect": false}, {"text": "B) Weekly full backups with daily differentials", "isCorrect": false}, {"text": "C) Full backup weekly with 4-hour incremental backups", "isCorrect": true}, {"text": "D) Monthly full backups with weekly incrementals", "isCorrect": false}]',
        'C) Full backup weekly with 4-hour incremental backups',
        'Incremental backups every 4 hours ensure data loss is limited to the RPO requirement of 4 hours.',
        '{"summary": "Backup strategy for 4-hour RPO:", "breakdown": ["Incremental backups every 4 hours meet RPO exactly", "Weekly full backups provide baseline restore point", "Captures all changes within acceptable data loss window", "Balances storage efficiency with recovery requirements"], "otherOptions": "A) Daily backups allow up to 24 hours data loss\nB) Daily differentials still allow 24 hours data loss\nD) Weekly incrementals allow up to 7 days data loss"}'),
       (39, 39, 'Operations - Backup Strategies', 'Comprehension', 'Domain 3',
        'What is the primary advantage of incremental backups over full backups?',
        '[{"text": "A) Faster restore times", "isCorrect": false}, {"text": "B) Better data compression", "isCorrect": false}, {"text": "C) Less storage space and faster backup time", "isCorrect": true}, {"text": "D) Higher reliability", "isCorrect": false}]',
        'C) Less storage space and faster backup time',
        'Incremental backups only capture changes since the last backup, requiring less storage and time.',
        '{"summary": "Incremental backup advantages:", "breakdown": ["Only backs up changed data since last backup", "Significantly less storage space required", "Faster backup execution time", "Reduced network bandwidth usage"], "otherOptions": "A) Restore times are actually slower\nB) Compression depends on backup software\nD) Reliability depends on backup chain integrity"}'),
       (40, 40, 'Security - IAM', 'Application', 'Domain 4',
        'A developer needs temporary access to debug a production issue in a specific S3 bucket. What is the most secure approach following the principle of least privilege?',
        '[{"text": "A) Add developer to administrators group", "isCorrect": false}, {"text": "B) Create IAM user with permanent S3 full access", "isCorrect": false}, {"text": "C) Create time-limited IAM role with bucket-specific permissions", "isCorrect": true}, {"text": "D) Share root account credentials", "isCorrect": false}]',
        'C) Create time-limited IAM role with bucket-specific permissions',
        'Time-limited IAM roles with specific permissions minimize security exposure while providing necessary access.',
        '{"summary": "Secure temporary access principles:", "breakdown": ["Time-bound access that automatically expires", "Scope limited to specific resources needed", "No permanent credentials to manage", "Audit trail of role assumption"], "otherOptions": "A) Administrative access violates least privilege\nB) Permanent access creates long-term security risk\nD) Root credentials should never be shared"}'),
       (41, 41, 'Security - IAM', 'Comprehension', 'Domain 4',
        'What is the primary difference between Role-Based Access Control (RBAC) and Attribute-Based Access Control (ABAC)?',
        '[{"text": "A) RBAC uses job functions, ABAC uses multiple attributes", "isCorrect": true}, {"text": "B) RBAC is more secure than ABAC", "isCorrect": false}, {"text": "C) RBAC works with cloud, ABAC works with on-premises", "isCorrect": false}, {"text": "D) RBAC is newer technology than ABAC", "isCorrect": false}]',
        'A) RBAC uses job functions, ABAC uses multiple attributes',
        'RBAC assigns permissions based on job roles, while ABAC uses multiple attributes like location, time, and resource sensitivity.',
        '{"summary": "RBAC vs ABAC comparison:", "breakdown": ["RBAC: Access based on predefined roles", "ABAC: Access based on multiple dynamic attributes", "RBAC: Simpler to implement and manage", "ABAC: More flexible and granular control"], "otherOptions": "B) Security depends on implementation\nC) Both work in cloud and on-premises\nD) ABAC is actually newer than RBAC"}'),
       (42, 42, 'Security - IAM', 'Knowledge', 'Domain 4',
        'Which authentication method provides the highest level of security for cloud access?',
        '[{"text": "A) Username and password only", "isCorrect": false}, {"text": "B) Multi-factor authentication (MFA)", "isCorrect": true}, {"text": "C) Single sign-on (SSO)", "isCorrect": false}, {"text": "D) API keys", "isCorrect": false}]',
        'B) Multi-factor authentication (MFA)',
        'MFA requires multiple authentication factors, significantly increasing security by requiring something you know, have, or are.',
        '{"summary": "MFA security benefits:", "breakdown": ["Requires multiple authentication factors", "Protects against credential theft", "Combines knowledge, possession, and inherence factors", "Significantly reduces unauthorized access risk"], "otherOptions": "A) Single factor is easily compromised\nC) SSO is about convenience, not necessarily security\nD) API keys are single factor"}'),
       (43, 43, 'Security - Encryption', 'Comprehension', 'Domain 4',
        'Which encryption approach protects data while it is being transmitted between your application and cloud storage?',
        '[{"text": "A) Encryption at rest", "isCorrect": false}, {"text": "B) Encryption in transit", "isCorrect": true}, {"text": "C) Database encryption", "isCorrect": false}, {"text": "D) File system encryption", "isCorrect": false}]',
        'B) Encryption in transit',
        'Encryption in transit protects data during transmission using protocols like TLS/SSL.',
        '{"summary": "Encryption in transit protects:", "breakdown": ["Data moving between client and server", "API calls and responses", "File uploads and downloads", "Database connections and queries"], "otherOptions": "A) At rest protects stored data\nC) Database encryption protects stored database data\nD) File system encryption protects local storage"}'),
       (44, 44, 'Security - Encryption', 'Application', 'Domain 4',
        'Your organization requires that sensitive data be encrypted both when stored and when transmitted. Which encryption strategy should be implemented?',
        '[{"text": "A) Encryption in transit only", "isCorrect": false}, {"text": "B) Encryption at rest only", "isCorrect": false}, {"text": "C) Both encryption at rest and in transit", "isCorrect": true}, {"text": "D) Application-level encryption only", "isCorrect": false}]',
        'C) Both encryption at rest and in transit',
        'Comprehensive data protection requires encrypting data both when stored (at rest) and when transmitted (in transit).',
        '{"summary": "Complete encryption strategy includes:", "breakdown": ["Encryption at rest protects stored data", "Encryption in transit secures data movement", "Protects against both storage and network attacks", "Meets compliance requirements for data protection"], "otherOptions": "A) Transit-only leaves stored data vulnerable\nB) Rest-only leaves network traffic vulnerable\nD) Application-level alone insufficient for comprehensive protection"}'),
       (45, 45, 'Security - Encryption', 'Knowledge', 'Domain 4',
        'What is the primary purpose of key management in cloud encryption?',
        '[{"text": "A) To reduce encryption costs", "isCorrect": false}, {"text": "B) To securely store, rotate, and control access to encryption keys", "isCorrect": true}, {"text": "C) To improve encryption performance", "isCorrect": false}, {"text": "D) To eliminate the need for encryption", "isCorrect": false}]',
        'B) To securely store, rotate, and control access to encryption keys',
        'Key management ensures encryption keys are securely stored, regularly rotated, and access is properly controlled.',
        '{"summary": "Key management responsibilities:", "breakdown": ["Secure key storage and protection", "Regular key rotation and lifecycle management", "Access control and audit logging", "Key recovery and backup procedures"], "otherOptions": "A) Cost reduction is not primary purpose\nC) Performance optimization is secondary\nD) Key management supports encryption, not eliminates it"}'),
       (46, 46, 'DevOps - CI/CD', 'Knowledge', 'Domain 5',
        'What is the primary purpose of Continuous Integration (CI) in a DevOps pipeline?',
        '[{"text": "A) Automatically deploy to production", "isCorrect": false}, {"text": "B) Integrate code changes frequently and run automated tests", "isCorrect": true}, {"text": "C) Monitor application performance", "isCorrect": false}, {"text": "D) Manage infrastructure as code", "isCorrect": false}]',
        'B) Integrate code changes frequently and run automated tests',
        'CI focuses on frequently integrating code changes and running automated builds and tests.',
        '{"summary": "Continuous Integration benefits:", "breakdown": ["Early detection of integration issues", "Automated build and test execution", "Frequent code integration reduces conflicts", "Faster feedback to development teams"], "otherOptions": "A) Production deployment is Continuous Deployment\nC) Performance monitoring is separate from CI\nD) IaC management is infrastructure automation"}'),
       (47, 47, 'DevOps - CI/CD', 'Comprehension', 'Domain 5',
        'What is the difference between Continuous Delivery and Continuous Deployment?',
        '[{"text": "A) Continuous Delivery deploys automatically, Continuous Deployment requires manual approval", "isCorrect": false}, {"text": "B) Continuous Delivery requires manual approval for production, Continuous Deployment is fully automated", "isCorrect": true}, {"text": "C) They are the same thing with different names", "isCorrect": false}, {"text": "D) Continuous Delivery is for testing, Continuous Deployment is for production", "isCorrect": false}]',
        'B) Continuous Delivery requires manual approval for production, Continuous Deployment is fully automated',
        'Continuous Delivery prepares code for production deployment but requires manual approval, while Continuous Deployment automatically deploys to production.',
        '{"summary": "CD vs CD comparison:", "breakdown": ["Continuous Delivery: Automated pipeline with manual production approval", "Continuous Deployment: Fully automated pipeline to production", "Both require robust testing and quality gates", "Continuous Deployment requires higher confidence in automation"], "otherOptions": "A) This reverses the definitions\nC) They have different automation levels\nD) Both involve production deployment"}'),
       (48, 48, 'DevOps - CI/CD', 'Application', 'Domain 5',
        'Your development team wants to catch integration issues early and run automated tests on every code commit. Which DevOps practice should be implemented first?',
        '[{"text": "A) Continuous Deployment", "isCorrect": false}, {"text": "B) Continuous Integration", "isCorrect": true}, {"text": "C) Infrastructure as Code", "isCorrect": false}, {"text": "D) Configuration Management", "isCorrect": false}]',
        'B) Continuous Integration',
        'Continuous Integration should be implemented first to establish automated builds and testing on every code commit.',
        '{"summary": "CI as foundation practice:", "breakdown": ["Establishes automated build processes", "Runs tests on every code commit", "Provides immediate feedback to developers", "Foundation for more advanced DevOps practices"], "otherOptions": "A) CD builds on CI foundation\nC) IaC is infrastructure focused\nD) Configuration management is separate concern"}'),
       (49, 49, 'DevOps - Containers', 'Comprehension', 'Domain 5',
        'Which container orchestration approach is best for production environments requiring automated scaling and service discovery?',
        '[{"text": "A) Standalone containers managed manually", "isCorrect": false}, {"text": "B) Container orchestration platform like Kubernetes", "isCorrect": true}, {"text": "C) Virtual machines with containers installed", "isCorrect": false}, {"text": "D) Containers running on a single host", "isCorrect": false}]',
        'B) Container orchestration platform like Kubernetes',
        'Container orchestration platforms provide automated management, scaling, and service discovery for production workloads.',
        '{"summary": "Orchestration platform benefits:", "breakdown": ["Automated container lifecycle management", "Built-in scaling and load balancing", "Service discovery and networking", "Rolling updates and rollback capabilities"], "otherOptions": "A) Manual management doesn''t scale for production\nC) VMs add unnecessary overhead\nD) Single host creates single point of failure"}'),
       (50, 50, 'DevOps - Containers', 'Application', 'Domain 5',
        'Your team needs to deploy a microservices application that can automatically scale, handle failures, and manage service discovery. Which approach is most suitable?',
        '[{"text": "A) Standalone containers on virtual machines", "isCorrect": false}, {"text": "B) Container orchestration with Kubernetes", "isCorrect": true}, {"text": "C) Virtual machines with manual deployment", "isCorrect": false}, {"text": "D) Serverless functions only", "isCorrect": false}]',
        'B) Container orchestration with Kubernetes',
        'Kubernetes provides comprehensive container orchestration with auto-scaling, self-healing, and service discovery capabilities.',
        '{"summary": "Kubernetes orchestration features:", "breakdown": ["Automatic scaling based on resource utilization", "Self-healing with pod restart and rescheduling", "Built-in service discovery and load balancing", "Rolling updates and rollback capabilities"], "otherOptions": "A) Standalone containers lack orchestration features\nC) VMs with manual deployment don''t provide automation\nD) Serverless alone insufficient for complex microservices"}'),
       (51, 51, 'Troubleshooting - Performance', 'Application', 'Domain 6',
        'Users report slow application response times. Your monitoring shows high CPU utilization but normal memory and disk usage. What should be your first troubleshooting step?',
        '[{"text": "A) Increase memory allocation", "isCorrect": false}, {"text": "B) Analyze CPU-intensive processes and optimize or scale CPU resources", "isCorrect": true}, {"text": "C) Restart all application servers", "isCorrect": false}, {"text": "D) Increase disk storage capacity", "isCorrect": false}]',
        'B) Analyze CPU-intensive processes and optimize or scale CPU resources',
        'High CPU utilization directly correlates with the performance issue, making CPU analysis the logical first step.',
        '{"summary": "CPU troubleshooting approach:", "breakdown": ["Identify CPU-intensive processes or queries", "Analyze application code for optimization opportunities", "Consider vertical scaling for more CPU power", "Implement horizontal scaling to distribute load"], "otherOptions": "A) Memory is not the bottleneck here\nC) Restart is temporary and doesn''t address root cause\nD) Disk capacity is not related to CPU issues"}'),
       (52, 52, 'Troubleshooting - Performance', 'Comprehension', 'Domain 6',
        'What is the most likely cause of high IOPS (Input/Output Operations Per Second) in a cloud environment?',
        '[{"text": "A) Network latency issues", "isCorrect": false}, {"text": "B) Database queries or file system operations", "isCorrect": true}, {"text": "C) CPU overutilization", "isCorrect": false}, {"text": "D) Memory leaks", "isCorrect": false}]',
        'B) Database queries or file system operations',
        'High IOPS typically indicates intensive database operations or file system read/write activities.',
        '{"summary": "Common causes of high IOPS:", "breakdown": ["Database queries and transactions", "File system read/write operations", "Application logging activities", "Backup and data replication processes"], "otherOptions": "A) Network latency affects throughput, not IOPS\nC) CPU issues don''t directly cause IOPS\nD) Memory leaks affect memory usage, not IOPS"}'),
       (53, 53, 'Troubleshooting - Network', 'Comprehension', 'Domain 6',
        'An application cannot connect to a database in another subnet. The database is running and accessible from other sources. What is the most likely cause?',
        '[{"text": "A) Database server is down", "isCorrect": false}, {"text": "B) Network security group or firewall blocking the connection", "isCorrect": true}, {"text": "C) DNS resolution failure", "isCorrect": false}, {"text": "D) Application code error", "isCorrect": false}]',
        'B) Network security group or firewall blocking the connection',
        'Network connectivity issues between subnets typically involve security group or firewall configuration problems.',
        '{"summary": "Network troubleshooting for subnet connectivity:", "breakdown": ["Check security group rules for required ports", "Verify network ACL configurations", "Ensure route table entries for subnet communication", "Confirm firewall rules on both source and destination"], "otherOptions": "A) Database is confirmed running and accessible\nC) DNS would affect name resolution, not subnet connectivity\nD) Code error wouldn''t be subnet-specific"}'),
       (54, 54, 'Troubleshooting - Network', 'Application', 'Domain 6',
        'A multi-tier application can communicate between web and application tiers, but the application tier cannot reach the database tier. What should you check first?',
        '[{"text": "A) Web server configuration", "isCorrect": false}, {"text": "B) Application server logs", "isCorrect": false}, {"text": "C) Network security groups and routing between application and database tiers", "isCorrect": true}, {"text": "D) Database server hardware status", "isCorrect": false}]',
        'C) Network security groups and routing between application and database tiers',
        'Network connectivity issues between specific tiers typically involve security group rules or routing configuration.',
        '{"summary": "Network troubleshooting for tier connectivity:", "breakdown": ["Security groups may block database port access", "Subnet routing tables might be misconfigured", "Network ACLs could prevent tier communication", "VPC peering or transit gateway issues possible"], "otherOptions": "A) Web tier communication works, not a web server issue\nB) App logs won''t show network configuration problems\nD) Hardware status wouldn''t be tier-specific"}'),
       (55, 55, 'Troubleshooting - Security', 'Comprehension', 'Domain 6',
        'An application suddenly cannot access a cloud storage bucket that worked fine yesterday. No code changes were made. What is the most likely cause?',
        '[{"text": "A) Storage bucket has been deleted", "isCorrect": false}, {"text": "B) Network connectivity issues", "isCorrect": false}, {"text": "C) IAM permissions or security policies changed", "isCorrect": true}, {"text": "D) Application server hardware failure", "isCorrect": false}]',
        'C) IAM permissions or security policies changed',
        'Sudden access failures without code changes typically indicate permission or security policy modifications.',
        '{"summary": "Common causes of sudden access loss:", "breakdown": ["IAM role permissions modified or revoked", "Security group rules changed", "Access keys expired or rotated", "Bucket policies or ACLs updated"], "otherOptions": "A) Deletion would affect all access, not just this app\nB) Network issues would show connectivity errors\nD) Hardware failure would cause broader application issues"}'),
       (56, 56, 'Cloud Architecture - Microservices', 'Comprehension', 'Domain 1',
        'What is the primary benefit of using microservices architecture in cloud environments?',
        '[{"text": "A) Reduced development complexity", "isCorrect": false}, {"text": "B) Lower infrastructure costs", "isCorrect": false}, {"text": "C) Independent scaling and deployment of services", "isCorrect": true}, {"text": "D) Simplified monitoring and logging", "isCorrect": false}]',
        'C) Independent scaling and deployment of services',
        'Microservices allow each service to be developed, deployed, and scaled independently, providing flexibility and resilience.',
        '{"summary": "Microservices benefits:", "breakdown": ["Independent service deployment and scaling", "Technology diversity across services", "Fault isolation and resilience", "Team autonomy and faster development cycles"], "otherOptions": "A) Actually increases complexity\nB) May increase infrastructure costs\nD) Monitoring becomes more complex"}'),
       (57, 57, 'Cloud Architecture - Managed Services', 'Application', 'Domain 1',
        'Your organization wants to reduce operational overhead for database management while maintaining high availability. Which approach is most suitable?',
        '[{"text": "A) Self-managed database on virtual machines", "isCorrect": false}, {"text": "B) Managed database service with multi-AZ deployment", "isCorrect": true}, {"text": "C) Containerized database with manual orchestration", "isCorrect": false}, {"text": "D) On-premises database with cloud backup", "isCorrect": false}]',
        'B) Managed database service with multi-AZ deployment',
        'Managed database services with multi-AZ deployment provide high availability while reducing operational overhead.',
        '{"summary": "Managed database benefits:", "breakdown": ["Automated patching and maintenance", "Built-in high availability with multi-AZ", "Automated backups and point-in-time recovery", "Reduced operational overhead"], "otherOptions": "A) Self-managed increases operational overhead\nC) Manual orchestration increases complexity\nD) On-premises doesn''t reduce overhead"}'),
       (58, 58, 'Deployments - Version Control', 'Knowledge', 'Domain 2',
        'Which version control operation allows developers to propose changes for review before merging into the main branch?',
        '[{"text": "A) Git push", "isCorrect": false}, {"text": "B) Git commit", "isCorrect": false}, {"text": "C) Pull request", "isCorrect": true}, {"text": "D) Git merge", "isCorrect": false}]',
        'C) Pull request',
        'Pull requests enable code review and discussion before changes are merged into the main codebase.',
        '{"summary": "Pull request benefits:", "breakdown": ["Facilitates peer code review process", "Enables discussion and feedback on changes", "Maintains code quality through review gates", "Provides audit trail of changes and approvals"], "otherOptions": "A) Push uploads code to repository\nB) Commit saves changes locally\nD) Merge combines branches without review"}'),
       (59, 59, 'Operations - Lifecycle Management', 'Comprehension', 'Domain 3',
        'What is the primary purpose of patch management in cloud environments?',
        '[{"text": "A) Improve application performance", "isCorrect": false}, {"text": "B) Reduce infrastructure costs", "isCorrect": false}, {"text": "C) Address security vulnerabilities and bugs", "isCorrect": true}, {"text": "D) Increase storage capacity", "isCorrect": false}]',
        'C) Address security vulnerabilities and bugs',
        'Patch management primarily addresses security vulnerabilities and software bugs to maintain system security and stability.',
        '{"summary": "Patch management objectives:", "breakdown": ["Address security vulnerabilities", "Fix software bugs and issues", "Maintain system stability", "Ensure compliance with security standards"], "otherOptions": "A) Performance improvements are secondary\nB) Cost reduction is not primary purpose\nD) Storage capacity is unrelated to patching"}'),
       (60, 60, 'Security - Compliance', 'Knowledge', 'Domain 4',
        'Which compliance framework is specifically designed for organizations handling credit card data?',
        '[{"text": "A) SOC 2", "isCorrect": false}, {"text": "B) GDPR", "isCorrect": false}, {"text": "C) PCI DSS", "isCorrect": true}, {"text": "D) HIPAA", "isCorrect": false}]',
        'C) PCI DSS',
        'PCI DSS (Payment Card Industry Data Security Standard) is specifically designed for organizations that handle credit card data.',
        '{"summary": "PCI DSS requirements:", "breakdown": ["Secure network and system configuration", "Protect cardholder data", "Maintain vulnerability management program", "Implement access control measures"], "otherOptions": "A) SOC 2 is for service organizations\nB) GDPR is for data privacy\nD) HIPAA is for healthcare data"}'),
       (61, 61, 'Cloud Migration and Capacity Planning', 'Application', 'Domain 1: Cloud Architecture and Design',
        'A retail company with 200 stores operates a legacy inventory system requiring 48 CPU cores, 256GB RAM, and 10TB storage with 15,000 IOPS. The system experiences 300% load increase during Black Friday sales. They want to migrate to the cloud with the ability to handle peak loads cost-effectively. Which migration strategy best meets these requirements?',
        '[{"text": "A) Lift-and-shift to cloud with 3x capacity provisioned year-round", "isCorrect": false}, {"text": "B) Re-architect as microservices with auto-scaling and cloud-native database", "isCorrect": true}, {"text": "C) Hybrid approach keeping database on-premises with cloud compute", "isCorrect": false}, {"text": "D) Containerize the application as-is and deploy to managed Kubernetes", "isCorrect": false}]',
        'B) Re-architect as microservices with auto-scaling and cloud-native database',
        'Re-architecting as microservices enables auto-scaling for the 300% peak load without over-provisioning year-round, while cloud-native databases provide elastic IOPS scaling.',
        '{"summary": "Microservices migration benefits for variable loads:", "breakdown": ["Auto-scaling handles 300% peak without year-round costs", "Cloud-native database scales IOPS on demand", "Service isolation allows scaling only needed components", "Pay-per-use model optimizes costs during normal operations"], "otherOptions": "A) 3x capacity year-round wastes resources and budget\nC) Hybrid approach doesn''t solve IOPS scaling challenge\nD) Containerizing monolith doesn''t enable granular scaling"}'),
       (62, 62, 'Cloud Security and Compliance', 'Analysis', 'Domain 2: Security',
        'A healthcare provider must implement cloud storage for patient records with these requirements: data encrypted at rest and in transit, 7-year retention for compliance, access logs retained for 1 year, and automatic deletion after retention period. They need to prove compliance during audits. Which security controls combination ensures all requirements are met?',
        '[{"text": "A) Client-side encryption, manual lifecycle policies, and quarterly access reviews", "isCorrect": false}, {"text": "B) Cloud provider encryption, automated lifecycle rules, immutable audit logs, and compliance certificates", "isCorrect": true}, {"text": "C) Third-party encryption tools, backup to tape, and annual compliance audits", "isCorrect": false}, {"text": "D) Database encryption, daily backups, and manual log reviews", "isCorrect": false}]',
        'B) Cloud provider encryption, automated lifecycle rules, immutable audit logs, and compliance certificates',
        'Automated lifecycle rules ensure compliant retention and deletion, immutable audit logs provide tamper-proof evidence, and cloud provider compliance certificates demonstrate adherence to healthcare standards.',
        '{"summary": "Healthcare compliance in cloud storage requires:", "breakdown": ["Automated lifecycle policies prevent human error in retention", "Immutable audit logs ensure tamper-proof compliance evidence", "Provider encryption meets regulatory requirements efficiently", "Compliance certificates (HIPAA, SOC2) simplify audit process"], "otherOptions": "A) Manual processes risk non-compliance through human error\nC) Tape backups don''t provide automated deletion\nD) Manual reviews insufficient for audit requirements"}'),
       (63, 63, 'Cloud Automation and Orchestration', 'Application', 'Domain 3: Deployment',
        'A development team deploys applications across dev, test, and prod environments in multiple cloud regions. They currently spend 15 hours weekly on manual deployments with a 5% error rate causing rollbacks. Which automation approach would best reduce deployment time and errors while maintaining environment-specific configurations?',
        '[{"text": "A) Shell scripts with environment variables for each deployment", "isCorrect": false}, {"text": "B) Infrastructure as Code with parameterized templates and CI/CD pipelines", "isCorrect": true}, {"text": "C) Configuration management tools with manual approval gates", "isCorrect": false}, {"text": "D) Container images with hardcoded environment settings", "isCorrect": false}]',
        'B) Infrastructure as Code with parameterized templates and CI/CD pipelines',
        'Infrastructure as Code with parameterized templates enables consistent deployments across environments while CI/CD pipelines automate the process, reducing both time and error rates.',
        '{"summary": "IaC and CI/CD benefits for multi-environment deployments:", "breakdown": ["Parameterized templates handle environment-specific configs", "Version control tracks all infrastructure changes", "Automated testing catches errors before production", "Consistent deployments reduce error rate from 5% to <1%"], "otherOptions": "A) Shell scripts lack version control and testing capabilities\nC) Manual approvals don''t reduce deployment time\nD) Hardcoded settings prevent environment flexibility"}'),
       (64, 64, 'Cloud Performance Optimization', 'Analysis', 'Domain 4: Operations and Support',
        'A SaaS application experiences intermittent performance issues reported by 15% of users. Monitoring shows normal CPU (40%), memory (60%), and network (30%) utilization. However, user session recordings reveal 3-second delays during specific database queries. What combination of tools and techniques would best identify and resolve the root cause?',
        '[{"text": "A) Increase server resources and implement load balancing", "isCorrect": false}, {"text": "B) Enable database query profiling, analyze execution plans, and implement query optimization with caching", "isCorrect": true}, {"text": "C) Add more monitoring agents and create utilization alerts", "isCorrect": false}, {"text": "D) Migrate to faster storage and increase network bandwidth", "isCorrect": false}]',
        'B) Enable database query profiling, analyze execution plans, and implement query optimization with caching',
        'Database query profiling identifies specific slow queries, execution plan analysis reveals inefficiencies, and strategic caching prevents repeated expensive operations.',
        '{"summary": "Database performance troubleshooting approach:", "breakdown": ["Query profiling pinpoints exact problematic queries", "Execution plans reveal missing indexes or inefficient joins", "Query optimization reduces 3-second delays to milliseconds", "Caching frequently accessed data prevents repeated slow queries"], "otherOptions": "A) Resources aren''t the issue (40% CPU, 60% memory)\nC) More monitoring won''t fix identified query delays\nD) Hardware upgrades don''t address query inefficiency"}'),
       (65, 65, 'Cloud Business Continuity', 'Application', 'Domain 5: Troubleshooting',
        'During a cloud provider outage, a company''s primary region becomes unavailable. Their disaster recovery plan activates, but the failover process takes 6 hours instead of the planned 2 hours. Post-incident analysis reveals DNS propagation delays and cold database replicas. Which improvements would most effectively achieve the 2-hour RTO target?',
        '[{"text": "A) Increase backup frequency and add more regions", "isCorrect": false}, {"text": "B) Implement DNS pre-staging with low TTL and maintain warm database replicas", "isCorrect": true}, {"text": "C) Create detailed runbooks and conduct monthly drills", "isCorrect": false}, {"text": "D) Purchase dedicated network connections between regions", "isCorrect": false}]',
        'B) Implement DNS pre-staging with low TTL and maintain warm database replicas',
        'DNS pre-staging with low TTL ensures rapid traffic redirection while warm database replicas eliminate lengthy data loading and cache warming during failover.',
        '{"summary": "Achieving 2-hour RTO requires addressing specific bottlenecks:", "breakdown": ["Low TTL DNS (5 minutes) enables quick traffic switching", "DNS pre-staging eliminates record creation time during disaster", "Warm replicas maintain recent data and cache, ready for traffic", "Combined approach reduces failover from 6 hours to under 2 hours"], "otherOptions": "A) More backups don''t address DNS or cold replica issues\nC) Runbooks help execution but don''t fix technical delays\nD) Network connections don''t solve DNS propagation delays"}'),
       (66, 66, 'Cloud Cost Management', 'Analysis', 'Domain 1: Cloud Architecture and Design',
        'A company''s cloud bill increased 150% over 6 months despite stable user numbers. Investigation reveals: 500 unused elastic IPs, 200TB of orphaned snapshots, 50 stopped but not terminated instances, and development databases running 24/7 on production-grade hardware. Which cost optimization strategy would yield the greatest immediate savings?',
        '[{"text": "A) Negotiate enterprise discounts and purchase reserved capacity", "isCorrect": false}, {"text": "B) Implement resource tagging and automated cleanup policies for unused resources", "isCorrect": true}, {"text": "C) Migrate to a different cloud provider with lower rates", "isCorrect": false}, {"text": "D) Reduce application features to decrease resource usage", "isCorrect": false}]',
        'B) Implement resource tagging and automated cleanup policies for unused resources',
        'Automated cleanup policies immediately eliminate costs from unused resources (IPs, snapshots, stopped instances) while resource tagging enables ongoing cost visibility and management.',
        '{"summary": "Immediate cost reduction through resource hygiene:", "breakdown": ["Unused elastic IPs: $0.005/hour each = $1,800/month savings", "Orphaned snapshots: $0.05/GB/month = $10,000/month savings", "Stopped instances: Still incur storage costs, termination saves 100%", "Automated policies prevent future resource accumulation"], "otherOptions": "A) Reserved capacity doesn''t address unused resources\nC) Migration costs outweigh potential savings\nD) Feature reduction impacts business unnecessarily"}'),
       (67, 67, 'Cloud Network Architecture', 'Application', 'Domain 1: Cloud Architecture and Design',
        'A global company needs to connect 15 branch offices to their cloud infrastructure. Each office has different bandwidth requirements (10Mbps to 1Gbps) and varying security policies. Current MPLS costs are $50,000/month. Which cloud networking solution provides the most flexible and cost-effective approach?',
        '[{"text": "A) Individual site-to-site VPNs for each branch office", "isCorrect": false}, {"text": "B) SD-WAN overlay with cloud backbone and local internet breakout", "isCorrect": true}, {"text": "C) Direct dedicated connections from each office to cloud", "isCorrect": false}, {"text": "D) Hub-and-spoke topology with central data center", "isCorrect": false}]',
        'B) SD-WAN overlay with cloud backbone and local internet breakout',
        'SD-WAN provides flexible bandwidth allocation, policy-based routing, and leverages cost-effective internet connections while maintaining security and performance.',
        '{"summary": "SD-WAN advantages for multi-site cloud connectivity:", "breakdown": ["Dynamic bandwidth allocation based on real-time needs", "Local internet breakout reduces backhaul costs", "Policy-based routing enforces security requirements per site", "Typical 40-60% cost reduction versus MPLS"], "otherOptions": "A) 15 individual VPNs create management complexity\nC) Dedicated connections too expensive for small sites\nD) Hub-and-spoke creates bottlenecks and latency"}'),
       (68, 68, 'Cloud Monitoring and Logging', 'Analysis', 'Domain 4: Operations and Support',
        'An e-commerce platform processes 1 million transactions daily across 50 microservices. The ops team struggles to troubleshoot issues due to distributed logs, missing correlation IDs, and 10TB daily log volume. Which observability strategy best addresses these challenges?',
        '[{"text": "A) Centralize all logs to a single database with full-text search", "isCorrect": false}, {"text": "B) Implement distributed tracing, structured logging with correlation IDs, and intelligent log sampling", "isCorrect": true}, {"text": "C) Increase log retention and add more verbose logging", "isCorrect": false}, {"text": "D) Create service-specific dashboards with custom metrics", "isCorrect": false}]',
        'B) Implement distributed tracing, structured logging with correlation IDs, and intelligent log sampling',
        'Distributed tracing provides end-to-end visibility, correlation IDs link related events across services, and intelligent sampling reduces volume while preserving important events.',
        '{"summary": "Modern observability for microservices requires:", "breakdown": ["Distributed tracing shows complete request flow across services", "Correlation IDs enable tracking single transactions through 50 services", "Structured logging improves queryability and reduces storage", "Intelligent sampling keeps important events while reducing volume 80%"], "otherOptions": "A) Single database can''t handle 10TB daily efficiently\nC) More logs worsen the volume problem\nD) Dashboards don''t solve log correlation issues"}'),
       (69, 69, 'Cloud Identity Management', 'Application', 'Domain 2: Security',
        'A company acquires three subsidiaries, each with different identity providers (AD, Google Workspace, Okta). They need unified cloud access for 5,000 total users while maintaining each subsidiary''s existing identity system. Compliance requires MFA and privileged access management. Which identity architecture best meets these requirements?',
        '[{"text": "A) Migrate all users to a single corporate identity provider", "isCorrect": false}, {"text": "B) Implement federated identity with SAML/OIDC, centralized MFA, and PAM solution", "isCorrect": true}, {"text": "C) Create cloud accounts for each subsidiary with separate identity systems", "isCorrect": false}, {"text": "D) Sync all identities to cloud provider''s native directory service", "isCorrect": false}]',
        'B) Implement federated identity with SAML/OIDC, centralized MFA, and PAM solution',
        'Federation allows each subsidiary to maintain their identity provider while SAML/OIDC provides secure cloud access. Centralized MFA and PAM ensure consistent security controls.',
        '{"summary": "Federated identity architecture benefits:", "breakdown": ["SAML/OIDC federation preserves existing identity investments", "Users maintain single credentials (reduced password fatigue)", "Centralized MFA policy applies regardless of source IdP", "PAM solution provides consistent privileged access controls"], "otherOptions": "A) Migration disrupts 5,000 users and requires retraining\nC) Separate accounts prevent unified access and compliance\nD) Sync creates password management and security challenges"}'),
       (70, 70, 'Cloud Service Models', 'Comprehension', 'Domain 1: Cloud Architecture and Design',
        'A software startup needs to choose between IaaS, PaaS, and SaaS solutions for their new mobile app backend. They have 3 developers, limited DevOps experience, need to reach market in 3 months, and have $10,000 monthly budget. Which approach best balances their constraints?',
        '[{"text": "A) IaaS with full control over infrastructure and custom configuration", "isCorrect": false}, {"text": "B) PaaS for backend services with managed databases and authentication", "isCorrect": true}, {"text": "C) SaaS solutions only with no custom development", "isCorrect": false}, {"text": "D) Hybrid approach with IaaS compute and SaaS databases", "isCorrect": false}]',
        'B) PaaS for backend services with managed databases and authentication',
        'PaaS provides the right abstraction level for a small team, offering managed services that accelerate development while staying within budget and timeline constraints.',
        '{"summary": "PaaS advantages for startups:", "breakdown": ["Managed infrastructure reduces DevOps burden on 3-person team", "Built-in services (auth, databases) accelerate 3-month timeline", "Pay-per-use model fits $10,000 budget with room to scale", "Focus remains on app development, not infrastructure"], "otherOptions": "A) IaaS requires DevOps expertise they lack\nC) Pure SaaS too limiting for custom mobile backend\nD) Hybrid approach adds unnecessary complexity"}'),
       (71, 71, 'DevOps - Automation', 'Application', 'Domain 5',
        'A DevOps team wants to automate the provisioning of new virtual machines, network configurations, and security groups whenever a new project starts. Which practice is best suited for this task?',
        '[{"text": "A) Manual configuration via cloud console for each project.", "isCorrect": false}, {"text": "B) Using shell scripts for server setup and manual network configuration.", "isCorrect": false}, {"text": "C) Implementing Infrastructure as Code (IaC) with templating tools.", "isCorrect": true}, {"text": "D) Creating a comprehensive manual checklist for infrastructure setup.", "isCorrect": false}]',
        'C) Implementing Infrastructure as Code (IaC) with templating tools.',
        'Infrastructure as Code (IaC) allows defining and provisioning infrastructure using code, ensuring repeatability, consistency, and reduced manual errors for new project environments.',
        '{"summary": "IaC for automated provisioning:", "breakdown": ["**Repeatability:** Ensures identical environments are deployed every time.", "**Consistency:** Eliminates configuration drift between environments.", "**Reduced Errors:** Automates complex setup processes, minimizing human error.", "**Version Control:** Infrastructure definitions can be versioned and managed like application code."], "otherOptions": "A) Manual configuration is prone to errors and inconsistencies, especially for complex setups. \nB) Shell scripts can automate some tasks but typically lack the comprehensive state management and idempotency of IaC tools for full infrastructure provisioning. Network configuration would still likely be manual or poorly managed. \nD) A manual checklist helps but does not automate or guarantee consistency, nor does it reduce the time spent on manual setup."}'),
       (72, 72, 'DevOps - CI/CD', 'Analysis', 'Domain 5',
        'A software company is experiencing frequent integration issues and broken builds after developers merge their code. They also have a slow release cycle. Which two (2) DevOps practices should they prioritize to address these problems?',
        '[{"text": "A) Implement Continuous Integration (CI) and establish automated testing.", "isCorrect": true}, {"text": "B) Adopt a Microservices architecture and use serverless functions.", "isCorrect": false}, {"text": "C) Focus on manual code reviews and increase documentation efforts.", "isCorrect": false}, {"text": "D) Implement Continuous Deployment (CD) and roll back frequently.", "isCorrect": false}]',
        'A) Implement Continuous Integration (CI) and establish automated testing.',
        'Continuous Integration (CI) focuses on frequent code integration and automated testing to catch issues early, while establishing automated testing verifies code quality and functionality, addressing broken builds and integration problems. These are foundational to speeding up the release cycle.',
        '{"summary": "Addressing integration issues and slow releases with CI and automated testing:", "breakdown": ["**Continuous Integration (CI):** Integrates code changes frequently (multiple times a day), reducing integration hell and catching conflicts early.", "**Automated Testing:** Runs tests on every code commit or integration, immediately identifying broken builds and ensuring code quality and functionality before merging.", "These two practices are fundamental for ensuring a stable codebase and enabling faster, more reliable releases."], "otherOptions": "B) Microservices and serverless functions are architectural choices that may *support* better CI/CD, but do not directly solve existing integration issues or broken builds. \nC) Manual code reviews are important but often too slow and cannot reliably catch all integration issues in a fast-paced environment. Increasing documentation does not solve technical problems. \nD) Implementing Continuous Deployment *before* fixing CI issues (broken builds, frequent integration problems) would lead to deploying broken software to production more rapidly, exacerbating the problem. Frequent rollbacks indicate a problematic CI/CD pipeline, not a solution."}'),
       (73, 73, 'Cloud Service Models', 'Knowledge', 'Domain 1: Cloud Architecture and Design',
        'A company wants to migrate their email system to the cloud but maintain full control over the operating system and middleware while letting the cloud provider manage the underlying infrastructure. Which service model BEST meets their requirements?',
        '[{"text": "A) Software as a Service (SaaS)", "isCorrect": false}, {"text": "B) Platform as a Service (PaaS)", "isCorrect": false}, {"text": "C) Infrastructure as a Service (IaaS)", "isCorrect": true}, {"text": "D) Function as a Service (FaaS)", "isCorrect": false}]',
        'C) Infrastructure as a Service (IaaS)',
        'IaaS provides virtual infrastructure while allowing customers to maintain control over the operating system, middleware, and applications.',
        '{"summary": "IaaS characteristics for email migration:", "breakdown": ["Customer controls: OS, middleware, applications, and data", "Provider manages: Physical hardware, hypervisor, networking", "Email flexibility: Can install any email server software", "Full administrative access to customize configurations"], "otherOptions": "A) SaaS provides ready-to-use applications with no OS control\nB) PaaS abstracts OS layer, limiting administrative control\nD) FaaS is for serverless functions, not email systems"}'),
       (74, 74, 'Cloud Service Models', 'Application', 'Domain 1: Cloud Architecture and Design',
        'A development team needs a cloud environment where they can deploy applications without managing servers, operating systems, or runtime environments. They want to focus solely on code development and automatic scaling based on demand. Which service model is MOST appropriate?',
        '[{"text": "A) Infrastructure as a Service (IaaS)", "isCorrect": false}, {"text": "B) Platform as a Service (PaaS)", "isCorrect": true}, {"text": "C) Software as a Service (SaaS)", "isCorrect": false}, {"text": "D) Desktop as a Service (DaaS)", "isCorrect": false}]',
        'B) Platform as a Service (PaaS)',
        'PaaS provides a development platform with automated scaling, allowing developers to focus on code while the platform handles infrastructure management.',
        '{"summary": "PaaS benefits for development teams:", "breakdown": ["Abstracted infrastructure: No server or OS management needed", "Built-in scaling: Automatic resource allocation based on demand", "Development tools: Integrated IDEs, databases, and services", "Focus on code: Developers concentrate on application logic"], "otherOptions": "A) IaaS requires managing servers and OS\nC) SaaS provides ready-made applications, not development platforms\nD) DaaS provides virtual desktops, not development platforms"}'),
       (75, 75, 'Shared Responsibility Model', 'Application', 'Domain 1: Cloud Architecture and Design',
        'A healthcare organization is concerned about data security compliance in their PaaS deployment. According to the shared responsibility model, which security aspects remain the customer''s responsibility in a PaaS environment?',
        '[{"text": "A) Physical security of data centers and network controls", "isCorrect": false}, {"text": "B) Application code security, data encryption, and user access management", "isCorrect": true}, {"text": "C) Hypervisor patching and host operating system security", "isCorrect": false}, {"text": "D) Hardware maintenance and power/cooling systems", "isCorrect": false}]',
        'B) Application code security, data encryption, and user access management',
        'In PaaS, customers are responsible for application-layer security including code security, data encryption, and identity/access management.',
        '{"summary": "PaaS customer responsibilities:", "breakdown": ["Application security: Secure coding practices and vulnerability management", "Data protection: Encryption at rest and in transit", "Identity management: User authentication and authorization", "Compliance: Meeting regulatory requirements for data handling"], "otherOptions": "A) Physical security is provider responsibility\nC) Platform infrastructure managed by provider\nD) Hardware and facilities managed by provider"}'),
       (76, 76, 'Cloud Storage Concepts', 'Analysis', 'Domain 1: Cloud Architecture and Design',
        'A media company needs storage for their video editing workflow with the following requirements: high IOPS for database operations, large capacity for raw video files, and long-term archival with cost optimization. Which storage architecture provides the BEST solution?',
        '[{"text": "A) All data on high-performance SSD storage", "isCorrect": false}, {"text": "B) Tiered storage with SSD for databases, HDD for active files, and cold storage for archives", "isCorrect": true}, {"text": "C) Network-attached storage (NAS) for all data types", "isCorrect": false}, {"text": "D) Object storage for all data with automated lifecycle policies", "isCorrect": false}]',
        'B) Tiered storage with SSD for databases, HDD for active files, and cold storage for archives',
        'Tiered storage matches storage types to specific use cases: SSD for high IOPS databases, HDD for large file capacity, cold storage for cost-effective archival.',
        '{"summary": "Optimal tiered storage strategy:", "breakdown": ["SSD tier: High IOPS and low latency for database operations", "HDD tier: Large capacity and moderate performance for active video files", "Cold storage: Cost-effective for long-term archival with slower retrieval", "Lifecycle automation: Automatic data movement based on access patterns"], "otherOptions": "A) All-SSD expensive for large video files and archives\nC) NAS doesn''t optimize for different performance requirements\nD) Object storage alone may not provide required IOPS for databases"}'),
       (77, 77, 'Container Technologies', 'Application', 'Domain 1: Cloud Architecture and Design',
        'A company runs microservices using standalone containers but experiences challenges with scaling, service discovery, and load balancing during peak traffic. Which approach would BEST address these operational challenges?',
        '[{"text": "A) Increase container resource limits and add more standalone containers", "isCorrect": false}, {"text": "B) Implement container orchestration with Kubernetes or Docker Swarm", "isCorrect": true}, {"text": "C) Migrate all containers to virtual machines", "isCorrect": false}, {"text": "D) Use container registries for better image management", "isCorrect": false}]',
        'B) Implement container orchestration with Kubernetes or Docker Swarm',
        'Container orchestration platforms provide automated scaling, service discovery, load balancing, and health management for containerized applications.',
        '{"summary": "Container orchestration benefits:", "breakdown": ["Auto-scaling: Automatic container scaling based on demand", "Service discovery: Automatic service registration and routing", "Load balancing: Built-in traffic distribution across containers", "Health management: Automatic restart of failed containers"], "otherOptions": "A) Manual scaling doesn''t solve automation challenges\nC) VMs don''t provide the orchestration features needed\nD) Registries help with image management but not runtime orchestration"}'),
       (78, 78, 'Cloud Deployment Models', 'Application', 'Domain 2: Cloud Deployment',
        'A financial institution requires strict data sovereignty, complete infrastructure control, and the ability to meet regulatory compliance requirements while gaining cloud benefits like scalability and self-service provisioning. Which deployment model is MOST suitable?',
        '[{"text": "A) Public cloud with dedicated instances", "isCorrect": false}, {"text": "B) Private cloud hosted on-premises", "isCorrect": true}, {"text": "C) Hybrid cloud with data replication", "isCorrect": false}, {"text": "D) Community cloud shared with other financial institutions", "isCorrect": false}]',
        'B) Private cloud hosted on-premises',
        'Private cloud provides complete control, data sovereignty, and regulatory compliance while offering cloud capabilities like automation and scalability.',
        '{"summary": "Private cloud benefits for financial institutions:", "breakdown": ["Complete control: Full authority over infrastructure and security", "Data sovereignty: Data remains within institutional boundaries", "Regulatory compliance: Easier to meet strict financial regulations", "Cloud benefits: Self-service, automation, and scalability features"], "otherOptions": "A) Public cloud may not meet data sovereignty requirements\nC) Hybrid cloud introduces complexity for strict compliance needs\nD) Community cloud shares resources with other organizations"}'),
       (79, 79, 'Cloud Migration', 'Analysis', 'Domain 2: Cloud Deployment',
        'A company has a legacy monolithic application that works well but has outdated dependencies and architecture. They want to move to the cloud quickly while minimizing risk, then modernize later. Which migration strategy is MOST appropriate for the initial move?',
        '[{"text": "A) Rehost (lift and shift) to move quickly with minimal changes", "isCorrect": true}, {"text": "B) Refactor to microservices architecture immediately", "isCorrect": false}, {"text": "C) Rebuild the entire application using cloud-native services", "isCorrect": false}, {"text": "D) Replace with a SaaS solution", "isCorrect": false}]',
        'A) Rehost (lift and shift) to move quickly with minimal changes',
        'Rehosting allows quick migration with minimal risk and changes, providing immediate cloud benefits while enabling future modernization phases.',
        '{"summary": "Rehost strategy advantages:", "breakdown": ["Speed: Fastest migration approach with minimal changes", "Low risk: Preserves existing functionality and stability", "Immediate benefits: Cost savings and basic cloud features", "Future flexibility: Provides foundation for later modernization"], "otherOptions": "B) Refactoring increases complexity and migration risk\nC) Rebuilding takes significant time and resources\nD) SaaS replacement may not maintain existing functionality"}'),
       (80, 80, 'Infrastructure as Code', 'Application', 'Domain 2: Cloud Deployment',
        'A DevOps team manages infrastructure across development, staging, and production environments. They experience configuration drift and inconsistencies between environments, leading to deployment failures. Which approach would BEST solve these consistency issues?',
        '[{"text": "A) Document all configurations in detailed runbooks", "isCorrect": false}, {"text": "B) Implement Infrastructure as Code (IaC) with version control", "isCorrect": true}, {"text": "C) Create golden images for all server configurations", "isCorrect": false}, {"text": "D) Use configuration management scripts", "isCorrect": false}]',
        'B) Implement Infrastructure as Code (IaC) with version control',
        'Infrastructure as Code ensures consistent, repeatable deployments across environments by defining infrastructure in version-controlled code templates.',
        '{"summary": "IaC benefits for environment consistency:", "breakdown": ["Declarative definitions: Infrastructure defined as code templates", "Version control: Track and rollback infrastructure changes", "Consistency: Identical deployments across all environments", "Automation: Eliminates manual configuration errors"], "otherOptions": "A) Documentation doesn''t prevent manual configuration errors\nC) Golden images don''t address infrastructure configuration drift\nD) Scripts can vary in execution and may not be declarative"}'),
       (81, 81, 'Cloud Observability', 'Analysis', 'Domain 3: Cloud Operations and Support',
        'A microservices application experiences intermittent performance issues that are difficult to trace across multiple services. Standard monitoring shows healthy individual services, but users report slow response times. Which observability approach would BEST identify the root cause?',
        '[{"text": "A) Increase log verbosity on all services", "isCorrect": false}, {"text": "B) Implement distributed tracing across microservices", "isCorrect": true}, {"text": "C) Add more performance counters and metrics", "isCorrect": false}, {"text": "D) Set up alerting based on response time thresholds", "isCorrect": false}]',
        'B) Implement distributed tracing across microservices',
        'Distributed tracing follows requests across multiple microservices, providing visibility into the complete request path and identifying bottlenecks.',
        '{"summary": "Distributed tracing benefits:", "breakdown": ["End-to-end visibility: Tracks requests across all microservices", "Bottleneck identification: Shows where delays occur in the request path", "Service dependencies: Maps interactions between services", "Performance analysis: Measures latency at each service hop"], "otherOptions": "A) More logs don''t provide cross-service correlation\nC) Additional metrics don''t show service interactions\nD) Alerting identifies problems but doesn''t show root cause"}'),
       (82, 82, 'Cloud Scaling', 'Application', 'Domain 3: Cloud Operations and Support',
        'An e-commerce application experiences predictable traffic patterns with gradual increases during business hours and sudden spikes during flash sales. Which auto-scaling strategy would provide the BEST performance and cost optimization?',
        '[{"text": "A) Reactive scaling based only on CPU utilization", "isCorrect": false}, {"text": "B) Predictive scaling with scheduled scaling for business hours and reactive scaling for spikes", "isCorrect": true}, {"text": "C) Fixed scaling with maximum capacity provisioned at all times", "isCorrect": false}, {"text": "D) Manual scaling based on sales calendar events", "isCorrect": false}]',
        'B) Predictive scaling with scheduled scaling for business hours and reactive scaling for spikes',
        'Combined predictive and reactive scaling handles both predictable patterns efficiently and responds to unexpected spikes automatically.',
        '{"summary": "Hybrid scaling strategy benefits:", "breakdown": ["Predictive scaling: Anticipates business hour traffic increases", "Reactive scaling: Responds automatically to unexpected spikes", "Cost optimization: Scales down during low-traffic periods", "Performance assurance: Maintains responsiveness during all scenarios"], "otherOptions": "A) CPU-only reactive scaling too slow for sudden spikes\nC) Fixed capacity wastes resources during low traffic\nD) Manual scaling can''t respond quickly to unexpected events"}'),
       (83, 83, 'Cloud Backup Strategies', 'Analysis', 'Domain 3: Cloud Operations and Support',
        'A company needs to design a backup strategy for critical business data with a Recovery Point Objective (RPO) of 1 hour and Recovery Time Objective (RTO) of 2 hours. The solution must be cost-effective while meeting compliance requirements for 7-year retention. Which backup approach is MOST suitable?',
        '[{"text": "A) Daily full backups with 7-year retention in hot storage", "isCorrect": false}, {"text": "B) Hourly incremental backups with tiered storage and lifecycle policies", "isCorrect": true}, {"text": "C) Real-time replication to a secondary site with immediate failover", "isCorrect": false}, {"text": "D) Weekly full backups with manual restore processes", "isCorrect": false}]',
        'B) Hourly incremental backups with tiered storage and lifecycle policies',
        'Hourly incremental backups meet the RPO requirement, while tiered storage and lifecycle policies optimize costs for long-term retention.',
        '{"summary": "Optimal backup strategy components:", "breakdown": ["Hourly incrementals: Meet 1-hour RPO requirement efficiently", "Tiered storage: Hot storage for recent backups, cold for long-term", "Lifecycle policies: Automatic movement to cheaper storage over time", "Fast recovery: 2-hour RTO achievable from recent incremental backups"], "otherOptions": "A) Daily backups exceed 1-hour RPO requirement\nC) Real-time replication expensive for 7-year retention\nD) Weekly backups far exceed RPO requirement"}'),
       (84, 84, 'Cloud Security - Access Management', 'Application', 'Domain 4: Cloud Security',
        'A global organization needs to manage user access to cloud resources across multiple locations with different security requirements. They want to implement Zero Trust principles while maintaining user productivity. Which approach BEST achieves these goals?',
        '[{"text": "A) VPN access with network-based security controls", "isCorrect": false}, {"text": "B) Multi-factor authentication with conditional access policies based on context", "isCorrect": true}, {"text": "C) Single sign-on with basic username/password authentication", "isCorrect": false}, {"text": "D) Role-based access control with periodic access reviews", "isCorrect": false}]',
        'B) Multi-factor authentication with conditional access policies based on context',
        'Conditional access with MFA implements Zero Trust by continuously verifying users based on context like location, device, and behavior patterns.',
        '{"summary": "Zero Trust conditional access benefits:", "breakdown": ["Context awareness: Considers location, device, time, and behavior", "Continuous verification: Doesn''t trust based solely on network location", "Risk-based decisions: Adjusts requirements based on calculated risk", "User productivity: Seamless access for low-risk scenarios"], "otherOptions": "A) VPN assumes trust based on network location\nC) Basic authentication insufficient for Zero Trust\nD) RBAC alone doesn''t provide continuous verification"}'),
       (85, 85, 'Cloud Compliance', 'Analysis', 'Domain 4: Cloud Security',
        'A healthcare organization moving to the cloud must demonstrate HIPAA compliance for patient data. They need automated compliance monitoring, evidence collection, and remediation capabilities. Which combination of cloud security controls provides the MOST comprehensive compliance framework?',
        '[{"text": "A) Manual security audits with document-based evidence collection", "isCorrect": false}, {"text": "B) Cloud security posture management (CSPM) with automated policy enforcement and audit trails", "isCorrect": true}, {"text": "C) Encryption of all data with annual compliance reviews", "isCorrect": false}, {"text": "D) Network segmentation with firewall rules and access logs", "isCorrect": false}]',
        'B) Cloud security posture management (CSPM) with automated policy enforcement and audit trails',
        'CSPM provides continuous compliance monitoring, automated policy enforcement, and detailed audit trails required for HIPAA compliance demonstration.',
        '{"summary": "CSPM benefits for HIPAA compliance:", "breakdown": ["Continuous monitoring: Real-time compliance posture assessment", "Automated enforcement: Immediate remediation of policy violations", "Audit trails: Comprehensive logging for compliance evidence", "Risk assessment: Identifies and prioritizes compliance gaps"], "otherOptions": "A) Manual audits don''t provide continuous compliance monitoring\nC) Encryption alone doesn''t address all HIPAA requirements\nD) Network controls are part of compliance but not comprehensive"}'),
       (86, 86, 'Cloud Security - Vulnerability Management', 'Application', 'Domain 4: Cloud Security',
        'A development team deploys applications using container images from various sources. Security scans reveal vulnerabilities in base images and third-party components. Which approach provides the BEST security posture for the container supply chain?',
        '[{"text": "A) Scan containers only in production environments", "isCorrect": false}, {"text": "B) Implement security scanning throughout the CI/CD pipeline with policy enforcement", "isCorrect": true}, {"text": "C) Use only official base images from operating system vendors", "isCorrect": false}, {"text": "D) Perform monthly vulnerability assessments on deployed containers", "isCorrect": false}]',
        'B) Implement security scanning throughout the CI/CD pipeline with policy enforcement',
        'Pipeline security scanning catches vulnerabilities early, enforces security policies, and prevents vulnerable images from reaching production.',
        '{"summary": "CI/CD security scanning benefits:", "breakdown": ["Shift-left security: Identifies vulnerabilities early in development", "Policy enforcement: Blocks deployment of vulnerable images", "Continuous scanning: Monitors throughout the software lifecycle", "Supply chain security: Validates all components and dependencies"], "otherOptions": "A) Production-only scanning allows vulnerabilities to reach live systems\nC) Official images can still contain vulnerabilities\nD) Monthly scans too infrequent for active development"}'),
       (87, 87, 'DevOps CI/CD', 'Application', 'Domain 5: DevOps Fundamentals',
        'A development team wants to implement automated deployments while ensuring code quality and minimizing deployment risks. They currently perform manual testing and deployment processes. Which CI/CD pipeline design BEST balances automation with quality assurance?',
        '[{"text": "A) Automated build and deployment without testing stages", "isCorrect": false}, {"text": "B) Automated build, test, and staged deployment with approval gates", "isCorrect": true}, {"text": "C) Manual build with automated deployment to all environments", "isCorrect": false}, {"text": "D) Automated deployment only to development environments", "isCorrect": false}]',
        'B) Automated build, test, and staged deployment with approval gates',
        'Staged deployment with automated testing and approval gates provides comprehensive automation while maintaining quality controls and risk mitigation.',
        '{"summary": "Comprehensive CI/CD pipeline benefits:", "breakdown": ["Automated testing: Catches issues early in the pipeline", "Staged deployment: Progressive rollout reduces risk", "Approval gates: Human oversight for critical stages", "Quality assurance: Multiple validation points ensure code quality"], "otherOptions": "A) No testing increases deployment risk\nC) Manual build defeats automation benefits\nD) Limited to dev environments doesn''t provide full deployment automation"}'),
       (88, 88, 'DevOps Version Control', 'Knowledge', 'Domain 5: DevOps Fundamentals',
        'A DevOps team manages both application code and infrastructure configurations. They need to implement version control strategies that support collaboration, change tracking, and rollback capabilities. Which approach provides the MOST comprehensive version management?',
        '[{"text": "A) Separate repositories for code and infrastructure with different branching strategies", "isCorrect": false}, {"text": "B) Single repository with unified branching strategy for both code and infrastructure", "isCorrect": true}, {"text": "C) Version control for application code only, with manual infrastructure management", "isCorrect": false}, {"text": "D) Multiple repositories per microservice with independent versioning", "isCorrect": false}]',
        'B) Single repository with unified branching strategy for both code and infrastructure',
        'Unified repository and branching strategy ensures synchronized changes between application code and infrastructure, simplifying deployment and rollback procedures.',
        '{"summary": "Unified version control benefits:", "breakdown": ["Synchronized changes: Code and infrastructure changes tracked together", "Simplified rollbacks: Single point to revert both code and infrastructure", "Consistent branching: Same workflow for all team members", "Atomic deployments: Code and infrastructure deployed as single unit"], "otherOptions": "A) Separate repositories can lead to version mismatches\nC) Manual infrastructure management introduces inconsistency\nD) Multiple repositories increase complexity and coordination overhead"}'),
       (89, 89, 'Cloud Troubleshooting - Network', 'Analysis', 'Domain 6: Troubleshooting',
        'Users report intermittent connectivity issues to a cloud-hosted web application. The application works fine from the office but fails sporadically from remote locations. Network monitoring shows no infrastructure issues. Which troubleshooting approach would MOST effectively identify the root cause?',
        '[{"text": "A) Increase server resources and add more instances", "isCorrect": false}, {"text": "B) Analyze network paths and implement distributed monitoring from multiple locations", "isCorrect": true}, {"text": "C) Check firewall logs and security group configurations", "isCorrect": false}, {"text": "D) Review application performance metrics and database queries", "isCorrect": false}]',
        'B) Analyze network paths and implement distributed monitoring from multiple locations',
        'Distributed monitoring from multiple geographic locations helps identify network path issues, ISP problems, or regional connectivity challenges.',
        '{"summary": "Distributed network troubleshooting approach:", "breakdown": ["Geographic perspective: Monitoring from affected user locations", "Network path analysis: Traces routes to identify bottlenecks", "ISP correlation: Identifies provider-specific issues", "Performance baselines: Compares connectivity quality across locations"], "otherOptions": "A) Resource scaling doesn''t address location-specific connectivity\nC) Security configurations affect access, not intermittent connectivity\nD) Application metrics don''t reveal network path issues"}'),
       (90, 90, 'Cloud Troubleshooting - Performance', 'Analysis', 'Domain 6: Troubleshooting',
        'A cloud application experiences performance degradation only during specific hours despite consistent user load. CPU and memory utilization remain within normal ranges. Database queries show normal execution times. Which factor is MOST likely causing the performance issues?',
        '[{"text": "A) Application memory leaks accumulating over time", "isCorrect": false}, {"text": "B) Resource contention with other workloads sharing the same physical infrastructure", "isCorrect": true}, {"text": "C) Database connection pool exhaustion", "isCorrect": false}, {"text": "D) Network bandwidth limitations during peak traffic", "isCorrect": false}]',
        'B) Resource contention with other workloads sharing the same physical infrastructure',
        'Time-specific performance issues with normal resource utilization often indicate "noisy neighbor" problems where other workloads compete for underlying physical resources.',
        '{"summary": "Noisy neighbor characteristics:", "breakdown": ["Time correlation: Performance degrades at specific, recurring times", "Normal metrics: Application-level resources appear adequate", "Shared infrastructure: Multiple workloads compete for physical resources", "External dependency: Performance affected by factors outside direct control"], "otherOptions": "A) Memory leaks would show gradually increasing memory usage\nC) Connection pool issues would show in database connection metrics\nD) Network bandwidth problems would show in network utilization metrics"}'),
       (91, 91, 'Cloud Troubleshooting - Security', 'Application', 'Domain 6: Troubleshooting',
        'A cloud application suddenly starts receiving "Access Denied" errors for API calls that were previously working. No code changes were deployed recently. Security logs show successful authentication but failed authorization. Which troubleshooting approach would MOST quickly identify the issue?',
        '[{"text": "A) Review recent changes to IAM roles, policies, and resource permissions", "isCorrect": true}, {"text": "B) Check network security group rules and firewall configurations", "isCorrect": false}, {"text": "C) Verify SSL certificates and encryption configurations", "isCorrect": false}, {"text": "D) Examine application logs for authentication token issues", "isCorrect": false}]',
        'A) Review recent changes to IAM roles, policies, and resource permissions',
        'Access Denied with successful authentication indicates an authorization problem, typically caused by recent changes to IAM policies or role permissions.',
        '{"summary": "Authorization troubleshooting focus areas:", "breakdown": ["IAM policy changes: Recent modifications to access permissions", "Role updates: Changes to role assignments or capabilities", "Resource permissions: Updates to resource-specific access controls", "Time-based policies: Scheduled changes or policy expirations"], "otherOptions": "B) Network rules affect connectivity, not authorization after authentication\nC) SSL issues would prevent successful authentication\nD) Authentication is working; the issue is with authorization"}'),
       (92, 92, 'Cloud Troubleshooting - Integration', 'Expert', 'Domain 6: Troubleshooting',
        'After migrating a legacy application to the cloud, users report that batch processing jobs that completed in 2 hours on-premises now take 6 hours in the cloud. The application code was not modified during migration. Which factors should be investigated FIRST to identify the performance degradation?',
        '[{"text": "A) Cloud instance sizing and compute resources compared to on-premises hardware", "isCorrect": false}, {"text": "B) Network latency between cloud services and data storage locations", "isCorrect": false}, {"text": "C) Storage I/O performance and disk configuration differences", "isCorrect": true}, {"text": "D) Application timeout settings and connection pool configurations", "isCorrect": false}]',
        'C) Storage I/O performance and disk configuration differences',
        'Batch processing performance is often heavily dependent on storage I/O patterns, which can be significantly different between on-premises and cloud storage configurations.',
        '{"summary": "Storage I/O impact on batch processing:", "breakdown": ["I/O patterns: Batch jobs typically involve intensive read/write operations", "Storage types: Cloud storage may have different performance characteristics", "Configuration differences: RAID, caching, and optimization settings", "Sequential vs random: Batch workloads often require high sequential throughput"], "otherOptions": "A) Compute resources would show in CPU/memory utilization\nB) Network latency affects real-time applications more than batch processing\nD) Configuration issues would likely cause failures, not just slower performance"}');
-- Create the schema for the prepper application
-- CREATE SCHEMA IF NOT EXISTS prepper;

-- Set the search path to the prepper schema
SET
search_path TO prepper;

-- Drop the old table if it exists to apply the new schema
-- DROP TABLE IF EXISTS prepper.comptia_cloud_plus_questions;
-- DROP TABLE IF EXISTS prepper.aws_certified_architect_associate_questions;



-- Create the updated table for AWS Certified Architect Associate questions
CREATE TABLE IF NOT EXISTS prepper.aws_certified_architect_associate_questions
(
    id
    SERIAL
    PRIMARY
    KEY,
    question_id
    INTEGER
    UNIQUE
    NOT
    NULL,
    question_number
    INTEGER
    NOT
    NULL,
    category
    TEXT,
    difficulty
    TEXT,
    domain
    TEXT,
    question_text
    TEXT
    NOT
    NULL,
    options
    JSONB
    NOT
    NULL,
    correct_answer
    TEXT
    NOT
    NULL,
    explanation
    TEXT,
    explanation_details
    JSONB
);

-- Create the updated table for AWS Certified Architect Associate questions
CREATE TABLE IF NOT EXISTS prepper.comptia_cloud_plus_questions
(
    id
    SERIAL
    PRIMARY
    KEY,
    question_id
    INTEGER
    UNIQUE
    NOT
    NULL,
    question_number
    INTEGER
    NOT
    NULL,
    category
    TEXT,
    difficulty
    TEXT,
    domain
    TEXT,
    question_text
    TEXT
    NOT
    NULL,
    options
    JSONB
    NOT
    NULL,
    correct_answer
    TEXT
    NOT
    NULL,
    explanation
    TEXT,
    explanation_details
    JSONB
);


-- Insert all AWS Certified Architect Associate questions
INSERT INTO prepper.aws_certified_architect_associate_questions (question_id, question_number, category, difficulty,
                                                                 domain, question_text, options, correct_answer,
                                                                 explanation, explanation_details)
VALUES (101, 1, 'AWS Architecture - High Availability', 'Application', 'Domain 1: Design Resilient Architectures',
        'A company runs a critical e-commerce application on a single EC2 instance in us-east-1a. The application must achieve 99.99% uptime and automatically recover from AZ failures. Which solution provides the MOST cost-effective approach to meet these requirements?',
        '[{"text": "A) Deploy a larger EC2 instance type in the same AZ", "isCorrect": false}, {"text": "B) Create an AMI and manually launch instances in other AZs when needed", "isCorrect": false}, {"text": "C) Use Auto Scaling Group with instances across multiple AZs behind an ALB", "isCorrect": true}, {"text": "D) Use Elastic Beanstalk with a single instance", "isCorrect": false}]',
        'C) Use Auto Scaling Group with instances across multiple AZs behind an ALB',
        'Auto Scaling Groups with multiple AZs provide automatic failure detection and recovery, while ALB distributes traffic and handles health checks.',
        '{"summary": "Multi-AZ Auto Scaling provides automated resilience:", "breakdown": ["Auto Scaling Group automatically replaces failed instances", "Multiple AZs protect against entire AZ failures", "ALB provides health checks and traffic distribution", "Most cost-effective as it only runs minimum required instances"], "otherOptions": "A) Larger instance doesn''t solve AZ failure\nB) Manual process doesn''t meet automation requirement\nD) Single instance still has single point of failure"}'),
       (102, 2, 'AWS Architecture - Data Resilience', 'Application', 'Domain 1: Design Resilient Architectures',
        'A financial services company stores critical transaction data in RDS MySQL. They need to ensure data can be recovered with RPO of 5 minutes and RTO of 15 minutes in case of primary database failure. Which combination of solutions meets these requirements? (Choose TWO)',
        '[{"text": "A) Enable automated backups with 5-minute backup frequency", "isCorrect": false}, {"text": "B) Configure RDS Multi-AZ deployment", "isCorrect": true}, {"text": "C) Create hourly manual snapshots", "isCorrect": false}, {"text": "D) Enable point-in-time recovery", "isCorrect": true}]',
        'B) Configure RDS Multi-AZ deployment',
        'Multi-AZ provides automatic failover within minutes, while point-in-time recovery enables restoration to any second within the backup retention period.',
        '{"summary": "Meeting RPO/RTO requirements:", "breakdown": ["Multi-AZ: Automatic failover typically completes in 60-120 seconds", "Point-in-time recovery: Can restore to any second (meets 5-min RPO)", "Automated backups: Only daily, can''t meet 5-minute RPO", "Manual snapshots: Too infrequent for RPO requirement"], "otherOptions": "A) Automated backups are daily, not every 5 minutes\nC) Hourly snapshots exceed 5-minute RPO requirement"}'),
       (103, 3, 'AWS Architecture - Cross-Region Resilience', 'Expert', 'Domain 1: Design Resilient Architectures',
        'A media company streams video content globally and needs to ensure service availability even if an entire AWS region becomes unavailable. The application uses ALB, EC2 Auto Scaling, and RDS MySQL. Which design provides the MOST comprehensive disaster recovery solution?',
        '[{"text": "A) Create read replicas in multiple regions with manual failover", "isCorrect": false}, {"text": "B) Use Route 53 health checks with cross-region ALBs and RDS cross-region automated backups", "isCorrect": false}, {"text": "C) Deploy identical infrastructure in two regions with Route 53 failover routing and RDS cross-region read replicas with promotion capability", "isCorrect": true}, {"text": "D) Use CloudFormation to deploy infrastructure in multiple regions manually when needed", "isCorrect": false}]',
        'C) Deploy identical infrastructure in two regions with Route 53 failover routing and RDS cross-region read replicas with promotion capability',
        'Full cross-region deployment with automated DNS failover and database replica promotion provides the fastest recovery from regional failures.',
        '{"summary": "Comprehensive multi-region DR strategy:", "breakdown": ["Active-passive setup in multiple regions", "Route 53 automatic failover based on health checks", "RDS read replicas can be promoted to standalone databases", "Infrastructure already running in secondary region"], "otherOptions": "A) Manual failover too slow for video streaming\nB) Cross-region backups take too long to restore\nD) Manual deployment during disaster is too slow"}'),
       (104, 4, 'AWS Storage - Backup Strategy', 'Application', 'Domain 1: Design Resilient Architectures',
        'A company needs to backup 50TB of data from on-premises to AWS. The data must be available within 4 hours during a disaster, and they want to minimize ongoing costs. The initial backup can take up to 2 weeks. Which solution is MOST appropriate?',
        '[{"text": "A) AWS Storage Gateway with Tape Gateway to S3 Glacier", "isCorrect": false}, {"text": "B) AWS Snowball Edge to S3, then lifecycle policy to S3 Intelligent-Tiering", "isCorrect": true}, {"text": "C) Direct upload to S3 Standard over internet connection", "isCorrect": false}, {"text": "D) AWS DataSync to S3 Glacier Deep Archive", "isCorrect": false}]',
        'B) AWS Snowball Edge to S3, then lifecycle policy to S3 Intelligent-Tiering',
        'Snowball Edge handles the initial 50TB transfer efficiently, while S3 Intelligent-Tiering automatically optimizes costs and meets the 4-hour recovery requirement.',
        '{"summary": "Large data backup strategy considerations:", "breakdown": ["Snowball Edge: Cost-effective for 50TB initial transfer", "S3 Intelligent-Tiering: Automatic cost optimization", "Standard tier availability: Immediate access for 4-hour RTO", "2-week initial timeline: Perfect for Snowball shipping"], "otherOptions": "A) Tape Gateway too slow for 4-hour recovery\nC) 50TB over internet too slow and expensive\nD) Glacier Deep Archive takes 12+ hours to retrieve"}'),
       (105, 5, 'AWS Performance - Database Optimization', 'Application',
        'Domain 2: Design High-Performing Architectures',
        'An application experiences slow database performance during peak hours. The RDS MySQL instance shows 95% CPU utilization, and 80% of queries are read operations. The application cannot tolerate any downtime. Which solution provides the BEST performance improvement with minimal operational overhead?',
        '[{"text": "A) Create a larger RDS instance and schedule maintenance window for upgrade", "isCorrect": false}, {"text": "B) Enable RDS Performance Insights to identify slow queries", "isCorrect": false}, {"text": "C) Create RDS read replicas and modify application to use them for read queries", "isCorrect": true}, {"text": "D) Migrate to Amazon Aurora MySQL with automatic scaling", "isCorrect": false}]',
        'C) Create RDS read replicas and modify application to use them for read queries',
        'Read replicas directly address the 80% read workload without downtime, providing immediate performance relief for read-heavy applications.',
        '{"summary": "Read replica benefits for read-heavy workloads:", "breakdown": ["80% read queries can be offloaded to replicas", "No downtime required for setup", "Immediate performance improvement", "Cost-effective solution for read scaling"], "otherOptions": "A) Instance upgrade requires downtime\nB) Performance Insights only identifies issues, doesn''t solve them\nD) Migration requires significant effort and testing"}'),
       (106, 6, 'AWS Performance - Caching Strategy', 'Expert', 'Domain 2: Design High-Performing Architectures',
        'A social media application serves user profiles stored in DynamoDB. The same profiles are accessed frequently by millions of users, causing high read costs and occasional throttling. The application requires sub-millisecond response times. Which caching strategy provides the BEST performance and cost optimization?',
        '[{"text": "A) Implement application-level caching with Redis ElastiCache", "isCorrect": false}, {"text": "B) Enable DynamoDB DAX (DynamoDB Accelerator)", "isCorrect": true}, {"text": "C) Use CloudFront to cache DynamoDB API responses", "isCorrect": false}, {"text": "D) Increase DynamoDB provisioned read capacity", "isCorrect": false}]',
        'B) Enable DynamoDB DAX (DynamoDB Accelerator)',
        'DAX provides microsecond-level response times specifically designed for DynamoDB, with automatic cache management and no application changes required.',
        '{"summary": "DAX advantages for DynamoDB caching:", "breakdown": ["Microsecond response times (sub-millisecond requirement)", "Transparent to application (no code changes)", "Automatic cache invalidation and management", "Reduces DynamoDB read costs significantly"], "otherOptions": "A) Redis requires application code changes and cache management\nC) CloudFront can''t cache DynamoDB API calls\nD) Increasing capacity doesn''t solve latency, only costs more"}'),
       (107, 7, 'AWS Performance - Content Delivery', 'Comprehension', 'Domain 2: Design High-Performing Architectures',
        'A global news website serves static content (images, videos, articles) to users worldwide. Users in Asia report slow page load times, while users in the US experience good performance. The origin servers are located in us-east-1. Which solution provides the MOST effective performance improvement for Asian users?',
        '[{"text": "A) Deploy additional EC2 instances in ap-southeast-1", "isCorrect": false}, {"text": "B) Implement CloudFront distribution with edge locations in Asia", "isCorrect": true}, {"text": "C) Use Transfer Acceleration for S3 uploads", "isCorrect": false}, {"text": "D) Create Route 53 latency-based routing to multiple regions", "isCorrect": false}]',
        'B) Implement CloudFront distribution with edge locations in Asia',
        'CloudFront edge locations cache static content closer to Asian users, dramatically reducing latency for static content delivery.',
        '{"summary": "CloudFront benefits for global content delivery:", "breakdown": ["Edge locations in Asia cache content locally", "Reduces latency from us-east-1 to Asia", "Handles static content (images, videos, articles) efficiently", "Automatic cache management and optimization"], "otherOptions": "A) Additional instances help dynamic content, not static\nC) Transfer Acceleration for uploads, not downloads\nD) Latency routing helps with dynamic content, not static"}'),
       (108, 8, 'AWS Security - IAM Policies', 'Expert', 'Domain 3: Design Secure Applications',
        'A company has multiple AWS accounts and needs to grant developers access to specific S3 buckets based on their team assignment. Developers should only access buckets prefixed with their team name (e.g., team-alpha-*). Which approach provides the MOST secure and scalable solution?',
        '[{"text": "A) Create individual IAM users in each account with specific S3 bucket policies", "isCorrect": false}, {"text": "B) Use AWS Organizations with SCPs and cross-account IAM roles with dynamic policy conditions", "isCorrect": true}, {"text": "C) Create shared IAM users with different access keys for each team", "isCorrect": false}, {"text": "D) Use S3 bucket policies with IAM user conditions for each team", "isCorrect": false}]',
        'B) Use AWS Organizations with SCPs and cross-account IAM roles with dynamic policy conditions',
        'Organizations with SCPs provide account-level governance, while cross-account roles with dynamic conditions enable secure, scalable team-based access.',
        '{"summary": "Multi-account secure access strategy:", "breakdown": ["SCPs provide organization-wide security boundaries", "Cross-account roles eliminate need for multiple IAM users", "Dynamic conditions based on user attributes (team tags)", "Centralized access management across accounts"], "otherOptions": "A) Individual users don''t scale across multiple accounts\nC) Shared users violate security best practices\nD) Bucket policies alone can''t handle multi-account scenarios efficiently"}'),
       (109, 9, 'AWS Security - Data Encryption', 'Application', 'Domain 3: Design Secure Applications',
        'A healthcare company must encrypt all data at rest and in transit. They need to maintain full control over encryption keys and meet HIPAA compliance requirements. The solution must integrate with RDS, S3, and EBS. Which key management approach is MOST appropriate?',
        '[{"text": "A) Use AWS managed keys (SSE-S3, default RDS encryption)", "isCorrect": false}, {"text": "B) Use AWS KMS customer managed keys with CloudTrail logging", "isCorrect": true}, {"text": "C) Use client-side encryption with application-managed keys", "isCorrect": false}, {"text": "D) Use AWS KMS AWS managed keys with detailed monitoring", "isCorrect": false}]',
        'B) Use AWS KMS customer managed keys with CloudTrail logging',
        'Customer managed KMS keys provide full control over key policies, rotation, and access, while CloudTrail provides complete audit trails required for HIPAA compliance.',
        '{"summary": "HIPAA-compliant key management requirements:", "breakdown": ["Customer managed keys: Full control over key policies and access", "CloudTrail: Complete audit trail of key usage", "Cross-service integration: Works with RDS, S3, EBS", "HIPAA compliance: Meets regulatory requirements for key control"], "otherOptions": "A) AWS managed keys don''t provide sufficient control for HIPAA\nC) Client-side encryption complex and doesn''t integrate well\nD) AWS managed keys still don''t give customer full control"}'),
       (110, 10, 'AWS Security - Network Security', 'Application', 'Domain 3: Design Secure Applications',
        'A three-tier web application (web, app, database) needs to be secured according to the principle of least privilege. The web tier must be accessible from the internet, app tier only from web tier, and database tier only from app tier. Which security group configuration is CORRECT?',
        '[{"text": "A) Web SG: 0.0.0.0/0:80,443 | App SG: Web-SG:8080 | DB SG: App-SG:3306", "isCorrect": true}, {"text": "B) Web SG: 0.0.0.0/0:80,443 | App SG: 0.0.0.0/0:8080 | DB SG: 0.0.0.0/0:3306", "isCorrect": false}, {"text": "C) All tiers: VPC CIDR:All ports for internal communication", "isCorrect": false}, {"text": "D) Web SG: 0.0.0.0/0:All | App SG: VPC CIDR:All | DB SG: VPC CIDR:All", "isCorrect": false}]',
        'A) Web SG: 0.0.0.0/0:80,443 | App SG: Web-SG:8080 | DB SG: App-SG:3306',
        'Security groups should reference other security groups for internal communication, implementing true least privilege access between tiers.',
        '{"summary": "Proper three-tier security group design:", "breakdown": ["Web tier: Internet access on standard web ports (80, 443)", "App tier: Only accepts traffic from web security group", "Database tier: Only accepts traffic from app security group", "Security group references: More secure than IP-based rules"], "otherOptions": "B) App and DB exposed to internet\nC) Too broad access within VPC\nD) Overly permissive on all tiers"}'),
       (111, 11, 'AWS Security - Advanced Threats', 'Expert', 'Domain 3: Design Secure Applications',
        'A company''s web application experiences DDoS attacks and SQL injection attempts. They need comprehensive protection that automatically adapts to new attack patterns. The solution must integrate with CloudFront and ALB. Which combination provides the MOST comprehensive protection?',
        '[{"text": "A) AWS Shield Standard + CloudFront + Security Groups", "isCorrect": false}, {"text": "B) AWS Shield Advanced + AWS WAF + AWS Firewall Manager", "isCorrect": true}, {"text": "C) AWS Shield Standard + AWS WAF + VPC Flow Logs", "isCorrect": false}, {"text": "D) Network ACLs + AWS Inspector + CloudWatch", "isCorrect": false}]',
        'B) AWS Shield Advanced + AWS WAF + AWS Firewall Manager',
        'Shield Advanced provides enhanced DDoS protection, WAF blocks application-layer attacks, and Firewall Manager centrally manages security policies across resources.',
        '{"summary": "Comprehensive web application protection stack:", "breakdown": ["Shield Advanced: Enhanced DDoS protection with attack response team", "WAF: Blocks SQL injection, XSS, and other application attacks", "Firewall Manager: Centralized security policy management", "Auto-adaptation: Machine learning capabilities for new threats"], "otherOptions": "A) Shield Standard insufficient for advanced DDoS\nC) VPC Flow Logs don''t provide active protection\nD) Inspector scans instances, doesn''t provide runtime protection"}'),
       (112, 12, 'AWS Cost Optimization - Instance Selection', 'Application',
        'Domain 4: Design Cost-Optimized Architectures',
        'A batch processing workload runs predictably every night from 2 AM to 6 AM for data analytics. The workload can tolerate interruptions and can resume from checkpoints. Current costs using On-Demand instances are $500/month. Which pricing model combination provides the MAXIMUM cost savings?',
        '[{"text": "A) Reserved Instances for consistent baseline capacity", "isCorrect": false}, {"text": "B) Spot Instances with Auto Scaling groups across multiple AZs", "isCorrect": true}, {"text": "C) Savings Plans for compute usage commitment", "isCorrect": false}, {"text": "D) Dedicated Hosts for workload isolation", "isCorrect": false}]',
        'B) Spot Instances with Auto Scaling groups across multiple AZs',
        'Spot Instances can provide up to 90% savings for fault-tolerant batch workloads, and multiple AZs reduce interruption risk.',
        '{"summary": "Spot Instances optimal for batch processing:", "breakdown": ["Up to 90% cost savings vs On-Demand", "Workload tolerates interruptions (checkpoint resume)", "Predictable 4-hour window reduces interruption risk", "Multiple AZs provide capacity diversification"], "otherOptions": "A) Reserved Instances only ~75% savings, overkill for 4-hour workload\nC) Savings Plans better for consistent 24/7 usage\nD) Dedicated Hosts most expensive option"}'),
       (113, 13, 'AWS Cost Optimization - Storage Lifecycle', 'Application',
        'Domain 4: Design Cost-Optimized Architectures',
        'A company stores application logs in S3. Logs are actively analyzed for 30 days, occasionally accessed for 90 days, and must be retained for 7 years for compliance. Current storage costs are $1000/month in S3 Standard. Which lifecycle policy provides the BEST cost optimization?',
        '[{"text": "A) Transition to S3 IA after 30 days, then S3 Glacier after 90 days, then Deep Archive after 1 year", "isCorrect": true}, {"text": "B) Keep all data in S3 Standard for quick access when needed", "isCorrect": false}, {"text": "C) Move everything to S3 Glacier immediately after upload", "isCorrect": false}, {"text": "D) Use S3 Intelligent-Tiering for automatic optimization", "isCorrect": false}]',
        'A) Transition to S3 IA after 30 days, then S3 Glacier after 90 days, then Deep Archive after 1 year',
        'Lifecycle policy matching access patterns: S3 Standard for active use, S3 IA for occasional access, Glacier for long-term retention, Deep Archive for compliance.',
        '{"summary": "Optimal lifecycle transitions for log data:", "breakdown": ["Days 1-30: S3 Standard for active analysis", "Days 31-90: S3 IA for occasional access (50% savings)", "Days 91-365: S3 Glacier for backup (68% savings)", "Years 1-7: Deep Archive for compliance (77% savings)"], "otherOptions": "B) Standard storage most expensive for long-term retention\nC) Glacier immediate transition prevents active analysis\nD) Intelligent-Tiering adds overhead for predictable patterns"}'),
       (114, 14, 'AWS Cost Optimization - Database Costs', 'Expert', 'Domain 4: Design Cost-Optimized Architectures',
        'A startup''s MySQL database experiences unpredictable traffic patterns ranging from 5% to 95% CPU utilization throughout the day. Current RDS costs are $800/month with frequent over-provisioning during low-traffic periods. Which solution provides the BEST cost optimization while maintaining performance?',
        '[{"text": "A) Migrate to Aurora Serverless v2 with automatic scaling", "isCorrect": true}, {"text": "B) Use RDS with scheduled Auto Scaling based on time patterns", "isCorrect": false}, {"text": "C) Switch to smaller RDS instance and add read replicas", "isCorrect": false}, {"text": "D) Migrate to DynamoDB with on-demand billing", "isCorrect": false}]',
        'A) Migrate to Aurora Serverless v2 with automatic scaling',
        'Aurora Serverless v2 automatically scales compute capacity based on actual demand, eliminating over-provisioning costs during low-traffic periods.',
        '{"summary": "Aurora Serverless v2 benefits for variable workloads:", "breakdown": ["Automatic scaling from 0.5 to 128 ACUs based on demand", "Pay only for actual compute used (per-second billing)", "No over-provisioning during low-traffic periods", "MySQL compatibility maintains application compatibility"], "otherOptions": "B) Scheduled scaling can''t handle unpredictable patterns\nC) Smaller instance may cause performance issues during peaks\nD) DynamoDB requires application rewrite from MySQL"}'),
       (115, 15, 'AWS Cost Optimization - Data Transfer', 'Application',
        'Domain 4: Design Cost-Optimized Architectures',
        'A company transfers 100TB of data monthly from EC2 instances to users worldwide. Current data transfer costs are $9,000/month. The data consists of software downloads that rarely change. Which solution provides the MOST significant cost reduction?',
        '[{"text": "A) Use VPC endpoints to reduce data transfer charges", "isCorrect": false}, {"text": "B) Implement CloudFront CDN with S3 origin", "isCorrect": true}, {"text": "C) Move EC2 instances to regions with lower data transfer costs", "isCorrect": false}, {"text": "D) Compress all data before transmission", "isCorrect": false}]',
        'B) Implement CloudFront CDN with S3 origin',
        'CloudFront eliminates most internet data transfer charges and caches static content globally, dramatically reducing costs for software downloads.',
        '{"summary": "CloudFront cost optimization for static content:", "breakdown": ["No data transfer charges between S3 and CloudFront", "Reduced CloudFront to internet rates vs EC2 to internet", "Caching reduces origin data transfer for repeated downloads", "Global edge locations improve performance and reduce costs"], "otherOptions": "A) VPC endpoints help AWS service costs, not internet transfer\nC) Regional pricing differences minimal for internet transfer\nD) Compression helps but doesn''t address core transfer costs"}'),
       (116, 16, 'AWS Architecture - Decoupling', 'Application', 'Domain 1: Design Resilient Architectures',
        'A photo processing application receives images via API, processes them, and stores results. During peak times, the processing queue becomes backlogged and API requests start failing. Which decoupling strategy provides the BEST resilience?',
        '[{"text": "A) Increase EC2 instance size for faster processing", "isCorrect": false}, {"text": "B) Use SQS queue between API and processing with Auto Scaling based on queue depth", "isCorrect": true}, {"text": "C) Add more API Gateway throttling to reduce incoming requests", "isCorrect": false}, {"text": "D) Use Lambda functions instead of EC2 for processing", "isCorrect": false}]',
        'B) Use SQS queue between API and processing with Auto Scaling based on queue depth',
        'SQS decouples components allowing the API to accept requests even when processing is slow, with Auto Scaling adding capacity based on queue backlog.',
        '{"summary": "SQS decoupling benefits:", "breakdown": ["API remains responsive during processing delays", "Queue acts as buffer for variable processing times", "Auto Scaling responds to actual demand (queue depth)", "No lost requests during traffic spikes"], "otherOptions": "A) Larger instances don''t solve architecture coupling\nC) Throttling causes request failures\nD) Lambda has 15-minute timeout limit for processing"}'),
       (117, 17, 'AWS Performance - Auto Scaling', 'Expert', 'Domain 2: Design High-Performing Architectures',
        'An application experiences sudden traffic spikes that cause Auto Scaling to launch instances, but by the time instances are ready, traffic has decreased. This causes unnecessary costs. Which scaling strategy provides the BEST optimization?',
        '[{"text": "A) Use predictive scaling with machine learning to anticipate traffic patterns", "isCorrect": true}, {"text": "B) Decrease Auto Scaling cooldown periods for faster scaling", "isCorrect": false}, {"text": "C) Use larger instance types that can handle traffic spikes", "isCorrect": false}, {"text": "D) Implement step scaling with smaller scaling increments", "isCorrect": false}]',
        'A) Use predictive scaling with machine learning to anticipate traffic patterns',
        'Predictive scaling uses machine learning to forecast traffic and pre-scale capacity, avoiding the lag time of reactive scaling.',
        '{"summary": "Predictive scaling advantages:", "breakdown": ["ML forecasts traffic patterns to pre-scale capacity", "Reduces lag time between traffic spike and available capacity", "Prevents over-scaling by understanding traffic duration", "Works with historical patterns and scheduled events"], "otherOptions": "B) Faster scaling still reactive, doesn''t solve core timing issue\nC) Over-provisioning increases costs unnecessarily\nD) Step scaling still reactive to traffic changes"}'),
       (118, 18, 'AWS Security - Secrets Management', 'Application', 'Domain 3: Design Secure Applications',
        'A microservices application needs to securely store and rotate database passwords, API keys, and certificates. The solution must integrate with RDS automatic rotation and provide audit trails. Which service combination is MOST appropriate?',
        '[{"text": "A) Store secrets in S3 with bucket encryption and versioning", "isCorrect": false}, {"text": "B) Use AWS Secrets Manager with CloudTrail logging", "isCorrect": true}, {"text": "C) Store secrets in Systems Manager Parameter Store with SecureString", "isCorrect": false}, {"text": "D) Use AWS KMS to encrypt secrets stored in DynamoDB", "isCorrect": false}]',
        'B) Use AWS Secrets Manager with CloudTrail logging',
        'Secrets Manager provides automatic rotation for RDS, integration with AWS services, and complete audit trails through CloudTrail.',
        '{"summary": "Secrets Manager comprehensive benefits:", "breakdown": ["Automatic rotation for RDS, Redshift, DocumentDB", "Native integration with AWS services", "Complete audit trail through CloudTrail", "Secure retrieval with fine-grained IAM permissions"], "otherOptions": "A) S3 not designed for secrets management\nC) Parameter Store lacks automatic rotation for databases\nD) Custom solution requires building rotation logic"}'),
       (119, 19, 'AWS Architecture - Service Limits', 'Expert', 'Domain 2: Design High-Performing Architectures',
        'An application needs to upload files up to 500GB to S3. Users report upload failures and timeouts. The application currently uses single PUT requests. Which solution addresses the root cause of the upload failures?',
        '[{"text": "A) Enable S3 Transfer Acceleration for faster uploads", "isCorrect": false}, {"text": "B) Implement S3 multipart upload for large files", "isCorrect": true}, {"text": "C) Use CloudFront for upload optimization", "isCorrect": false}, {"text": "D) Increase application timeout settings", "isCorrect": false}]',
        'B) Implement S3 multipart upload for large files',
        'S3 has a 5GB limit for single PUT operations. Files larger than 5GB require multipart upload, which is recommended for files over 100MB.',
        '{"summary": "S3 upload limits and best practices:", "breakdown": ["Single PUT limit: 5GB maximum file size", "Multipart upload: Required for files >5GB, recommended >100MB", "500GB files: Must use multipart upload", "Improves reliability with retry capability per part"], "otherOptions": "A) Transfer Acceleration helps speed, doesn''t solve size limit\nC) CloudFront not designed for large file uploads\nD) Timeouts don''t solve the 5GB PUT limit"}'),
       (120, 20, 'AWS Architecture - Lambda Limitations', 'Expert', 'Domain 1: Design Resilient Architectures',
        'A data processing function needs to run for 20 minutes to process large datasets. The current Lambda function times out after 15 minutes. Which solution provides the BEST architecture for this requirement?',
        '[{"text": "A) Increase Lambda timeout to maximum 15 minutes and optimize code", "isCorrect": false}, {"text": "B) Break processing into smaller Lambda functions using Step Functions", "isCorrect": true}, {"text": "C) Use Lambda with SQS to process data in smaller chunks", "isCorrect": false}, {"text": "D) Migrate to ECS Fargate for longer-running tasks", "isCorrect": false}]',
        'B) Break processing into smaller Lambda functions using Step Functions',
        'Lambda has a hard 15-minute timeout limit. Step Functions can orchestrate multiple Lambda functions to handle longer workflows.',
        '{"summary": "Lambda timeout limits and solutions:", "breakdown": ["Lambda maximum timeout: 15 minutes (hard limit)", "Step Functions: Can orchestrate multi-step workflows", "Break down processing: Multiple Lambda functions in sequence", "State management: Step Functions handles workflow state"], "otherOptions": "A) 15 minutes is the maximum timeout\nC) SQS helps with async processing but doesn''t solve timeout\nD) ECS Fargate valid but Step Functions more serverless-native"}'),
       (121, 21, 'AWS Cost Optimization - Compute Savings', 'Expert', 'Domain 4: Design Cost-Optimized Architectures',
        'A company runs a web application with the following pattern: 2 instances minimum 24/7, scales to 10 instances during business hours (9 AM-5 PM weekdays), and spikes to 50 instances during monthly sales (first 3 days). Current monthly On-Demand costs are $15,000. Which purchasing strategy provides MAXIMUM cost savings while maintaining performance?',
        '[{"text": "A) All-Upfront Reserved Instances for 10 instances, On-Demand for the rest", "isCorrect": false}, {"text": "B) Reserved Instances for 2 instances, Savings Plans for business hours capacity, Spot for sales spikes", "isCorrect": true}, {"text": "C) Compute Savings Plans for all capacity needs", "isCorrect": false}, {"text": "D) Scheduled Reserved Instances for business hours, On-Demand for spikes", "isCorrect": false}]',
        'B) Reserved Instances for 2 instances, Savings Plans for business hours capacity, Spot for sales spikes',
        'Layered purchasing strategy: RIs for baseline (70% savings), Savings Plans for predictable scaling (66% savings), Spot for spikes (up to 90% savings).',
        '{"summary": "Optimal cost strategy for variable workloads:", "breakdown": ["Reserved Instances: 2 baseline instances (24/7) - 70% savings", "Savings Plans: 8 additional instances for business hours - 66% savings", "Spot Instances: 40 instances for monthly sales - up to 90% savings", "Total savings: Approximately 75% reduction from $15,000"], "otherOptions": "A) Over-provisioning RIs for 10 instances wastes money nights/weekends\nC) Savings Plans less discount than RIs for baseline\nD) Scheduled RIs discontinued, wouldn''t cover spikes"}'),
       (122, 22, 'AWS Cost Optimization - Data Processing', 'Application',
        'Domain 4: Design Cost-Optimized Architectures',
        'A data analytics company processes 10TB of log files daily using EMR clusters. Processing takes 4 hours and runs once daily at 2 AM. The current approach uses On-Demand instances costing $8,000/month. Which architecture change provides the BEST cost optimization?',
        '[{"text": "A) Migrate to AWS Glue for serverless ETL processing", "isCorrect": false}, {"text": "B) Use EMR on Spot Instances with diverse instance types across multiple AZs", "isCorrect": true}, {"text": "C) Pre-process data with Lambda before EMR to reduce cluster size", "isCorrect": false}, {"text": "D) Use smaller EMR cluster running 24/7 with Reserved Instances", "isCorrect": false}]',
        'B) Use EMR on Spot Instances with diverse instance types across multiple AZs',
        'EMR on Spot Instances can provide 50-80% cost savings for batch processing workloads that can tolerate interruptions.',
        '{"summary": "EMR Spot Instance optimization:", "breakdown": ["Spot savings: 50-80% reduction from On-Demand pricing", "Instance diversity: Reduces interruption risk", "Multiple AZs: Ensures capacity availability", "4-hour processing window: Perfect for Spot reliability"], "otherOptions": "A) Glue more expensive for 10TB daily processing\nC) Lambda timeout and cost limitations for TB-scale data\nD) 24/7 cluster wasteful for 4-hour daily job"}'),
       (123, 23, 'AWS Cost Optimization - Storage Optimization', 'Expert',
        'Domain 4: Design Cost-Optimized Architectures',
        'A media company stores 500TB of video content in S3 Standard. Analytics show: 10% accessed daily (hot), 30% accessed weekly (warm), 60% accessed rarely but unpredictably. Retrieval must complete within 12 hours. Current cost is $11,500/month. Which storage strategy provides optimal cost reduction?',
        '[{"text": "A) S3 Intelligent-Tiering for all content with archive configuration", "isCorrect": true}, {"text": "B) S3 Standard for hot, S3 IA for warm, S3 Glacier for cold data", "isCorrect": false}, {"text": "C) S3 One Zone-IA for all content to reduce costs", "isCorrect": false}, {"text": "D) S3 Standard for hot, S3 Glacier Instant Retrieval for all other content", "isCorrect": false}]',
        'A) S3 Intelligent-Tiering for all content with archive configuration',
        'S3 Intelligent-Tiering automatically optimizes storage costs for unpredictable access patterns without retrieval fees, perfect for this use case.',
        '{"summary": "Intelligent-Tiering benefits for unpredictable access:", "breakdown": ["Automatic tiering: No manual lifecycle policies needed", "No retrieval fees: Critical for unpredictable access", "Archive configuration: Moves rarely accessed objects to archive tiers", "Cost savings: Up to 70% reduction while maintaining performance"], "otherOptions": "B) Manual lifecycle rules don''t handle unpredictable access well\nC) One Zone-IA risky for 500TB of valuable content\nD) Glacier Instant Retrieval has retrieval fees for 60% of content"}'),
       (124, 24, 'AWS Security - ML Data Protection', 'Application', 'Domain 3: Design Secure Applications',
        'A healthcare company uses Amazon Comprehend Medical to analyze patient records and Amazon Textract for insurance form processing. They must ensure HIPAA compliance and data isolation. Which security configuration is MOST appropriate?',
        '[{"text": "A) Use default service encryption and enable CloudTrail logging", "isCorrect": false}, {"text": "B) Enable VPC endpoints, use customer-managed KMS keys, and create separate IAM roles per service", "isCorrect": true}, {"text": "C) Process all data in a single private subnet with network ACLs", "isCorrect": false}, {"text": "D) Use AWS managed keys and restrict access with S3 bucket policies", "isCorrect": false}]',
        'B) Enable VPC endpoints, use customer-managed KMS keys, and create separate IAM roles per service',
        'VPC endpoints ensure private connectivity, customer-managed KMS keys provide encryption control, and separate IAM roles enable least privilege access for HIPAA compliance.',
        '{"summary": "HIPAA-compliant ML service configuration:", "breakdown": ["VPC endpoints: Keep data traffic within AWS network", "Customer-managed KMS: Full control over encryption keys", "Separate IAM roles: Least privilege per service", "Audit trail: CloudTrail logs all API calls for compliance"], "otherOptions": "A) Default encryption insufficient for HIPAA requirements\nC) Network ACLs don''t provide service-level isolation\nD) AWS managed keys don''t provide sufficient control"}'),
       (125, 25, 'AWS Security - Container Security', 'Expert', 'Domain 3: Design Secure Applications',
        'A fintech application runs on EKS with sensitive payment processing workloads. The security team requires pod-level network isolation, secrets rotation every 30 days, and runtime threat detection. Which solution meets ALL requirements?',
        '[{"text": "A) Calico network policies, AWS Secrets Manager, and Amazon Inspector", "isCorrect": false}, {"text": "B) AWS App Mesh, Systems Manager Parameter Store, and CloudWatch Container Insights", "isCorrect": false}, {"text": "C) EKS security groups for pods, AWS Secrets Manager with rotation, and Amazon GuardDuty EKS protection", "isCorrect": true}, {"text": "D) Network ACLs, HashiCorp Vault, and AWS Config rules", "isCorrect": false}]',
        'C) EKS security groups for pods, AWS Secrets Manager with rotation, and Amazon GuardDuty EKS protection',
        'EKS security groups provide pod-level isolation, Secrets Manager handles automatic rotation, and GuardDuty EKS protection offers runtime threat detection.',
        '{"summary": "Comprehensive EKS security architecture:", "breakdown": ["Security groups for pods: Fine-grained network isolation at pod level", "Secrets Manager: Automatic 30-day rotation with EKS integration", "GuardDuty EKS: Runtime threat detection and anomaly detection", "Native AWS integration: Simplified management and compliance"], "otherOptions": "A) Inspector scans vulnerabilities, not runtime threats\nB) App Mesh is service mesh, not security focused\nD) Network ACLs too coarse for pod-level control"}'),
       (126, 26, 'AWS Architecture - Event-Driven Resilience', 'Expert', 'Domain 1: Design Resilient Architectures',
        'An e-commerce platform must process orders from multiple channels (web, mobile, partners) with different formats. The system must handle 50,000 orders/hour during sales, ensure no order loss, and enable new channel integration without affecting existing ones. Which architecture provides the BEST resilience and scalability?',
        '[{"text": "A) API Gateway with Lambda functions writing directly to DynamoDB", "isCorrect": false}, {"text": "B) Amazon EventBridge with channel-specific rules routing to SQS queues and containerized processors", "isCorrect": true}, {"text": "C) Kinesis Data Streams with Lambda consumers for each channel", "isCorrect": false}, {"text": "D) Application Load Balancer with Auto Scaling EC2 instances", "isCorrect": false}]',
        'B) Amazon EventBridge with channel-specific rules routing to SQS queues and containerized processors',
        'EventBridge provides event routing with schema registry, SQS ensures message durability, and containerized processors enable independent scaling per channel.',
        '{"summary": "Event-driven architecture benefits:", "breakdown": ["EventBridge: Central event router with content-based filtering", "Schema Registry: Validates events and enables discovery", "SQS queues: Guaranteed message delivery and buffering", "Container isolation: Each channel processor scales independently"], "otherOptions": "A) Direct writes risk data loss during high load\nC) Kinesis overkill for transactional data, complex scaling\nD) Monolithic approach, difficult to add new channels"}'),
       (127, 27, 'AWS Architecture - Disaster Recovery', 'Application', 'Domain 1: Design Resilient Architectures',
        'A financial services application requires RPO of 1 hour and RTO of 4 hours. The application uses Aurora PostgreSQL, EC2 instances with custom AMIs, and stores documents in S3. The DR solution must be cost-effective. Which approach meets these requirements?',
        '[{"text": "A) Aurora Global Database with standby EC2 instances in secondary region", "isCorrect": false}, {"text": "B) Aurora backups to secondary region, AMIs copied cross-region, S3 cross-region replication, and AWS Backup", "isCorrect": true}, {"text": "C) Multi-region active-active deployment with Route 53 failover", "isCorrect": false}, {"text": "D) Database snapshots and EC2 snapshots copied hourly to secondary region", "isCorrect": false}]',
        'B) Aurora backups to secondary region, AMIs copied cross-region, S3 cross-region replication, and AWS Backup',
        'Backup-based DR meets RPO/RTO requirements cost-effectively: Aurora automated backups (continuous), AMIs ready for quick launch, S3 CRR for documents.',
        '{"summary": "Cost-effective DR strategy components:", "breakdown": ["Aurora backups: Point-in-time recovery within 5 minutes (meets 1-hour RPO)", "Cross-region AMI copies: Ready for quick EC2 launch", "S3 CRR: Near real-time replication for documents", "AWS Backup: Centralized backup management and compliance"], "otherOptions": "A) Global Database expensive for 4-hour RTO requirement\nC) Active-active overkill and complex for these requirements\nD) Manual snapshots miss continuous data changes"}'),
       (128, 28, 'AWS Performance - API Optimization', 'Application', 'Domain 2: Design High-Performing Architectures',
        'A mobile app backend serves personalized content to 10 million users globally. The API Gateway shows high latency for data aggregation from multiple microservices. Each request makes 5-7 service calls with 200ms average latency per call. Which solution provides the BEST performance improvement?',
        '[{"text": "A) Implement API Gateway caching for all endpoints", "isCorrect": false}, {"text": "B) Use AWS AppSync with GraphQL to batch and parallelize service calls", "isCorrect": true}, {"text": "C) Add ElastiCache in front of each microservice", "isCorrect": false}, {"text": "D) Increase Lambda memory allocation for faster processing", "isCorrect": false}]',
        'B) Use AWS AppSync with GraphQL to batch and parallelize service calls',
        'AppSync with GraphQL reduces API calls through query batching and parallel resolution, dramatically reducing overall latency for aggregated data.',
        '{"summary": "AppSync optimization benefits:", "breakdown": ["Query batching: Single request instead of 5-7 calls", "Parallel resolution: Service calls execute simultaneously", "Reduced latency: From 1000-1400ms to 200-300ms total", "Caching built-in: Further performance improvements"], "otherOptions": "A) Caching doesn''t help with personalized content\nC) Still requires serial service calls\nD) Lambda memory doesn''t solve aggregation latency"}'),
       (129, 29, 'AWS Performance - ML Inference', 'Expert', 'Domain 2: Design High-Performing Architectures',
        'A video streaming platform needs real-time content moderation using computer vision ML models. They process 1,000 concurrent streams with <100ms inference latency requirement. Current SageMaker endpoint shows 300ms latency. Which optimization provides the required performance?',
        '[{"text": "A) Use larger SageMaker instance types with more GPUs", "isCorrect": false}, {"text": "B) Deploy models to Lambda with Provisioned Concurrency", "isCorrect": false}, {"text": "C) Implement SageMaker multi-model endpoints with edge deployment using AWS IoT Greengrass", "isCorrect": false}, {"text": "D) Use Amazon EC2 Inf1 instances with AWS Neuron for optimized inference", "isCorrect": true}]',
        'D) Use Amazon EC2 Inf1 instances with AWS Neuron for optimized inference',
        'EC2 Inf1 instances with AWS Inferentia chips provide the lowest latency and highest throughput for real-time ML inference at scale.',
        '{"summary": "Inf1 instance optimization benefits:", "breakdown": ["AWS Inferentia chips: Purpose-built for ML inference", "Sub-100ms latency: Achievable with optimized models", "High throughput: Handle 1,000 concurrent streams", "Cost-effective: Up to 70% lower cost than GPU instances"], "otherOptions": "A) Larger instances don''t guarantee latency reduction\nB) Lambda cold starts and limits prevent consistent <100ms\nC) Edge deployment adds complexity without meeting core requirement"}'),
       (130, 30, 'AWS Cost Optimization - Hybrid Architecture', 'Expert',
        'Domain 4: Design Cost-Optimized Architectures',
        'A company wants to modernize their on-premises data warehouse (50TB) to AWS. They need to maintain some on-premises reporting tools while enabling cloud analytics. Budget is limited to $5,000/month. Query patterns: 20% hot data (daily), 30% warm (weekly), 50% cold (monthly). Which architecture provides the BEST cost optimization?',
        '[{"text": "A) Migrate everything to Redshift with Reserved Instances", "isCorrect": false}, {"text": "B) Use Redshift for hot data, Redshift Spectrum for warm/cold data in S3, with AWS Direct Connect", "isCorrect": true}, {"text": "C) Keep all data on-premises and use Athena federated queries", "isCorrect": false}, {"text": "D) Use RDS PostgreSQL with read replicas for all data", "isCorrect": false}]',
        'B) Use Redshift for hot data, Redshift Spectrum for warm/cold data in S3, with AWS Direct Connect',
        'Tiered approach with Redshift for hot data and Spectrum for S3-based warm/cold data provides optimal price-performance within budget constraints.',
        '{"summary": "Hybrid data warehouse optimization:", "breakdown": ["Redshift (10TB): ~$2,500/month for hot data performance", "S3 + Spectrum (40TB): ~$1,000/month for warm/cold storage", "Direct Connect: ~$1,500/month for reliable hybrid connectivity", "Total: ~$5,000/month within budget with optimal performance"], "otherOptions": "A) Full Redshift for 50TB exceeds budget significantly\nC) Athena federated queries too slow for hot data needs\nD) RDS not optimized for analytics workloads, would exceed budget"}'),
       (131, 31, 'AWS Cost Optimization - Serverless Economics', 'Expert',
        'Domain 4: Design Cost-Optimized Architectures',
        'A news aggregation service processes 50 million articles monthly with unpredictable spikes during breaking news (up to 10x normal load). Current EC2-based architecture costs $12,000/month and struggles with spike handling. Articles average 5KB, require text analysis, and must be searchable within 1 minute. Which serverless architecture provides the best cost optimization while meeting requirements?',
        '[{"text": "A) API Gateway → Lambda → DynamoDB with ElasticSearch", "isCorrect": false}, {"text": "B) EventBridge → SQS → Lambda with concurrent execution limits → S3 → Athena with Glue crawlers", "isCorrect": true}, {"text": "C) Kinesis Data Streams → Kinesis Analytics → RDS", "isCorrect": false}, {"text": "D) Step Functions orchestrating multiple Lambda functions → Aurora Serverless", "isCorrect": false}]',
        'B) EventBridge → SQS → Lambda with concurrent execution limits → S3 → Athena with Glue crawlers',
        'EventBridge with SQS provides event buffering for spikes, Lambda with concurrency limits controls costs, S3 offers cheap storage, and Athena enables SQL searches without database costs.',
        '{"summary": "Serverless architecture cost optimization:", "breakdown": ["EventBridge + SQS: Handles 10x spikes without over-provisioning", "Lambda concurrency limits: Prevents runaway costs during spikes", "S3 storage: $0.023/GB vs database storage at $0.10/GB", "Athena queries: Pay-per-query ($5/TB scanned) vs always-on database"], "otherOptions": "A) ElasticSearch cluster costs $3000+/month minimum\nC) Kinesis Analytics expensive for sporadic spikes\nD) Aurora Serverless still has minimum capacity costs"}'),
       (132, 32, 'AWS Cost Optimization - Container Right-Sizing', 'Application',
        'Domain 4: Design Cost-Optimized Architectures',
        'A microservices application runs 40 containers on ECS with varying resource needs: 10 containers need 4 vCPU/8GB (API servers), 20 need 1 vCPU/2GB (workers), and 10 need 0.5 vCPU/1GB (sidecars). Current costs using m5.4xlarge instances are $8,000/month with 35% CPU utilization. Which optimization strategy provides maximum cost savings?',
        '[{"text": "A) Switch to larger instances for better CPU:memory ratio", "isCorrect": false}, {"text": "B) Use ECS capacity providers with mixed instance types and Spot for workers", "isCorrect": true}, {"text": "C) Migrate all containers to Fargate with right-sized task definitions", "isCorrect": false}, {"text": "D) Implement cluster auto-scaling based on CPU utilization", "isCorrect": false}]',
        'B) Use ECS capacity providers with mixed instance types and Spot for workers',
        'Capacity providers enable mixing instance types optimized for different workloads, while Spot instances for stateless workers can reduce costs by 70-90%.',
        '{"summary": "Container cost optimization strategy:", "breakdown": ["Mixed instances: c5 for CPU-intensive, r5 for memory-intensive", "Spot for workers: 70% savings on 20 containers (stateless)", "Better bin packing: Increases utilization from 35% to 75%", "Total savings: Approximately 60% reduction from $8,000"], "otherOptions": "A) Larger instances worsen utilization problems\nC) Fargate more expensive for predictable workloads\nD) Auto-scaling doesn''t address instance type mismatch"}'),
       (133, 33, 'AWS ML Services Integration', 'Application', 'Domain 3: Design Secure Applications',
        'A social media platform needs to detect inappropriate content in user posts (text and images) in real-time. They process 1 million posts daily and must comply with COPPA regulations for users under 13. Which combination of AWS services provides comprehensive content moderation with compliance controls?',
        '[{"text": "A) Amazon Comprehend for text and custom SageMaker model for images", "isCorrect": false}, {"text": "B) Amazon Rekognition Content Moderation, Amazon Comprehend toxicity detection, with Lambda age-gating logic", "isCorrect": true}, {"text": "C) Amazon Textract for text extraction and Amazon Personalize for content filtering", "isCorrect": false}, {"text": "D) AWS Glue for data preparation and Amazon Forecast for predicting inappropriate content", "isCorrect": false}]',
        'B) Amazon Rekognition Content Moderation, Amazon Comprehend toxicity detection, with Lambda age-gating logic',
        'Rekognition provides pre-trained image moderation, Comprehend detects toxic text, and Lambda implements age-appropriate filtering logic for COPPA compliance.',
        '{"summary": "Content moderation architecture components:", "breakdown": ["Rekognition: Detects inappropriate images (violence, adult content)", "Comprehend: Identifies toxic text, PII, and sentiment", "Lambda age-gating: Applies stricter rules for users under 13", "Real-time processing: Sub-second moderation decisions"], "otherOptions": "A) Custom models require training data and maintenance\nC) Textract is for document processing, not moderation\nD) Forecast predicts time-series data, not content issues"}'),
       (134, 34, 'AWS Architecture - IoT and Edge', 'Expert', 'Domain 1: Design Resilient Architectures',
        'A manufacturing company operates 500 factories with 100 IoT sensors each, generating 1KB messages every second. They need local anomaly detection with sub-second response, cloud-based analytics, and must operate during internet outages. Which architecture provides the required edge computing capabilities?',
        '[{"text": "A) Direct IoT Core ingestion with Kinesis Analytics for anomaly detection", "isCorrect": false}, {"text": "B) AWS IoT Greengrass with Lambda functions, local message routing, and intermittent cloud sync", "isCorrect": true}, {"text": "C) EC2 instances in each factory with VPN connections to cloud", "isCorrect": false}, {"text": "D) AWS Outposts in each factory for local processing", "isCorrect": false}]',
        'B) AWS IoT Greengrass with Lambda functions, local message routing, and intermittent cloud sync',
        'IoT Greengrass enables local Lambda execution for sub-second anomaly detection, continues operating offline, and synchronizes with cloud when connected.',
        '{"summary": "Edge computing with IoT Greengrass benefits:", "breakdown": ["Local Lambda: Sub-second anomaly detection at edge", "Offline operation: Continues processing during outages", "Efficient sync: Batches data for cloud analytics when connected", "Cost-effective: No need for servers in 500 locations"], "otherOptions": "A) Requires constant internet, no local processing\nC) Managing 500 EC2 instances operationally complex\nD) Outposts overkill and expensive for IoT workloads"}'),
       (135, 35, 'AWS Cost Optimization - Streaming Data', 'Application',
        'Domain 4: Design Cost-Optimized Architectures',
        'A ride-sharing app ingests 100,000 location updates per second during peak hours (6-9 AM, 5-8 PM) but only 5,000 per second during off-peak. Current Kinesis Data Streams with provisioned shards costs $15,000/month. Real-time processing is required only during peak hours; batch processing is acceptable off-peak. Which architecture reduces costs while meeting requirements?',
        '[{"text": "A) Kinesis Data Streams with auto-scaling shards based on traffic", "isCorrect": false}, {"text": "B) Kinesis Data Streams On-Demand for peaks, Kinesis Firehose to S3 for off-peak batch", "isCorrect": true}, {"text": "C) Replace with SQS queues and Lambda functions", "isCorrect": false}, {"text": "D) DynamoDB Streams with Lambda triggers", "isCorrect": false}]',
        'B) Kinesis Data Streams On-Demand for peaks, Kinesis Firehose to S3 for off-peak batch',
        'Kinesis On-Demand eliminates over-provisioning costs while Firehose to S3 provides cost-effective batch processing during off-peak hours.',
        '{"summary": "Streaming cost optimization strategy:", "breakdown": ["Kinesis On-Demand: Pay only for actual throughput", "Peak hours: Real-time processing as required", "Off-peak Firehose: 90% cheaper than real-time streams", "Estimated savings: 65% reduction from $15,000/month"], "otherOptions": "A) Auto-scaling still requires minimum shards\nC) SQS lacks streaming semantics for real-time\nD) DynamoDB Streams tied to table operations"}'),
       (136, 36, 'AWS Architecture - Multi-Region', 'Expert', 'Domain 1: Design Resilient Architectures',
        'A global trading platform requires <10ms latency for 99% of users across US, EU, and APAC regions. The platform processes 1 million trades daily with strict consistency requirements. Each trade must be reflected globally within 100ms. Current single-region deployment shows 150ms+ latency for distant users. Which multi-region architecture best meets these requirements?',
        '[{"text": "A) Deploy full stack in each region with eventual consistency", "isCorrect": false}, {"text": "B) Aurora Global Database with local read replicas, Route 53 geolocation routing, and DynamoDB Global Tables for session data", "isCorrect": true}, {"text": "C) Single region with CloudFront caching for static content", "isCorrect": false}, {"text": "D) Active-active regions with asynchronous replication", "isCorrect": false}]',
        'B) Aurora Global Database with local read replicas, Route 53 geolocation routing, and DynamoDB Global Tables for session data',
        'Aurora Global Database provides <1 second replication with strong consistency, local read replicas ensure <10ms reads, while DynamoDB Global Tables handle session state with automatic conflict resolution.',
        '{"summary": "Multi-region architecture for low latency trading:", "breakdown": ["Aurora Global: <1 second cross-region replication", "Local read replicas: <10ms read latency per region", "Write forwarding: Maintains consistency for trades", "DynamoDB Global Tables: Multi-master for session data"], "otherOptions": "A) Eventual consistency violates trade requirements\nC) CloudFront doesn''t help with dynamic trade data\nD) Async replication can''t guarantee 100ms consistency"}'),
       (137, 37, 'AWS Security - Zero Trust Architecture', 'Expert', 'Domain 3: Design Secure Applications',
        'A financial services company needs to implement zero-trust access to their AWS environment for 500 developers across 50 teams. Requirements include: no long-lived credentials, audit trail of all actions, team-based access boundaries, and integration with existing Okta SSO. Which solution provides the most comprehensive zero-trust implementation?',
        '[{"text": "A) IAM users with MFA and IP restrictions for each developer", "isCorrect": false}, {"text": "B) AWS SSO with Okta integration, Permission Sets with session tags, and CloudTrail with EventBridge rules", "isCorrect": true}, {"text": "C) Cognito user pools with custom authorizers", "isCorrect": false}, {"text": "D) IAM roles with external ID and session policies", "isCorrect": false}]',
        'B) AWS SSO with Okta integration, Permission Sets with session tags, and CloudTrail with EventBridge rules',
        'AWS SSO provides temporary credentials, Okta integration enables SAML-based authentication, Permission Sets with session tags enforce team boundaries, while CloudTrail ensures complete audit trails.',
        '{"summary": "Zero-trust implementation components:", "breakdown": ["AWS SSO: No long-lived credentials, automatic rotation", "Permission Sets: Team-based access with session tags", "Okta SAML: Leverages existing identity provider", "CloudTrail + EventBridge: Real-time audit and alerting"], "otherOptions": "A) IAM users require long-lived access keys\nC) Cognito designed for application users, not AWS access\nD) Missing centralized management for 500 developers"}'),
       (138, 38, 'AWS Performance - Hybrid Networking', 'Application', 'Domain 2: Design High-Performing Architectures',
        'A media company needs to transfer 50TB of daily video content from on-premises editing stations to S3 for processing. Current internet upload takes 20 hours, causing production delays. The solution must complete transfers within 4 hours and support 100 concurrent editor workstations. Which approach best meets these performance requirements?',
        '[{"text": "A) Multiple Site-to-Site VPN connections with ECMP", "isCorrect": false}, {"text": "B) AWS Direct Connect with Virtual Interfaces and S3 Transfer Acceleration", "isCorrect": true}, {"text": "C) Storage Gateway File Gateway with local cache", "isCorrect": false}, {"text": "D) Daily AWS Snowball Edge shipments", "isCorrect": false}]',
        'B) AWS Direct Connect with Virtual Interfaces and S3 Transfer Acceleration',
        'Direct Connect provides dedicated bandwidth for consistent transfer speeds, VIFs enable private S3 connectivity, and Transfer Acceleration optimizes the upload path for concurrent workstations.',
        '{"summary": "High-performance hybrid transfer solution:", "breakdown": ["Direct Connect: 10Gbps dedicated bandwidth", "VIF to S3: Private connectivity, no internet congestion", "Transfer Acceleration: Optimizes for 100 concurrent uploads", "50TB in 4 hours: Requires ~3.5Gbps sustained throughput"], "otherOptions": "A) VPN limited to ~1.25Gbps aggregate bandwidth\nC) File Gateway adds latency and cache limitations\nD) Snowball shipping time exceeds 4-hour requirement"}'),
       (139, 39, 'AWS Architecture - Event-Driven Serverless', 'Expert',
        'Domain 2: Design High-Performing Architectures',
        'An e-commerce platform needs to process order events with multiple downstream systems: inventory (must process once), shipping (at-least-once), analytics (can tolerate duplicates), and email (exactly-once). Order volume varies from 100/minute to 10,000/minute during flash sales. Which event-driven architecture ensures correct delivery semantics for each consumer?',
        '[{"text": "A) SNS with SQS queues for all consumers", "isCorrect": false}, {"text": "B) EventBridge with rules routing to SQS FIFO for inventory, standard SQS for shipping, Kinesis for analytics, and Step Functions for email", "isCorrect": true}, {"text": "C) Kinesis Data Streams with Lambda consumers", "isCorrect": false}, {"text": "D) Direct Lambda invocations with error handling", "isCorrect": false}]',
        'B) EventBridge with rules routing to SQS FIFO for inventory, standard SQS for shipping, Kinesis for analytics, and Step Functions for email',
        'EventBridge provides content-based routing, SQS FIFO ensures exactly-once processing, standard SQS handles at-least-once, Kinesis supports analytics replay, and Step Functions manages email workflow state.',
        '{"summary": "Event delivery semantics per consumer:", "breakdown": ["SQS FIFO: Exactly-once for critical inventory updates", "Standard SQS: At-least-once with retry for shipping", "Kinesis: Replay capability for analytics reprocessing", "Step Functions: Manages email state to prevent duplicates"], "otherOptions": "A) SNS+SQS doesn''t provide exactly-once semantics\nC) Kinesis requires all consumers to process in order\nD) Direct invocations lack delivery guarantees"}'),
       (140, 40, 'AWS Cost Optimization - Modern Architectures', 'Expert',
        'Domain 4: Design Cost-Optimized Architectures',
        'A SaaS company runs 200 customer environments, each with identical architecture: ALB, 2-10 EC2 instances, RDS MySQL, and 50GB of S3 storage. Monthly costs are $400,000 with only 30% resource utilization. Customers require isolation for compliance. Which modernization approach provides maximum cost reduction while maintaining isolation?',
        '[{"text": "A) Containerize applications and run all customers on shared EKS cluster", "isCorrect": false}, {"text": "B) AWS App Runner with customer-specific environments, Aurora Serverless v2, and S3 bucket policies", "isCorrect": true}, {"text": "C) Lambda functions with RDS Proxy and separate VPCs", "isCorrect": false}, {"text": "D) Consolidate all customers into a single multi-tenant application", "isCorrect": false}]',
        'B) AWS App Runner with customer-specific environments, Aurora Serverless v2, and S3 bucket policies',
        'App Runner provides serverless compute with environment isolation, Aurora Serverless v2 scales database resources based on actual usage, eliminating overprovisioning while maintaining compliance isolation.',
        '{"summary": "Serverless multi-tenant cost optimization:", "breakdown": ["App Runner: Pay-per-request pricing, no idle EC2 costs", "Aurora Serverless v2: Scales down to 0.5 ACU when idle", "Environment isolation: Separate App Runner services per customer", "Estimated savings: 70% reduction through utilization-based pricing"], "otherOptions": "A) Shared cluster violates compliance isolation requirements\nC) Lambda cold starts problematic for web applications\nD) Multi-tenant architecture breaks compliance requirements"}'),
       (141, 41, 'AWS Storage Concepts', 'Knowledge', 'Domain 3: Design High-Performing Architectures',
        'A company needs to choose between different AWS storage types for various workloads. Which statement BEST describes the appropriate use cases for each storage type?',
        '[{"text": "A) Use object storage for databases, block storage for file shares, file storage for web content", "isCorrect": false}, {"text": "B) Use block storage for databases, file storage for shared access, object storage for web content", "isCorrect": true}, {"text": "C) Use file storage for databases, object storage for shared access, block storage for web content", "isCorrect": false}, {"text": "D) All storage types can be used interchangeably for any workload", "isCorrect": false}]',
        'B) Use block storage for databases, file storage for shared access, object storage for web content',
        'Block storage (EBS) provides high IOPS for databases, file storage (EFS) enables shared access, object storage (S3) scales for web content.',
        '{"summary": "AWS storage type characteristics:", "breakdown": ["Block storage (EBS): High IOPS, low latency, ideal for databases and boot volumes", "File storage (EFS): POSIX-compliant, shared access, good for content repositories", "Object storage (S3): Auto-scaling, REST API, perfect for web content and backups", "Each type optimized for specific access patterns and performance requirements"], "otherOptions": "A) Object storage not suitable for database IOPS requirements\nC) File storage lacks the IOPS performance needed for databases\nD) Each storage type has specific use cases and limitations"}'),
       (142, 42, 'AWS Storage Concepts', 'Application', 'Domain 2: Design Resilient Architectures',
        'An enterprise is migrating from on-premises and needs to maintain compatibility with existing storage protocols. They use NFS for Unix systems, SMB for Windows, and iSCSI for SAN storage. Which AWS services provide the BEST protocol compatibility?',
        '[{"text": "A) S3 for all protocols using third-party gateways", "isCorrect": false}, {"text": "B) EFS for NFS, FSx for Windows for SMB, Storage Gateway for iSCSI", "isCorrect": true}, {"text": "C) EBS for all protocols using EC2 instance configuration", "isCorrect": false}, {"text": "D) DataSync for protocol translation across all storage types", "isCorrect": false}]',
        'B) EFS for NFS, FSx for Windows for SMB, Storage Gateway for iSCSI',
        'AWS provides native protocol support: EFS (NFS), FSx for Windows (SMB), Storage Gateway (iSCSI) for seamless migration.',
        '{"summary": "Native AWS protocol support:", "breakdown": ["EFS: Native NFS v4.1, POSIX-compliant, Linux/Unix compatibility", "FSx for Windows: Native SMB, Active Directory integration", "Storage Gateway: iSCSI interface, hybrid cloud connectivity", "No application changes required for protocol compatibility"], "otherOptions": "A) S3 doesn''t natively support NFS/SMB/iSCSI protocols\nC) EBS provides block storage but not protocol-level compatibility\nD) DataSync is for data transfer, not protocol translation"}'),
       (143, 43, 'AWS S3 Deep Dive', 'Application', 'Domain 2: Design Resilient Architectures',
        'A media company requires 99.999999999% (11 9s) data durability for their video assets. They need to understand the difference between S3 durability and availability for their SLA reporting. Which statement is CORRECT about S3 durability vs availability?',
        '[{"text": "A) Durability and availability are identical metrics across all S3 storage classes", "isCorrect": false}, {"text": "B) Durability (11 9s) represents data loss protection, while availability varies by storage class", "isCorrect": true}, {"text": "C) Availability is always higher than durability in S3", "isCorrect": false}, {"text": "D) Only S3 Standard provides 11 9s durability", "isCorrect": false}]',
        'B) Durability (11 9s) represents data loss protection, while availability varies by storage class',
        'S3 provides 11 9s durability (data loss protection) across ALL storage classes, but availability (uptime) varies by class.',
        '{"summary": "S3 durability vs availability:", "breakdown": ["Durability (11 9s): Probability of NOT losing data over a year", "Availability varies: Standard (99.99%), IA (99.9%), Glacier (variable)", "All storage classes: Same durability guarantee", "Durability: Data integrity protection through redundancy"], "otherOptions": "A) Different metrics with different SLA percentages\nC) Availability percentages are typically lower than durability\nD) All S3 storage classes maintain 11 9s durability"}'),
       (144, 44, 'AWS S3 Deep Dive', 'Application', 'Domain 1: Design Secure Architectures',
        'A healthcare organization stores patient records in S3 and needs to maintain full control over encryption keys while ensuring HIPAA compliance. They want to minimize key management overhead but need complete audit trails. Which encryption approach provides the BEST balance?',
        '[{"text": "A) SSE-S3 with AWS-managed keys", "isCorrect": false}, {"text": "B) SSE-KMS with customer-managed keys", "isCorrect": true}, {"text": "C) SSE-C with customer-provided keys", "isCorrect": false}, {"text": "D) Client-side encryption with S3 Encryption Client", "isCorrect": false}]',
        'B) SSE-KMS with customer-managed keys',
        'SSE-KMS with customer-managed keys provides control over keys, audit trails via CloudTrail, and AWS handles key management infrastructure.',
        '{"summary": "SSE-KMS benefits for HIPAA compliance:", "breakdown": ["Customer control: Create, rotate, and manage encryption keys", "Audit trails: CloudTrail logs all key usage and access", "AWS management: Infrastructure and availability handled by AWS", "HIPAA compliant: Meets regulatory requirements for key control"], "otherOptions": "A) AWS-managed keys don''t provide sufficient customer control\nC) SSE-C requires managing key storage and availability\nD) Client-side encryption adds complexity without AWS integration benefits"}'),
       (145, 45, 'AWS S3 Deep Dive', 'Expert', 'Domain 4: Design Cost-Optimized Architectures',
        'A content delivery platform stores 200TB of video files with unpredictable access patterns. Analytics show some videos remain popular for months while others are rarely accessed after the first week. They need cost optimization without performance penalties for popular content. Which S3 storage strategy provides optimal cost savings?',
        '[{"text": "A) Lifecycle policy transitioning all content to S3 IA after 30 days", "isCorrect": false}, {"text": "B) S3 Intelligent-Tiering with Deep Archive Access tier enabled", "isCorrect": true}, {"text": "C) Keep popular content in Standard, move others to Glacier based on view counts", "isCorrect": false}, {"text": "D) Use S3 One Zone-IA for all content to reduce costs", "isCorrect": false}]',
        'B) S3 Intelligent-Tiering with Deep Archive Access tier enabled',
        'S3 Intelligent-Tiering automatically optimizes costs based on access patterns without retrieval fees for frequent/infrequent tiers, perfect for unpredictable patterns.',
        '{"summary": "Intelligent-Tiering benefits for unpredictable access:", "breakdown": ["Automatic optimization: No manual lifecycle policies needed", "No retrieval fees: For frequent and infrequent access tiers", "Deep Archive tier: Additional savings for rarely accessed content", "Performance maintained: Popular content stays in frequent tier"], "otherOptions": "A) Fixed lifecycle doesn''t handle unpredictable patterns\nC) Manual management based on view counts is complex and error-prone\nD) One Zone-IA lacks resilience for valuable content"}'),
       (146, 46, 'AWS EBS Deep Dive', 'Application', 'Domain 3: Design High-Performing Architectures',
        'A database application requires consistent 20,000 IOPS with low latency for transaction processing. The database size is 8TB and growing. Which EBS volume type and configuration provides the BEST performance while supporting future growth?',
        '[{"text": "A) gp3 volumes with 20,000 provisioned IOPS", "isCorrect": false}, {"text": "B) io2 Block Express volumes with 20,000 provisioned IOPS", "isCorrect": true}, {"text": "C) gp2 volumes in RAID 0 configuration", "isCorrect": false}, {"text": "D) st1 throughput optimized volumes for cost savings", "isCorrect": false}]',
        'B) io2 Block Express volumes with 20,000 provisioned IOPS',
        'io2 Block Express provides consistent IOPS up to 256,000, supports volumes up to 64TB, and offers sub-millisecond latency for databases.',
        '{"summary": "io2 Block Express advantages for databases:", "breakdown": ["Consistent IOPS: 20,000 IOPS guaranteed regardless of volume size", "Sub-millisecond latency: Optimal for transaction processing", "Scalability: Supports up to 64TB volumes for future growth", "Durability: 99.999% annual durability vs 99.9% for gp3"], "otherOptions": "A) gp3 IOPS can vary with volume size and burst credits\nC) RAID 0 increases failure risk and management complexity\nD) st1 optimized for throughput, not IOPS-intensive workloads"}'),
       (147, 47, 'AWS EBS Deep Dive', 'Application', 'Domain 2: Design Resilient Architectures',
        'A production database uses EBS volumes and requires point-in-time recovery capability with minimal data loss. The current snapshot strategy creates daily snapshots, but the RPO requirement is 4 hours. Which enhancement provides the BEST backup strategy improvement?',
        '[{"text": "A) Increase snapshot frequency to every 4 hours using scheduled snapshots", "isCorrect": true}, {"text": "B) Enable EBS Fast Snapshot Restore on existing daily snapshots", "isCorrect": false}, {"text": "C) Configure EBS Multi-Attach to create live replicas", "isCorrect": false}, {"text": "D) Use AWS Backup service with continuous backup enabled", "isCorrect": false}]',
        'A) Increase snapshot frequency to every 4 hours using scheduled snapshots',
        'Scheduling snapshots every 4 hours directly meets the RPO requirement by ensuring maximum data loss is limited to 4 hours.',
        '{"summary": "Snapshot strategy for 4-hour RPO:", "breakdown": ["Scheduled snapshots: Every 4 hours meets exact RPO requirement", "Incremental backups: Only changed blocks stored, cost-effective", "Point-in-time recovery: Can restore to any snapshot timestamp", "Automated lifecycle: Can retain snapshots based on retention policy"], "otherOptions": "B) Fast Snapshot Restore improves RTO, not RPO\nC) Multi-Attach doesn''t provide backup functionality\nD) Continuous backup overkill for 4-hour RPO requirement"}'),
       (148, 48, 'AWS EBS Deep Dive', 'Expert', 'Domain 1: Design Secure Architectures',
        'A financial application stores sensitive data on EBS volumes and requires encryption in transit and at rest. The application accesses data across multiple AZs and needs to ensure encryption keys remain under customer control with automatic rotation. Which configuration provides comprehensive encryption coverage?',
        '[{"text": "A) EBS encryption with AWS managed keys and HTTPS application protocols", "isCorrect": false}, {"text": "B) EBS encryption with customer-managed KMS keys, automatic rotation enabled, and application-level TLS", "isCorrect": true}, {"text": "C) Instance store volumes with client-side encryption", "isCorrect": false}, {"text": "D) EBS encryption with customer-provided keys (SSE-C)", "isCorrect": false}]',
        'B) EBS encryption with customer-managed KMS keys, automatic rotation enabled, and application-level TLS',
        'Customer-managed KMS keys provide control with automatic rotation, EBS encryption handles data at rest, and TLS provides encryption in transit.',
        '{"summary": "Comprehensive EBS encryption strategy:", "breakdown": ["Customer-managed KMS keys: Full control over encryption keys", "Automatic rotation: Annual key rotation without service interruption", "EBS encryption: All data at rest encrypted including snapshots", "Application TLS: Encrypts data in transit between instances"], "otherOptions": "A) AWS managed keys don''t provide customer control\nC) Instance store data is temporary, not suitable for persistent financial data\nD) Customer-provided keys require manual key management"}'),
       (149, 49, 'AWS File Systems Deep Dive', 'Application', 'Domain 3: Design High-Performing Architectures',
        'A video editing company needs shared storage accessible from 50 EC2 instances across multiple AZs for collaborative editing projects. Files range from 1GB to 50GB, and editors require low-latency access. Performance needs vary from 1 GB/s during off-hours to 10 GB/s during production. Which file system solution provides the BEST performance and scalability?',
        '[{"text": "A) Amazon EFS with General Purpose performance mode", "isCorrect": false}, {"text": "B) Amazon EFS with Max I/O performance mode and Provisioned Throughput", "isCorrect": true}, {"text": "C) Amazon FSx for Lustre with scratch file system", "isCorrect": false}, {"text": "D) Multiple EBS volumes shared using NFS on EC2", "isCorrect": false}]',
        'B) Amazon EFS with Max I/O performance mode and Provisioned Throughput',
        'EFS Max I/O mode supports higher IOPS for concurrent access, while Provisioned Throughput ensures consistent 10 GB/s performance regardless of file system size.',
        '{"summary": "EFS configuration for video editing workloads:", "breakdown": ["Max I/O mode: Higher IOPS limit for 50 concurrent clients", "Provisioned Throughput: Guarantees 10 GB/s during peak production", "Multi-AZ access: Native support across availability zones", "POSIX compliance: Standard file system semantics for editing software"], "otherOptions": "A) General Purpose mode has lower IOPS limits for concurrent access\nC) Lustre scratch file system designed for HPC, not collaborative editing\nD) EBS volumes can''t be natively shared between instances"}'),
       (150, 50, 'AWS File Systems Deep Dive', 'Expert', 'Domain 2: Design High-Performing Architectures',
        'A Windows-based application requires high-performance file storage with Active Directory integration, SMB protocol support, and sub-millisecond latencies. The workload involves intensive random I/O operations on small files. Current on-premises Windows File Server achieves 100,000 IOPS. Which AWS solution provides equivalent performance?',
        '[{"text": "A) Amazon FSx for Windows File Server with SSD storage", "isCorrect": true}, {"text": "B) Amazon EFS with Windows file gateway", "isCorrect": false}, {"text": "C) EC2 Windows instance with attached io2 EBS volumes", "isCorrect": false}, {"text": "D) Amazon FSx for Lustre with Windows connectivity", "isCorrect": false}]',
        'A) Amazon FSx for Windows File Server with SSD storage',
        'FSx for Windows File Server provides native SMB, Active Directory integration, SSD storage for high IOPS, and sub-millisecond latencies specifically optimized for Windows workloads.',
        '{"summary": "FSx for Windows File Server benefits:", "breakdown": ["Native SMB: Full Windows file system compatibility", "Active Directory: Seamless user authentication and authorization", "SSD storage: Up to 100,000+ IOPS for random I/O workloads", "Sub-millisecond latency: Optimized for Windows application performance"], "otherOptions": "B) EFS doesn''t natively support SMB or Active Directory\nC) Self-managed solution lacks native SMB optimization\nD) FSx for Lustre designed for Linux HPC workloads, not Windows"}'),
       (151, 51, 'AWS Hybrid Storage Migration', 'Application', 'Domain 1: Design Resilient Architectures',
        'A company needs to migrate 500TB of archival data from on-premises tape storage to AWS. The data is rarely accessed but must be retrievable within 12 hours when needed. The migration must complete within 3 months with minimal impact on existing network bandwidth. Which migration strategy is MOST efficient?',
        '[{"text": "A) AWS DataSync over Direct Connect for gradual migration", "isCorrect": false}, {"text": "B) Multiple AWS Snowball Edge devices shipped in batches to S3 Glacier Deep Archive", "isCorrect": true}, {"text": "C) Storage Gateway Tape Gateway for gradual cloud migration", "isCorrect": false}, {"text": "D) AWS Transfer Family for secure file transfer", "isCorrect": false}]',
        'B) Multiple AWS Snowball Edge devices shipped in batches to S3 Glacier Deep Archive',
        'Snowball Edge devices handle large data volumes efficiently without network impact, while Glacier Deep Archive provides cost-effective storage with 12-hour retrieval.',
        '{"summary": "Large-scale archival migration strategy:", "breakdown": ["Snowball Edge capacity: 80TB per device, 7 devices for 500TB", "No network impact: Physical data transfer bypasses internet", "Glacier Deep Archive: Lowest cost storage for rarely accessed data", "12-hour retrieval: Meets business requirement for archive access"], "otherOptions": "A) DataSync over network would take months and impact bandwidth\nC) Tape Gateway for ongoing hybrid, not one-time migration\nD) Transfer Family not optimized for 500TB bulk migration"}'),
       (152, 52, 'AWS Hybrid Storage Migration', 'Expert', 'Domain 3: Design High-Performing Architectures',
        'A media company operates a hybrid architecture where video editors work on-premises but need seamless access to cloud storage for rendering jobs. Local performance must remain unaffected while providing transparent cloud storage scalability. Frequently accessed files should be available locally. Which hybrid storage solution provides the BEST user experience?',
        '[{"text": "A) AWS Storage Gateway File Gateway with local cache", "isCorrect": true}, {"text": "B) AWS DataSync scheduled sync between on-premises and S3", "isCorrect": false}, {"text": "C) Direct Connect with EFS mounted on workstations", "isCorrect": false}, {"text": "D) S3 bucket mounted using third-party NFS gateway", "isCorrect": false}]',
        'A) AWS Storage Gateway File Gateway with local cache',
        'Storage Gateway File Gateway provides NFS interface with intelligent local caching, transparent S3 integration, and optimal performance for frequently accessed files.',
        '{"summary": "File Gateway hybrid benefits:", "breakdown": ["Local cache: Frequently accessed files available at local speeds", "Transparent scaling: Files automatically stored in S3", "NFS interface: Standard file system access for editing applications", "Intelligent caching: Recently and frequently used files cached locally"], "otherOptions": "B) DataSync requires scheduled sync, not transparent access\nC) EFS over Direct Connect has latency for large video files\nD) Third-party solutions lack AWS service integration and support"}'),
       (153, 53, 'AWS Data Security Concepts', 'Application', 'Domain 1: Design Secure Architectures',
        'A financial services company needs to automatically discover and protect sensitive data across 200 S3 buckets containing customer documents. They require automated classification, policy enforcement, and immediate alerts for data exposure risks. Which combination provides comprehensive data security automation?',
        '[{"text": "A) AWS Config rules for bucket policies and CloudWatch for monitoring", "isCorrect": false}, {"text": "B) Amazon Macie for discovery and classification, EventBridge for alerts, and S3 Block Public Access", "isCorrect": true}, {"text": "C) AWS Inspector for vulnerability scanning and GuardDuty for threats", "isCorrect": false}, {"text": "D) Custom Lambda functions with CloudTrail for access monitoring", "isCorrect": false}]',
        'B) Amazon Macie for discovery and classification, EventBridge for alerts, and S3 Block Public Access',
        'Macie uses ML to discover sensitive data, provides automatic classification, EventBridge enables real-time alerts, and Block Public Access prevents exposure.',
        '{"summary": "Automated data security components:", "breakdown": ["Amazon Macie: ML-powered sensitive data discovery and classification", "EventBridge integration: Real-time alerts for policy violations", "S3 Block Public Access: Prevents accidental public exposure", "Automated findings: Continuous monitoring across all 200 buckets"], "otherOptions": "A) Config and CloudWatch don''t provide sensitive data discovery\nC) Inspector and GuardDuty focus on infrastructure, not data classification\nD) Custom solutions require significant development and maintenance"}'),
       (154, 54, 'AWS Data Security Concepts', 'Expert', 'Domain 1: Design Secure Architectures',
        'A healthcare organization must implement data classification and protection for patient records across multiple AWS services (S3, RDS, DynamoDB). They need automated discovery of PHI, policy-based access controls, and audit trails meeting HIPAA requirements. Which architecture provides comprehensive data protection?',
        '[{"text": "A) Amazon Macie for S3, database activity streams for RDS, and VPC Flow Logs", "isCorrect": false}, {"text": "B) Amazon Macie for S3, AWS CloudTrail data events, and customer-managed KMS keys across all services", "isCorrect": true}, {"text": "C) AWS Config for compliance monitoring and GuardDuty for threat detection", "isCorrect": false}, {"text": "D) Custom data loss prevention (DLP) solution with third-party tools", "isCorrect": false}]',
        'B) Amazon Macie for S3, AWS CloudTrail data events, and customer-managed KMS keys across all services',
        'Macie discovers PHI in S3, CloudTrail data events provide comprehensive audit trails, and customer-managed KMS keys ensure encryption control across all services.',
        '{"summary": "HIPAA-compliant data protection architecture:", "breakdown": ["Macie: Automated PHI discovery and classification in S3", "CloudTrail data events: Complete audit trail of data access", "Customer-managed KMS: Encryption control across S3, RDS, DynamoDB", "Policy-based access: IAM policies with resource-based conditions"], "otherOptions": "A) Missing encryption control and comprehensive audit coverage\nC) Config and GuardDuty don''t provide PHI discovery and classification\nD) Custom solutions require extensive compliance validation"}'),
       (155, 55, 'AWS Encryption', 'Expert', 'Domain 1: Design Secure Architectures',
        'A multinational corporation needs end-to-end encryption for data stored across AWS services in multiple regions. They require customer control over encryption keys, automatic rotation, cross-region access, and integration with their existing HSM infrastructure. Which encryption strategy provides the MOST comprehensive solution?',
        '[{"text": "A) AWS KMS customer-managed keys with automatic rotation in each region", "isCorrect": false}, {"text": "B) AWS CloudHSM cluster with custom key management application", "isCorrect": true}, {"text": "C) Client-side encryption with application-managed keys", "isCorrect": false}, {"text": "D) AWS KMS with imported key material from on-premises HSM", "isCorrect": false}]',
        'B) AWS CloudHSM cluster with custom key management application',
        'CloudHSM provides dedicated hardware security modules with full customer control, integrates with existing HSM infrastructure, and supports cross-region key management.',
        '{"summary": "CloudHSM comprehensive encryption benefits:", "breakdown": ["Dedicated HSM: Full customer control over encryption operations", "FIPS 140-2 Level 3: Highest security certification available", "Cross-region clustering: Keys available across multiple regions", "HSM integration: Compatible with existing on-premises HSM infrastructure"], "otherOptions": "A) KMS keys are region-specific and don''t integrate with existing HSM\nC) Application-managed keys lack HSM security and are difficult to manage at scale\nD) Imported key material in KMS doesn''t provide full HSM integration"}'),
       (156, 56, 'AWS Encryption', 'Application', 'Domain 1: Design Secure Architectures',
        'A development team needs to encrypt application data before storing it in DynamoDB. They want to minimize performance impact while ensuring field-level encryption for sensitive columns (SSN, credit card numbers) but allow searching on encrypted email addresses. Which encryption approach provides the BEST balance of security and functionality?',
        '[{"text": "A) DynamoDB encryption at rest with AWS managed keys", "isCorrect": false}, {"text": "B) AWS DynamoDB Encryption Client with deterministic encryption for emails, probabilistic for sensitive fields", "isCorrect": true}, {"text": "C) Application-level AES encryption for all fields", "isCorrect": false}, {"text": "D) AWS KMS encryption for entire DynamoDB table", "isCorrect": false}]',
        'B) AWS DynamoDB Encryption Client with deterministic encryption for emails, probabilistic for sensitive fields',
        'DynamoDB Encryption Client provides field-level encryption with deterministic encryption for searchable fields and probabilistic encryption for maximum security on sensitive data.',
        '{"summary": "Field-level encryption strategy:", "breakdown": ["Deterministic encryption: Same email always produces same ciphertext (searchable)", "Probabilistic encryption: Different ciphertext each time (maximum security)", "Client-side encryption: Data encrypted before sending to DynamoDB", "Performance optimized: Only sensitive fields encrypted, not entire records"], "otherOptions": "A) Table-level encryption doesn''t provide field-level control\nC) Application encryption without library optimization impacts performance\nD) KMS table encryption encrypts all data, doesn''t enable selective searching"}'),
       (157, 57, 'AWS Governance Compliance', 'Expert', 'Domain 1: Design Secure Architectures',
        'A regulated financial institution must demonstrate continuous compliance with SOC 2 requirements for data storage. They need automated policy enforcement, compliance reporting, and evidence collection across 50 AWS accounts. Non-compliant resources must be automatically remediated. Which governance framework provides comprehensive compliance automation?',
        '[{"text": "A) AWS Config with custom rules and SNS notifications", "isCorrect": false}, {"text": "B) AWS Organizations with SCPs, Config Conformance Packs, and Systems Manager Automation", "isCorrect": true}, {"text": "C) AWS CloudFormation with compliance templates", "isCorrect": false}, {"text": "D) AWS Security Hub with manual remediation workflows", "isCorrect": false}]',
        'B) AWS Organizations with SCPs, Config Conformance Packs, and Systems Manager Automation',
        'Organizations provides account governance, Conformance Packs enable SOC 2 compliance checks, and Systems Manager Automation handles remediation across all accounts.',
        '{"summary": "Comprehensive compliance automation:", "breakdown": ["Organizations + SCPs: Account-level policy enforcement", "Config Conformance Packs: Pre-built SOC 2 compliance rules", "Systems Manager Automation: Automatic remediation of non-compliant resources", "Centralized reporting: Compliance status across all 50 accounts"], "otherOptions": "A) Config alone lacks account-level governance and automated remediation\nC) CloudFormation provides deployment compliance, not ongoing monitoring\nD) Security Hub requires manual remediation, not automated"}'),
       (158, 58, 'AWS Governance Compliance', 'Application', 'Domain 1: Design Secure Architectures',
        'A global company must ensure S3 buckets across all regions comply with data residency requirements. EU data must stay in EU regions, US data in US regions, with automatic policy enforcement and compliance reporting. Which solution provides the MOST effective governance?',
        '[{"text": "A) IAM policies restricting S3 actions based on user location", "isCorrect": false}, {"text": "B) S3 bucket policies with aws:RequestedRegion condition keys", "isCorrect": false}, {"text": "C) AWS Organizations SCPs with region restrictions based on data classification tags", "isCorrect": true}, {"text": "D) AWS Config rules monitoring bucket creation across regions", "isCorrect": false}]',
        'C) AWS Organizations SCPs with region restrictions based on data classification tags',
        'Organizations SCPs provide account-level enforcement that cannot be overridden, with tag-based conditions enabling automatic data residency compliance.',
        '{"summary": "Data residency governance with SCPs:", "breakdown": ["SCPs: Cannot be overridden by account-level permissions", "Tag-based conditions: Automatically enforce based on data classification", "Region restrictions: Prevent EU data creation in non-EU regions", "Organization-wide: Applies to all current and future accounts"], "otherOptions": "A) IAM policies can be overridden by account administrators\nB) Bucket policies only apply after bucket creation\nD) Config rules detect violations but don''t prevent them"}'),
       (159, 59, 'AWS Storage Resiliency Scalability', 'Expert', 'Domain 2: Design Resilient Architectures',
        'A video streaming platform stores 1PB of content with global distribution requirements. They need 99.99% availability, automatic failover, and the ability to handle 10x traffic spikes during major events. Current single-region S3 architecture shows latency issues for international users. Which architecture provides optimal resiliency and global performance?',
        '[{"text": "A) S3 Cross-Region Replication with CloudFront distribution", "isCorrect": true}, {"text": "B) Multi-region S3 buckets with Route 53 latency-based routing", "isCorrect": false}, {"text": "C) S3 Transfer Acceleration with single-region storage", "isCorrect": false}, {"text": "D) EFS with VPC peering across regions", "isCorrect": false}]',
        'A) S3 Cross-Region Replication with CloudFront distribution',
        'S3 CRR provides data resilience and regional availability, while CloudFront edge locations ensure global low-latency access and can handle traffic spikes automatically.',
        '{"summary": "Global content delivery architecture:", "breakdown": ["S3 CRR: Automatic replication across regions for resilience", "CloudFront: 400+ edge locations for global low-latency delivery", "Auto-scaling: CloudFront handles 10x traffic spikes automatically", "99.99% availability: S3 + CloudFront combined SLA exceeds requirement"], "otherOptions": "B) Route 53 doesn''t provide edge caching for content delivery\nC) Transfer Acceleration only helps uploads, not global content delivery\nD) EFS not designed for 1PB content delivery workloads"}'),
       (160, 60, 'AWS Storage Resiliency Scalability', 'Application', 'Domain 2: Design Resilient Architectures',
        'An e-commerce platform experiences seasonal traffic patterns with 50x increase during holiday sales. Their product catalog and user-generated content storage must scale automatically without performance degradation. Current EBS-based storage architecture requires manual intervention during peaks. Which storage architecture provides automatic scalability?',
        '[{"text": "A) Auto Scaling EBS volumes with CloudWatch metrics", "isCorrect": false}, {"text": "B) Amazon S3 with CloudFront for content delivery and Lambda for processing", "isCorrect": true}, {"text": "C) Amazon EFS with Provisioned Throughput mode", "isCorrect": false}, {"text": "D) Multiple EBS volumes in RAID configuration", "isCorrect": false}]',
        'B) Amazon S3 with CloudFront for content delivery and Lambda for processing',
        'S3 provides unlimited automatic scaling, CloudFront handles traffic spikes through edge caching, and Lambda scales processing automatically with demand.',
        '{"summary": "Auto-scaling storage architecture:", "breakdown": ["S3 unlimited scaling: Handles any amount of data and requests", "CloudFront edge caching: Reduces origin load during traffic spikes", "Lambda auto-scaling: Processing scales from 0 to thousands of concurrent executions", "No manual intervention: All components scale automatically"], "otherOptions": "A) EBS volumes cannot auto-scale capacity\nC) EFS requires pre-provisioned throughput planning\nD) RAID configuration still requires manual capacity planning"}'),
       (161, 61, 'AWS Data Lifecycle', 'Application', 'Domain 4: Design Cost-Optimized Architectures',
        'A research institution stores genomics data with the following access pattern: analyzed intensively for 30 days, occasionally referenced for 6 months, and archived for 10 years for compliance. Current costs are $50,000/month in S3 Standard. Which lifecycle policy provides maximum cost optimization while meeting access requirements?',
        '[{"text": "A) Standard for 30 days → IA at 30 days → Glacier at 180 days → Deep Archive at 1 year", "isCorrect": true}, {"text": "B) Intelligent-Tiering for all data with Deep Archive enabled", "isCorrect": false}, {"text": "C) Standard for 30 days → Glacier immediately at 30 days", "isCorrect": false}, {"text": "D) One Zone-IA for all data to reduce costs", "isCorrect": false}]',
        'A) Standard for 30 days → IA at 30 days → Glacier at 180 days → Deep Archive at 1 year',
        'Lifecycle transitions match access patterns: Standard for intensive analysis, IA for occasional access, Glacier for long-term storage, Deep Archive for compliance.',
        '{"summary": "Optimized lifecycle transitions for research data:", "breakdown": ["Days 1-30: S3 Standard for intensive analysis", "Days 31-180: S3 IA for occasional reference (68% cost reduction)", "Days 181-365: S3 Glacier for backup storage (77% cost reduction)", "Years 1-10: Deep Archive for compliance (80% cost reduction)"], "otherOptions": "B) Intelligent-Tiering adds overhead for predictable access patterns\nC) Early Glacier transition makes occasional access expensive\nD) One Zone-IA lacks resilience for valuable research data"}'),
       (162, 62, 'AWS Data Lifecycle', 'Expert', 'Domain 4: Design Cost-Optimized Architectures',
        'A media company has complex data lifecycle needs: news content (hot for 7 days, cold afterwards), evergreen content (unpredictable access), and archived footage (rare access but immediate retrieval when needed). They want automated optimization without management overhead. Which strategy handles all three patterns optimally?',
        '[{"text": "A) Separate buckets with different lifecycle policies for each content type", "isCorrect": false}, {"text": "B) S3 Intelligent-Tiering with object tagging and tag-based lifecycle policies", "isCorrect": true}, {"text": "C) Manual lifecycle management based on content metadata", "isCorrect": false}, {"text": "D) Single lifecycle policy with shortest transition times", "isCorrect": false}]',
        'B) S3 Intelligent-Tiering with object tagging and tag-based lifecycle policies',
        'Intelligent-Tiering with object tags enables different optimization strategies per content type while maintaining automation and handling unpredictable access patterns.',
        '{"summary": "Tag-based intelligent lifecycle management:", "breakdown": ["Object tagging: Classify content types (news, evergreen, archive)", "Intelligent-Tiering: Automatic optimization for unpredictable evergreen content", "Tag-based policies: Specific rules for news (7-day pattern) and archive content", "No management overhead: All transitions automated based on access patterns"], "otherOptions": "A) Multiple buckets increase management complexity\nC) Manual management doesn''t scale and introduces errors\nD) Single policy can''t optimize for different access patterns"}'),
       (163, 63, 'AWS Storage Cost Management', 'Expert', 'Domain 4: Design Cost-Optimized Architectures',
        'A data analytics company spends $100,000/month on storage across S3, EBS, and EFS for various workloads. They need comprehensive cost optimization while maintaining performance. Analysis shows: 40% rarely accessed, 30% predictable patterns, 20% unpredictable access, 10% high-performance needs. Which multi-service optimization strategy provides maximum savings?',
        '[{"text": "A) Move all data to cheapest storage classes regardless of access patterns", "isCorrect": false}, {"text": "B) S3 lifecycle policies for rarely accessed, Intelligent-Tiering for unpredictable, gp3 optimization for EBS, EFS IA for infrequent file access", "isCorrect": true}, {"text": "C) Consolidate all storage to S3 with single lifecycle policy", "isCorrect": false}, {"text": "D) Implement Reserved Capacity for all storage services", "isCorrect": false}]',
        'B) S3 lifecycle policies for rarely accessed, Intelligent-Tiering for unpredictable, gp3 optimization for EBS, EFS IA for infrequent file access',
        'Tailored optimization per access pattern: lifecycle for predictable patterns, Intelligent-Tiering for unpredictable, gp3 for cost-effective EBS performance, EFS IA for infrequent access.',
        '{"summary": "Multi-service cost optimization strategy:", "breakdown": ["40% rarely accessed: S3 lifecycle to Glacier/Deep Archive (75% savings)", "20% unpredictable: S3 Intelligent-Tiering (40% average savings)", "EBS optimization: gp3 volumes with right-sized IOPS (20% savings)", "EFS optimization: IA storage class for infrequent access (85% savings)"], "otherOptions": "A) Ignoring access patterns can cause performance issues and retrieval costs\nC) Not all workloads suit object storage architecture\nD) Reserved Capacity not available for all storage types and may not match usage patterns"}'),
       (164, 64, 'AWS Storage Cost Management', 'Application', 'Domain 4: Design Cost-Optimized Architectures',
        'A startup''s S3 storage costs have grown to $25,000/month with 80% of objects never accessed after 90 days. They also have high data transfer costs from direct client downloads. The application serves user-uploaded images and documents globally. Which combination provides the most comprehensive cost reduction?',
        '[{"text": "A) S3 lifecycle policy to Glacier and CloudFront distribution", "isCorrect": true}, {"text": "B) S3 Intelligent-Tiering and S3 Transfer Acceleration", "isCorrect": false}, {"text": "C) Move all data to S3 One Zone-IA with direct client access", "isCorrect": false}, {"text": "D) Compress all objects and implement client-side caching", "isCorrect": false}]',
        'A) S3 lifecycle policy to Glacier and CloudFront distribution',
        'Lifecycle policy to Glacier reduces storage costs by 68% for unused objects, while CloudFront eliminates data transfer costs and provides global acceleration.',
        '{"summary": "Comprehensive S3 cost optimization:", "breakdown": ["Lifecycle to Glacier: 68% storage cost reduction for 80% of objects", "CloudFront: Eliminates data transfer costs from S3 to internet", "Global caching: Improves performance while reducing origin costs", "Combined savings: Approximately 70% total cost reduction"], "otherOptions": "B) Intelligent-Tiering adds overhead for predictable 90-day pattern\nC) One Zone-IA lacks resilience and doesn''t address transfer costs\nD) Compression helps but doesn''t address core storage lifecycle and transfer costs"}'),
       (165, 65, 'AWS Storage Comprehensive', 'Expert', 'Domain 3: Design High-Performing Architectures',
        'A large enterprise needs a comprehensive storage strategy for their cloud migration. Requirements include: 500TB database storage with 50,000 IOPS, 2PB file shares for 1000 users, 10PB object storage with global access, and hybrid connectivity to on-premises. Which architecture provides optimal performance, cost, and integration?',
        '[{"text": "A) io2 Block Express EBS, EFS Max I/O, S3 with CloudFront, and Storage Gateway", "isCorrect": true}, {"text": "B) gp3 EBS, FSx for Lustre, S3 Intelligent-Tiering, and Direct Connect", "isCorrect": false}, {"text": "C) Aurora storage, EFS General Purpose, S3 Standard, and VPN connections", "isCorrect": false}, {"text": "D) Instance store, FSx for Windows, S3 Glacier, and AWS Outposts", "isCorrect": false}]',
        'A) io2 Block Express EBS, EFS Max I/O, S3 with CloudFront, and Storage Gateway',
        'io2 Block Express provides required IOPS, EFS Max I/O handles concurrent users, S3+CloudFront scales globally, Storage Gateway enables hybrid integration.',
        '{"summary": "Enterprise storage architecture components:", "breakdown": ["io2 Block Express: 50,000 IOPS with sub-millisecond latency for databases", "EFS Max I/O: Scales to 1000+ concurrent users with higher IOPS", "S3 + CloudFront: Unlimited object storage with global low-latency access", "Storage Gateway: Seamless hybrid connectivity with caching"], "otherOptions": "B) gp3 may not provide consistent 50,000 IOPS, FSx Lustre not ideal for general file shares\nC) Aurora storage limits database choice, VPN insufficient for 500TB+ workloads\nD) Instance store temporary, FSx Windows not mentioned as requirement, Glacier too slow"}'),
       (166, 66, 'AWS Storage Comprehensive', 'Expert', 'Domain 1: Design Secure Architectures',
        'A financial services firm requires end-to-end security for their storage infrastructure handling sensitive customer data across S3, EBS, and EFS. Requirements include: customer-controlled encryption, automated compliance monitoring, data classification, and audit trails meeting regulatory standards. Which comprehensive security architecture meets all requirements?',
        '[{"text": "A) AWS managed encryption, GuardDuty monitoring, and CloudTrail logging", "isCorrect": false}, {"text": "B) Customer-managed KMS keys, Amazon Macie, Config Conformance Packs, and CloudTrail data events", "isCorrect": true}, {"text": "C) CloudHSM encryption, Security Hub, and VPC Flow Logs", "isCorrect": false}, {"text": "D) Client-side encryption, Inspector scanning, and AWS Config rules", "isCorrect": false}]',
        'B) Customer-managed KMS keys, Amazon Macie, Config Conformance Packs, and CloudTrail data events',
        'Customer-managed KMS provides encryption control, Macie classifies sensitive data, Conformance Packs ensure compliance, CloudTrail data events provide complete audit trails.',
        '{"summary": "Comprehensive security architecture for financial services:", "breakdown": ["Customer-managed KMS: Full control over encryption keys across all storage services", "Amazon Macie: Automated discovery and classification of sensitive financial data", "Config Conformance Packs: Continuous compliance monitoring for financial regulations", "CloudTrail data events: Complete audit trail of all data access activities"], "otherOptions": "A) AWS managed encryption lacks customer control required for financial services\nC) CloudHSM overkill, Security Hub doesn''t provide data classification\nD) Client-side encryption complex to manage, Inspector focuses on vulnerabilities not data classification"}');


-- Insert all CompTIA Cloud+ questions
INSERT INTO prepper.comptia_cloud_plus_questions (question_id, question_number, category, difficulty, domain,
                                                  question_text, options, correct_answer, explanation,
                                                  explanation_details)
VALUES (1, 1, 'Cloud Architecture - Service Models', 'Knowledge', 'Domain 1',
        'Which cloud service model provides virtualized computing resources over the internet, allowing users to rent servers, storage, and networking without purchasing physical hardware?',
        '[{"text": "A) Software as a Service (SaaS)", "isCorrect": false}, {"text": "B) Platform as a Service (PaaS)", "isCorrect": false}, {"text": "C) Infrastructure as a Service (IaaS)", "isCorrect": true}, {"text": "D) Function as a Service (FaaS)", "isCorrect": false}]',
        'C) Infrastructure as a Service (IaaS)',
        'IaaS provides virtualized computing resources including servers, storage, and networking infrastructure.',
        '{"summary": "IaaS characteristics include:", "breakdown": ["Virtualized computing resources over the internet", "Users rent infrastructure components rather than buying hardware", "Control over operating systems and applications", "Examples: AWS EC2, Azure VMs, Google Compute Engine"], "otherOptions": "A) SaaS delivers complete software applications\nB) PaaS provides development platforms\nD) FaaS provides serverless function execution"}'),
       (2, 2, 'Cloud Architecture - Service Models', 'Comprehension', 'Domain 1',
        'Your company wants to deploy a web application without managing the underlying servers, operating systems, or runtime environments. Which service model best fits this requirement?',
        '[{"text": "A) Infrastructure as a Service (IaaS)", "isCorrect": false}, {"text": "B) Platform as a Service (PaaS)", "isCorrect": true}, {"text": "C) Software as a Service (SaaS)", "isCorrect": false}, {"text": "D) Function as a Service (FaaS)", "isCorrect": false}]',
        'B) Platform as a Service (PaaS)',
        'PaaS abstracts away infrastructure management while providing a platform for application development and deployment.',
        '{"summary": "PaaS characteristics for this scenario:", "breakdown": ["Eliminates server and OS management overhead", "Provides development tools and runtime environments", "Allows focus on application code and business logic", "Examples: Azure App Service, Google App Engine, Heroku"], "otherOptions": "A) IaaS requires managing servers and OS\nC) SaaS provides complete applications\nD) FaaS is for event-driven functions"}'),
       (3, 3, 'Cloud Architecture - Service Models', 'Application', 'Domain 1',
        'A developer needs to execute code in response to events without managing any infrastructure. The code should automatically scale and only run when triggered. Which service model is most appropriate?',
        '[{"text": "A) Infrastructure as a Service (IaaS)", "isCorrect": false}, {"text": "B) Platform as a Service (PaaS)", "isCorrect": false}, {"text": "C) Software as a Service (SaaS)", "isCorrect": false}, {"text": "D) Function as a Service (FaaS)", "isCorrect": true}]',
        'D) Function as a Service (FaaS)',
        'FaaS allows developers to execute code in response to events without managing infrastructure, with automatic scaling.',
        '{"summary": "FaaS characteristics for this scenario:", "breakdown": ["Event-driven execution model", "No server management required", "Automatic scaling based on demand", "Pay-per-execution pricing model"], "otherOptions": "A) IaaS requires infrastructure management\nB) PaaS still requires some platform management\nC) SaaS provides complete applications, not custom code execution"}'),
       (4, 4, 'Cloud Architecture - Service Models', 'Knowledge', 'Domain 1',
        'Which service model is represented by applications like Salesforce, Office 365, and Gmail?',
        '[{"text": "A) Infrastructure as a Service (IaaS)", "isCorrect": false}, {"text": "B) Platform as a Service (PaaS)", "isCorrect": false}, {"text": "C) Software as a Service (SaaS)", "isCorrect": true}, {"text": "D) Function as a Service (FaaS)", "isCorrect": false}]',
        'C) Software as a Service (SaaS)',
        'SaaS delivers complete software applications over the internet that users access through web browsers.',
        '{"summary": "SaaS characteristics:", "breakdown": ["Complete software applications delivered over internet", "No software installation or maintenance required", "Subscription-based pricing model", "Multi-tenant architecture"], "otherOptions": "A) IaaS provides infrastructure components\nB) PaaS provides development platforms\nD) FaaS provides serverless function execution"}'),
       (5, 5, 'Cloud Architecture - Shared Responsibility', 'Comprehension', 'Domain 1',
        'In the shared responsibility model, a customer using Amazon RDS (managed database service) is responsible for which of the following?',
        '[{"text": "A) Database engine patching and updates", "isCorrect": false}, {"text": "B) Physical security of the data center", "isCorrect": false}, {"text": "C) Data encryption and access control configuration", "isCorrect": true}, {"text": "D) Hardware maintenance and replacement", "isCorrect": false}]',
        'C) Data encryption and access control configuration',
        'In managed services like RDS, customers are responsible for data security, access controls, and encryption configuration.',
        '{"summary": "Customer responsibilities in managed database services:", "breakdown": ["Data encryption at rest and in transit", "User access management and IAM policies", "Network security groups and firewall rules", "Backup retention and recovery testing"], "otherOptions": "A) AWS manages engine patching\nB) AWS handles physical security\nD) AWS manages hardware infrastructure"}'),
       (6, 6, 'Cloud Architecture - Shared Responsibility', 'Application', 'Domain 1',
        'Your organization is using IaaS virtual machines. According to the shared responsibility model, which security aspect is the customer responsible for?',
        '[{"text": "A) Physical security of the data center", "isCorrect": false}, {"text": "B) Hypervisor security and maintenance", "isCorrect": false}, {"text": "C) Operating system patches and configuration", "isCorrect": true}, {"text": "D) Network infrastructure hardware security", "isCorrect": false}]',
        'C) Operating system patches and configuration',
        'In IaaS, customers are responsible for securing the guest operating system, including patching and configuration.',
        '{"summary": "Customer responsibilities in IaaS:", "breakdown": ["Guest operating system security and patching", "Application security and configuration", "Network traffic protection (encryption)", "Identity and access management"], "otherOptions": "A) Physical security is provider responsibility\nB) Hypervisor is managed by cloud provider\nD) Network hardware is provider responsibility"}'),
       (7, 7, 'Cloud Architecture - Availability', 'Application', 'Domain 1',
        'Your application requires 99.99% uptime and must survive the failure of an entire data center. Which architecture approach best meets these requirements?',
        '[{"text": "A) Deploy in a single availability zone with multiple instances", "isCorrect": false}, {"text": "B) Deploy across multiple availability zones in the same region", "isCorrect": true}, {"text": "C) Use larger instance types for better reliability", "isCorrect": false}, {"text": "D) Implement only vertical scaling", "isCorrect": false}]',
        'B) Deploy across multiple availability zones in the same region',
        'Multi-AZ deployment provides data center-level fault tolerance while maintaining low latency.',
        '{"summary": "Multi-AZ deployment benefits:", "breakdown": ["Survives entire data center failures", "Maintains low latency within region", "Automatic failover capabilities", "Meets high availability requirements (99.99%)"], "otherOptions": "A) Single AZ cannot survive data center failure\nC) Instance size doesn''t address availability zones\nD) Vertical scaling doesn''t provide fault tolerance"}'),
       (8, 8, 'Cloud Architecture - Availability', 'Knowledge', 'Domain 1',
        'What is the primary purpose of availability zones in cloud computing?',
        '[{"text": "A) To provide different pricing tiers", "isCorrect": false}, {"text": "B) To separate different cloud services", "isCorrect": false}, {"text": "C) To provide redundancy and fault tolerance", "isCorrect": true}, {"text": "D) To comply with data sovereignty requirements", "isCorrect": false}]',
        'C) To provide redundancy and fault tolerance',
        'Availability zones provide redundancy and fault tolerance by isolating failures to specific geographic locations.',
        '{"summary": "Availability zone characteristics:", "breakdown": ["Isolated data center locations within a region", "Independent power, cooling, and networking", "Designed to prevent cascading failures", "Enable high availability architecture design"], "otherOptions": "A) Pricing is not determined by AZ\nB) Services can span multiple AZs\nD) Data sovereignty is handled at region level"}'),
       (9, 9, 'Cloud Architecture - Availability', 'Comprehension', 'Domain 1',
        'Which cloud strategy allows an organization to handle sudden traffic spikes by temporarily using public cloud resources while maintaining their private cloud for normal operations?',
        '[{"text": "A) Multi-cloud deployment", "isCorrect": false}, {"text": "B) Hybrid cloud architecture", "isCorrect": false}, {"text": "C) Cloud bursting", "isCorrect": true}, {"text": "D) Edge computing", "isCorrect": false}]',
        'C) Cloud bursting',
        'Cloud bursting allows organizations to scale from private to public cloud during peak demand periods.',
        '{"summary": "Cloud bursting characteristics:", "breakdown": ["Temporary use of public cloud resources", "Handles unexpected traffic spikes", "Cost-effective scaling approach", "Maintains private cloud for normal operations"], "otherOptions": "A) Multi-cloud uses multiple providers simultaneously\nB) Hybrid cloud is permanent architecture\nD) Edge computing brings processing closer to users"}'),
       (10, 10, 'Cloud Architecture - Storage', 'Knowledge', 'Domain 1',
        'Which storage type is best suited for frequently accessed data requiring low latency and high throughput?',
        '[{"text": "A) Cold storage tier", "isCorrect": false}, {"text": "B) Archive storage tier", "isCorrect": false}, {"text": "C) Hot storage tier", "isCorrect": true}, {"text": "D) Warm storage tier", "isCorrect": false}]',
        'C) Hot storage tier',
        'Hot storage tier is optimized for frequently accessed data with low latency requirements.',
        '{"summary": "Hot storage characteristics:", "breakdown": ["Optimized for frequent access patterns", "Provides low latency data retrieval", "Higher cost but better performance", "Ideal for production application data"], "otherOptions": "A) Cold storage for infrequent access\nB) Archive for long-term retention\nD) Warm storage for moderate access"}'),
       (11, 11, 'Cloud Architecture - Storage', 'Comprehension', 'Domain 1',
        'An application requires high IOPS for database operations and low-level access to storage blocks. Which storage combination is most appropriate?',
        '[{"text": "A) Object storage with HDD", "isCorrect": false}, {"text": "B) File storage with SSD", "isCorrect": false}, {"text": "C) Block storage with SSD", "isCorrect": true}, {"text": "D) Object storage with SSD", "isCorrect": false}]',
        'C) Block storage with SSD',
        'Block storage provides low-level access ideal for databases, while SSDs deliver the high IOPS required.',
        '{"summary": "Block storage with SSD advantages:", "breakdown": ["Block-level access optimal for database workloads", "SSD provides high IOPS and low latency", "Direct attachment to compute instances", "Suitable for transactional applications"], "otherOptions": "A) Object storage lacks low-level access; HDD has lower IOPS\nB) File storage not optimal for databases\nD) Object storage doesn''t provide block-level access"}'),
       (12, 12, 'Cloud Architecture - Storage', 'Application', 'Domain 1',
        'Your organization needs to store 100TB of archived data that is accessed once per year for compliance purposes. Which storage solution offers the best cost optimization?',
        '[{"text": "A) Hot storage tier", "isCorrect": false}, {"text": "B) Warm storage tier", "isCorrect": false}, {"text": "C) Cold storage tier", "isCorrect": false}, {"text": "D) Archive storage tier", "isCorrect": true}]',
        'D) Archive storage tier',
        'Archive storage tier is designed for long-term retention of rarely accessed data with lowest cost.',
        '{"summary": "Archive storage characteristics:", "breakdown": ["Lowest cost storage option", "Designed for long-term retention", "Higher retrieval times and costs", "Ideal for compliance and backup data"], "otherOptions": "A) Hot storage too expensive for rarely accessed data\nB) Warm storage still higher cost than needed\nC) Cold storage more expensive than archive"}'),
       (13, 13, 'Cloud Architecture - Network Components', 'Knowledge', 'Domain 1',
        'Which network component helps reduce latency for global users by caching content at edge locations closest to them?',
        '[{"text": "A) Application load balancer", "isCorrect": false}, {"text": "B) Network load balancer", "isCorrect": false}, {"text": "C) Content Delivery Network (CDN)", "isCorrect": true}, {"text": "D) Application gateway", "isCorrect": false}]',
        'C) Content Delivery Network (CDN)',
        'CDNs distribute content across geographically dispersed servers to minimize latency for end users.',
        '{"summary": "CDN functionality:", "breakdown": ["Caches static content at edge locations globally", "Routes requests to nearest geographic server", "Reduces bandwidth usage and server load", "Improves website performance and user experience"], "otherOptions": "A) ALB distributes traffic to backend servers\nB) NLB handles network layer traffic\nD) Application gateway provides secure access"}'),
       (14, 14, 'Cloud Architecture - Network Components', 'Comprehension', 'Domain 1',
        'What is the primary difference between an application load balancer and a network load balancer?',
        '[{"text": "A) Application load balancer works at Layer 7, network load balancer at Layer 4", "isCorrect": true}, {"text": "B) Network load balancer is more expensive than application load balancer", "isCorrect": false}, {"text": "C) Application load balancer only works with HTTP, network load balancer with HTTPS", "isCorrect": false}, {"text": "D) Network load balancer provides SSL termination, application load balancer does not", "isCorrect": false}]',
        'A) Application load balancer works at Layer 7, network load balancer at Layer 4',
        'Application load balancers operate at Layer 7 (application layer) while network load balancers operate at Layer 4 (transport layer).',
        '{"summary": "Load balancer layer differences:", "breakdown": ["ALB: Layer 7 - can route based on HTTP headers, URLs, cookies", "NLB: Layer 4 - routes based on IP protocol data", "ALB: Content-based routing capabilities", "NLB: Higher performance, lower latency"], "otherOptions": "B) Pricing varies by provider and usage\nC) Both can handle HTTP and HTTPS\nD) Both can provide SSL termination"}'),
       (15, 15, 'Cloud Architecture - Network Components', 'Application', 'Domain 1',
        'Your web application needs to route traffic based on URL paths (/api/* to API servers, /images/* to image servers). Which load balancer type should you use?',
        '[{"text": "A) Network load balancer", "isCorrect": false}, {"text": "B) Application load balancer", "isCorrect": true}, {"text": "C) Classic load balancer", "isCorrect": false}, {"text": "D) Gateway load balancer", "isCorrect": false}]',
        'B) Application load balancer',
        'Application load balancers can route traffic based on URL paths, headers, and other application-layer information.',
        '{"summary": "Application load balancer routing capabilities:", "breakdown": ["Path-based routing (/api, /images, etc.)", "Header-based routing", "Query parameter-based routing", "Cookie-based routing"], "otherOptions": "A) Network load balancer routes at Layer 4, cannot inspect URLs\nC) Classic load balancer has limited routing capabilities\nD) Gateway load balancer is for traffic inspection"}'),
       (16, 16, 'Cloud Architecture - Disaster Recovery', 'Knowledge', 'Domain 1',
        'What does RTO (Recovery Time Objective) represent in disaster recovery planning?',
        '[{"text": "A) The amount of data that can be lost during a disaster", "isCorrect": false}, {"text": "B) The maximum acceptable downtime after a disaster", "isCorrect": true}, {"text": "C) The cost of implementing disaster recovery", "isCorrect": false}, {"text": "D) The frequency of disaster recovery testing", "isCorrect": false}]',
        'B) The maximum acceptable downtime after a disaster',
        'RTO defines the maximum acceptable duration within which a system must be restored after a disruption.',
        '{"summary": "RTO characteristics:", "breakdown": ["Maximum acceptable downtime duration", "Measured in hours, minutes, or seconds", "Drives disaster recovery strategy selection", "Affects cost and complexity of DR solutions"], "otherOptions": "A) That describes RPO (Recovery Point Objective)\nC) Cost is a factor but not what RTO measures\nD) Testing frequency is separate from RTO"}'),
       (17, 17, 'Cloud Architecture - Disaster Recovery', 'Comprehension', 'Domain 1',
        'What is the difference between RPO and RTO in disaster recovery?',
        '[{"text": "A) RPO is about data loss, RTO is about downtime", "isCorrect": true}, {"text": "B) RPO is about downtime, RTO is about data loss", "isCorrect": false}, {"text": "C) RPO and RTO both measure downtime but in different units", "isCorrect": false}, {"text": "D) RPO is for applications, RTO is for infrastructure", "isCorrect": false}]',
        'A) RPO is about data loss, RTO is about downtime',
        'RPO (Recovery Point Objective) measures acceptable data loss, while RTO (Recovery Time Objective) measures acceptable downtime.',
        '{"summary": "RPO vs RTO:", "breakdown": ["RPO: Maximum tolerable data loss (measured in time)", "RTO: Maximum tolerable downtime", "RPO drives backup frequency requirements", "RTO drives disaster recovery strategy selection"], "otherOptions": "B) This reverses the definitions\nC) They measure different aspects of recovery\nD) Both apply to applications and infrastructure"}'),
       (18, 18, 'Cloud Architecture - Disaster Recovery', 'Application', 'Domain 1',
        'Your organization has an RTO of 2 hours and RPO of 30 minutes for a critical application. Which disaster recovery strategy best meets these requirements?',
        '[{"text": "A) Cold site with daily backups", "isCorrect": false}, {"text": "B) Warm site with automated failover", "isCorrect": true}, {"text": "C) Hot site with real-time replication", "isCorrect": false}, {"text": "D) Backup and restore only", "isCorrect": false}]',
        'B) Warm site with automated failover',
        'Warm site provides the right balance of cost and recovery time to meet 2-hour RTO requirements.',
        '{"summary": "Warm site characteristics for this scenario:", "breakdown": ["Can achieve 2-hour RTO with quick startup", "30-minute RPO achievable with frequent backups", "More cost-effective than hot site", "Automated failover reduces manual intervention"], "otherOptions": "A) Cold site takes too long for 2-hour RTO\nC) Hot site exceeds requirements and increases cost\nD) Backup/restore cannot meet 2-hour RTO"}'),
       (19, 19, 'Cloud Architecture - Multicloud', 'Application', 'Domain 1',
        'Your organization wants to avoid vendor lock-in while leveraging best-of-breed services from multiple cloud providers. What strategy should you implement?',
        '[{"text": "A) Single cloud deployment with multiple regions", "isCorrect": false}, {"text": "B) Multicloud tenancy strategy", "isCorrect": true}, {"text": "C) Hybrid cloud with on-premises integration", "isCorrect": false}, {"text": "D) Private cloud deployment only", "isCorrect": false}]',
        'B) Multicloud tenancy strategy',
        'Multicloud tenancy allows using services from multiple providers, avoiding vendor lock-in while accessing optimal services.',
        '{"summary": "Multicloud tenancy benefits:", "breakdown": ["Avoids dependency on a single cloud provider", "Enables selection of best services from each provider", "Provides redundancy and improved reliability", "Allows cost optimization through competitive pricing"], "otherOptions": "A) Single cloud still creates vendor dependency\nC) Hybrid focuses on on-premises integration\nD) Private cloud limits service options"}'),
       (20, 20, 'Cloud Architecture - Multicloud', 'Comprehension', 'Domain 1',
        'Which scenario best represents a valid use case for multicloud strategy?',
        '[{"text": "A) Using AWS for compute and Azure for AI/ML services", "isCorrect": true}, {"text": "B) Using the same cloud provider in multiple regions", "isCorrect": false}, {"text": "C) Using private cloud for all applications", "isCorrect": false}, {"text": "D) Using on-premises servers with cloud storage", "isCorrect": false}]',
        'A) Using AWS for compute and Azure for AI/ML services',
        'Multicloud involves using different cloud providers for different services based on their strengths.',
        '{"summary": "Multicloud strategy benefits:", "breakdown": ["Best-of-breed service selection", "Avoid vendor lock-in", "Improved negotiating position", "Reduced single point of failure risk"], "otherOptions": "B) Multiple regions with same provider is not multicloud\nC) Private cloud only is not multicloud\nD) Hybrid cloud, not multicloud"}'),
       (21, 21, 'Deployments - Migration Types', 'Comprehension', 'Domain 2',
        'Which migration strategy involves moving applications to the cloud with minimal changes, often called "lift and shift"?',
        '[{"text": "A) Rehosting", "isCorrect": true}, {"text": "B) Refactoring", "isCorrect": false}, {"text": "C) Rearchitecting", "isCorrect": false}, {"text": "D) Rebuilding", "isCorrect": false}]',
        'A) Rehosting', 'Rehosting (lift and shift) moves applications to cloud with minimal modification.',
        '{"summary": "Rehosting characteristics:", "breakdown": ["Minimal application changes required", "Fastest migration approach", "Lower initial cost and complexity", "May not fully utilize cloud benefits"], "otherOptions": "B) Refactoring modifies applications for cloud\nC) Rearchitecting redesigns for cloud-native\nD) Rebuilding creates new applications"}'),
       (22, 22, 'Deployments - Migration Types', 'Application', 'Domain 2',
        'Your legacy application needs significant changes to take advantage of cloud-native features like auto-scaling and managed services. Which migration approach is most appropriate?',
        '[{"text": "A) Rehosting", "isCorrect": false}, {"text": "B) Refactoring", "isCorrect": true}, {"text": "C) Retiring", "isCorrect": false}, {"text": "D) Retaining", "isCorrect": false}]',
        'B) Refactoring',
        'Refactoring involves modifying applications to take advantage of cloud-native features and services.',
        '{"summary": "Refactoring characteristics:", "breakdown": ["Modifies applications for cloud optimization", "Enables use of managed services", "Improves scalability and performance", "Higher effort than rehosting but better ROI"], "otherOptions": "A) Rehosting doesn''t modify applications\nC) Retiring eliminates the application\nD) Retaining keeps app on-premises"}'),
       (23, 23, 'Deployments - Migration Types', 'Knowledge', 'Domain 2',
        'Which migration strategy involves completely redesigning an application to be cloud-native from the ground up?',
        '[{"text": "A) Rehosting", "isCorrect": false}, {"text": "B) Refactoring", "isCorrect": false}, {"text": "C) Rearchitecting", "isCorrect": true}, {"text": "D) Replacing", "isCorrect": false}]',
        'C) Rearchitecting',
        'Rearchitecting involves completely redesigning applications to be cloud-native and take full advantage of cloud capabilities.',
        '{"summary": "Rearchitecting characteristics:", "breakdown": ["Complete application redesign", "Full utilization of cloud-native features", "Highest development effort and cost", "Maximum long-term benefits and flexibility"], "otherOptions": "A) Rehosting moves with minimal changes\nB) Refactoring makes modifications but not complete redesign\nD) Replacing uses different software"}'),
       (24, 24, 'Deployments - Infrastructure as Code', 'Application', 'Domain 2',
        'Your team needs to deploy identical environments across development, staging, and production. Which approach ensures consistency and reduces manual errors?',
        '[{"text": "A) Manual deployment through cloud console", "isCorrect": false}, {"text": "B) Infrastructure as Code (IaC) templates", "isCorrect": true}, {"text": "C) Copying virtual machine images", "isCorrect": false}, {"text": "D) Using different configurations for each environment", "isCorrect": false}]',
        'B) Infrastructure as Code (IaC) templates',
        'IaC templates ensure consistent, repeatable, and version-controlled infrastructure deployments.',
        '{"summary": "IaC benefits for environment consistency:", "breakdown": ["Version-controlled infrastructure definitions", "Automated and repeatable deployments", "Eliminates configuration drift", "Enables testing of infrastructure changes"], "otherOptions": "A) Manual processes are error-prone\nC) VM images don''t cover full infrastructure\nD) Different configs create inconsistency"}'),
       (25, 25, 'Deployments - Infrastructure as Code', 'Comprehension', 'Domain 2',
        'What is the primary benefit of using declarative Infrastructure as Code templates?',
        '[{"text": "A) Faster execution than imperative scripts", "isCorrect": false}, {"text": "B) Describes desired end state rather than step-by-step instructions", "isCorrect": true}, {"text": "C) Requires less storage space than imperative code", "isCorrect": false}, {"text": "D) Works only with specific cloud providers", "isCorrect": false}]',
        'B) Describes desired end state rather than step-by-step instructions',
        'Declarative IaC describes the desired end state, allowing the system to determine how to achieve it.',
        '{"summary": "Declarative IaC benefits:", "breakdown": ["Describes desired infrastructure state", "System determines implementation steps", "Idempotent operations (safe to run multiple times)", "Easier to understand and maintain"], "otherOptions": "A) Execution speed depends on implementation\nC) Storage size not a primary consideration\nD) Many tools work across multiple providers"}'),
       (26, 26, 'Deployments - Infrastructure as Code', 'Knowledge', 'Domain 2',
        'Which of the following is a key characteristic of Infrastructure as Code (IaC)?',
        '[{"text": "A) Infrastructure is managed manually through web consoles", "isCorrect": false}, {"text": "B) Infrastructure is defined in code and version controlled", "isCorrect": true}, {"text": "C) Infrastructure changes require physical hardware installation", "isCorrect": false}, {"text": "D) Infrastructure is managed by third-party vendors only", "isCorrect": false}]',
        'B) Infrastructure is defined in code and version controlled',
        'IaC treats infrastructure as code that can be version controlled, tested, and deployed programmatically.',
        '{"summary": "IaC key characteristics:", "breakdown": ["Infrastructure defined in code files", "Version control for infrastructure changes", "Automated deployment and provisioning", "Repeatable and consistent deployments"], "otherOptions": "A) IaC eliminates manual console management\nC) IaC works with virtual/cloud infrastructure\nD) IaC can be managed by internal teams"}'),
       (27, 27, 'Deployments - Deployment Strategies', 'Application', 'Domain 2',
        'Your application needs zero-downtime deployment with the ability to quickly rollback if issues occur. Which deployment strategy is most appropriate?',
        '[{"text": "A) Blue-green deployment", "isCorrect": true}, {"text": "B) In-place deployment", "isCorrect": false}, {"text": "C) Rolling deployment", "isCorrect": false}, {"text": "D) Recreate deployment", "isCorrect": false}]',
        'A) Blue-green deployment',
        'Blue-green deployment provides zero downtime and instant rollback capabilities by maintaining two identical environments.',
        '{"summary": "Blue-green deployment characteristics:", "breakdown": ["Two identical production environments", "Instant traffic switching between environments", "Zero downtime during deployment", "Quick rollback by switching traffic back"], "otherOptions": "B) In-place deployment causes downtime\nC) Rolling deployment has slower rollback\nD) Recreate deployment causes downtime"}'),
       (28, 28, 'Deployments - Deployment Strategies', 'Comprehension', 'Domain 2',
        'What is the primary advantage of canary deployment strategy?',
        '[{"text": "A) Fastest deployment method", "isCorrect": false}, {"text": "B) Lowest resource requirements", "isCorrect": false}, {"text": "C) Limits impact of issues to small user subset", "isCorrect": true}, {"text": "D) Simplest to implement", "isCorrect": false}]',
        'C) Limits impact of issues to small user subset',
        'Canary deployment gradually releases changes to small user groups, limiting the impact of potential issues.',
        '{"summary": "Canary deployment benefits:", "breakdown": ["Gradual rollout to subset of users", "Early detection of issues with limited impact", "Ability to monitor and validate changes", "Reduced risk of widespread problems"], "otherOptions": "A) Not the fastest method\nB) Requires additional monitoring infrastructure\nD) More complex than simple deployments"}'),
       (29, 29, 'Deployments - Deployment Strategies', 'Knowledge', 'Domain 2',
        'In a rolling deployment strategy, how are application instances updated?',
        '[{"text": "A) All instances are updated simultaneously", "isCorrect": false}, {"text": "B) Instances are updated one at a time or in small batches", "isCorrect": true}, {"text": "C) New instances are created while old ones remain", "isCorrect": false}, {"text": "D) All instances are shut down before updates", "isCorrect": false}]',
        'B) Instances are updated one at a time or in small batches',
        'Rolling deployment updates instances gradually, one at a time or in small batches, maintaining service availability.',
        '{"summary": "Rolling deployment characteristics:", "breakdown": ["Gradual instance updates", "Maintains service availability", "Lower resource requirements than blue-green", "Slower rollback process"], "otherOptions": "A) That describes in-place deployment\nC) That describes blue-green deployment\nD) That would cause service interruption"}'),
       (30, 30, 'Deployments - CI/CD', 'Application', 'Domain 2',
        'Your development team wants to automatically deploy code changes to production after passing all tests in the staging environment. Which CI/CD practice should be implemented?',
        '[{"text": "A) Continuous Integration only", "isCorrect": false}, {"text": "B) Continuous Delivery only", "isCorrect": false}, {"text": "C) Continuous Deployment", "isCorrect": true}, {"text": "D) Manual deployment with CI", "isCorrect": false}]',
        'C) Continuous Deployment',
        'Continuous Deployment automatically releases code changes to production after passing all pipeline stages.',
        '{"summary": "Continuous Deployment characteristics:", "breakdown": ["Fully automated pipeline from code to production", "No manual intervention required for deployment", "Requires robust testing and monitoring", "Enables rapid feature delivery to users"], "otherOptions": "A) CI only handles code integration\nB) CD deploys to staging but requires manual production release\nD) Manual deployment contradicts automation goals"}'),
       (31, 31, 'Operations - Scaling', 'Application', 'Domain 3',
        'Your web application experiences predictable traffic spikes every weekday from 9 AM to 5 PM. Which scaling approach would be most cost-effective?',
        '[{"text": "A) Vertical scaling with larger instances", "isCorrect": false}, {"text": "B) Horizontal auto-scaling based on schedule and metrics", "isCorrect": true}, {"text": "C) Manual scaling during business hours", "isCorrect": false}, {"text": "D) Keeping maximum capacity running at all times", "isCorrect": false}]',
        'B) Horizontal auto-scaling based on schedule and metrics',
        'Scheduled auto-scaling with metric triggers optimizes both cost and performance for predictable patterns.',
        '{"summary": "Auto-scaling advantages for predictable traffic:", "breakdown": ["Proactive scaling before traffic spikes", "Automatic scale-down during off-hours", "Combines scheduled and reactive scaling", "Optimizes cost while maintaining performance"], "otherOptions": "A) Vertical scaling requires downtime\nC) Manual scaling is reactive and error-prone\nD) Maximum capacity wastes money during off-hours"}'),
       (32, 32, 'Operations - Scaling', 'Comprehension', 'Domain 3',
        'What is the primary difference between horizontal and vertical scaling?',
        '[{"text": "A) Horizontal adds more instances, vertical increases instance size", "isCorrect": true}, {"text": "B) Horizontal is cheaper, vertical is more expensive", "isCorrect": false}, {"text": "C) Horizontal is automatic, vertical is manual", "isCorrect": false}, {"text": "D) Horizontal works with databases, vertical works with web servers", "isCorrect": false}]',
        'A) Horizontal adds more instances, vertical increases instance size',
        'Horizontal scaling adds more instances (scale out), while vertical scaling increases the capacity of existing instances (scale up).',
        '{"summary": "Scaling approach differences:", "breakdown": ["Horizontal: Scale out - add more instances", "Vertical: Scale up - increase instance capacity", "Horizontal: Better for distributed applications", "Vertical: Simpler but has hardware limits"], "otherOptions": "B) Cost depends on specific implementation\nC) Both can be automated\nD) Both work with various application types"}'),
       (33, 33, 'Operations - Scaling', 'Knowledge', 'Domain 3',
        'Which scaling trigger would be most appropriate for a CPU-intensive application?',
        '[{"text": "A) Memory utilization", "isCorrect": false}, {"text": "B) Network bandwidth", "isCorrect": false}, {"text": "C) CPU utilization", "isCorrect": true}, {"text": "D) Storage capacity", "isCorrect": false}]',
        'C) CPU utilization',
        'CPU-intensive applications should scale based on CPU utilization metrics to ensure adequate processing power.',
        '{"summary": "Scaling trigger selection:", "breakdown": ["CPU utilization for compute-intensive workloads", "Memory utilization for memory-intensive applications", "Network bandwidth for high-throughput applications", "Custom metrics for application-specific needs"], "otherOptions": "A) Memory not primary bottleneck for CPU-intensive apps\nB) Network may not be bottleneck\nD) Storage not relevant for CPU-intensive scaling"}'),
       (34, 34, 'Operations - Monitoring', 'Knowledge', 'Domain 3',
        'Which type of monitoring provides insights into application performance and user experience?',
        '[{"text": "A) Infrastructure monitoring", "isCorrect": false}, {"text": "B) Application Performance Monitoring (APM)", "isCorrect": true}, {"text": "C) Network monitoring", "isCorrect": false}, {"text": "D) Security monitoring", "isCorrect": false}]',
        'B) Application Performance Monitoring (APM)',
        'APM focuses on application behavior, response times, and user experience metrics.',
        '{"summary": "APM monitoring includes:", "breakdown": ["Application response times and throughput", "Error rates and exception tracking", "User experience and transaction traces", "Code-level performance insights"], "otherOptions": "A) Infrastructure monitors servers and resources\nC) Network monitoring focuses on connectivity\nD) Security monitoring tracks threats and vulnerabilities"}'),
       (35, 35, 'Operations - Monitoring', 'Comprehension', 'Domain 3',
        'What is the primary purpose of distributed tracing in microservices architecture?',
        '[{"text": "A) Monitor individual service performance", "isCorrect": false}, {"text": "B) Track requests across multiple services", "isCorrect": true}, {"text": "C) Collect application logs", "isCorrect": false}, {"text": "D) Measure network latency", "isCorrect": false}]',
        'B) Track requests across multiple services',
        'Distributed tracing tracks request paths through multiple services, identifying bottlenecks and failures.',
        '{"summary": "Distributed tracing benefits:", "breakdown": ["Visualizes request journey across microservices", "Identifies latency bottlenecks in service chain", "Correlates spans across distributed components", "Enables root cause analysis for performance issues"], "otherOptions": "A) That''s service-level monitoring\nC) That''s log aggregation\nD) That''s network monitoring"}'),
       (36, 36, 'Operations - Monitoring', 'Application', 'Domain 3',
        'Your application is experiencing intermittent performance issues. Which observability practice would best help trace the request flow through your microservices architecture?',
        '[{"text": "A) Log aggregation", "isCorrect": false}, {"text": "B) Metrics monitoring", "isCorrect": false}, {"text": "C) Distributed tracing", "isCorrect": true}, {"text": "D) Alert configuration", "isCorrect": false}]',
        'C) Distributed tracing',
        'Distributed tracing tracks request paths through multiple services, identifying bottlenecks and failures.',
        '{"summary": "Distributed tracing advantages:", "breakdown": ["Visualizes request journey across microservices", "Identifies latency bottlenecks in service chain", "Correlates spans across distributed components", "Enables root cause analysis for performance issues"], "otherOptions": "A) Logs provide events but not request flow\nB) Metrics show performance but not trace paths\nD) Alerts notify of issues but don''t trace flow"}'),
       (37, 37, 'Operations - Backup Strategies', 'Knowledge', 'Domain 3',
        'Which backup type only captures changes made since the last full backup?',
        '[{"text": "A) Full backup", "isCorrect": false}, {"text": "B) Incremental backup", "isCorrect": false}, {"text": "C) Differential backup", "isCorrect": true}, {"text": "D) Snapshot backup", "isCorrect": false}]',
        'C) Differential backup',
        'Differential backups capture all changes since the last full backup, not since the last backup of any type.',
        '{"summary": "Differential backup characteristics:", "breakdown": ["Captures changes since last full backup", "Faster than full backup, slower than incremental", "Requires only full backup + latest differential for restore", "Size grows until next full backup"], "otherOptions": "A) Full backup captures everything\nB) Incremental captures changes since last backup\nD) Snapshot is point-in-time image"}'),
       (38, 38, 'Operations - Backup Strategies', 'Application', 'Domain 3',
        'Your organization has an RPO of 4 hours for a critical database. The database receives constant updates throughout business hours. Which backup strategy best meets this requirement?',
        '[{"text": "A) Daily full backups at midnight", "isCorrect": false}, {"text": "B) Weekly full backups with daily differentials", "isCorrect": false}, {"text": "C) Full backup weekly with 4-hour incremental backups", "isCorrect": true}, {"text": "D) Monthly full backups with weekly incrementals", "isCorrect": false}]',
        'C) Full backup weekly with 4-hour incremental backups',
        'Incremental backups every 4 hours ensure data loss is limited to the RPO requirement of 4 hours.',
        '{"summary": "Backup strategy for 4-hour RPO:", "breakdown": ["Incremental backups every 4 hours meet RPO exactly", "Weekly full backups provide baseline restore point", "Captures all changes within acceptable data loss window", "Balances storage efficiency with recovery requirements"], "otherOptions": "A) Daily backups allow up to 24 hours data loss\nB) Daily differentials still allow 24 hours data loss\nD) Weekly incrementals allow up to 7 days data loss"}'),
       (39, 39, 'Operations - Backup Strategies', 'Comprehension', 'Domain 3',
        'What is the primary advantage of incremental backups over full backups?',
        '[{"text": "A) Faster restore times", "isCorrect": false}, {"text": "B) Better data compression", "isCorrect": false}, {"text": "C) Less storage space and faster backup time", "isCorrect": true}, {"text": "D) Higher reliability", "isCorrect": false}]',
        'C) Less storage space and faster backup time',
        'Incremental backups only capture changes since the last backup, requiring less storage and time.',
        '{"summary": "Incremental backup advantages:", "breakdown": ["Only backs up changed data since last backup", "Significantly less storage space required", "Faster backup execution time", "Reduced network bandwidth usage"], "otherOptions": "A) Restore times are actually slower\nB) Compression depends on backup software\nD) Reliability depends on backup chain integrity"}'),
       (40, 40, 'Security - IAM', 'Application', 'Domain 4',
        'A developer needs temporary access to debug a production issue in a specific S3 bucket. What is the most secure approach following the principle of least privilege?',
        '[{"text": "A) Add developer to administrators group", "isCorrect": false}, {"text": "B) Create IAM user with permanent S3 full access", "isCorrect": false}, {"text": "C) Create time-limited IAM role with bucket-specific permissions", "isCorrect": true}, {"text": "D) Share root account credentials", "isCorrect": false}]',
        'C) Create time-limited IAM role with bucket-specific permissions',
        'Time-limited IAM roles with specific permissions minimize security exposure while providing necessary access.',
        '{"summary": "Secure temporary access principles:", "breakdown": ["Time-bound access that automatically expires", "Scope limited to specific resources needed", "No permanent credentials to manage", "Audit trail of role assumption"], "otherOptions": "A) Administrative access violates least privilege\nB) Permanent access creates long-term security risk\nD) Root credentials should never be shared"}'),
       (41, 41, 'Security - IAM', 'Comprehension', 'Domain 4',
        'What is the primary difference between Role-Based Access Control (RBAC) and Attribute-Based Access Control (ABAC)?',
        '[{"text": "A) RBAC uses job functions, ABAC uses multiple attributes", "isCorrect": true}, {"text": "B) RBAC is more secure than ABAC", "isCorrect": false}, {"text": "C) RBAC works with cloud, ABAC works with on-premises", "isCorrect": false}, {"text": "D) RBAC is newer technology than ABAC", "isCorrect": false}]',
        'A) RBAC uses job functions, ABAC uses multiple attributes',
        'RBAC assigns permissions based on job roles, while ABAC uses multiple attributes like location, time, and resource sensitivity.',
        '{"summary": "RBAC vs ABAC comparison:", "breakdown": ["RBAC: Access based on predefined roles", "ABAC: Access based on multiple dynamic attributes", "RBAC: Simpler to implement and manage", "ABAC: More flexible and granular control"], "otherOptions": "B) Security depends on implementation\nC) Both work in cloud and on-premises\nD) ABAC is actually newer than RBAC"}'),
       (42, 42, 'Security - IAM', 'Knowledge', 'Domain 4',
        'Which authentication method provides the highest level of security for cloud access?',
        '[{"text": "A) Username and password only", "isCorrect": false}, {"text": "B) Multi-factor authentication (MFA)", "isCorrect": true}, {"text": "C) Single sign-on (SSO)", "isCorrect": false}, {"text": "D) API keys", "isCorrect": false}]',
        'B) Multi-factor authentication (MFA)',
        'MFA requires multiple authentication factors, significantly increasing security by requiring something you know, have, or are.',
        '{"summary": "MFA security benefits:", "breakdown": ["Requires multiple authentication factors", "Protects against credential theft", "Combines knowledge, possession, and inherence factors", "Significantly reduces unauthorized access risk"], "otherOptions": "A) Single factor is easily compromised\nC) SSO is about convenience, not necessarily security\nD) API keys are single factor"}'),
       (43, 43, 'Security - Encryption', 'Comprehension', 'Domain 4',
        'Which encryption approach protects data while it is being transmitted between your application and cloud storage?',
        '[{"text": "A) Encryption at rest", "isCorrect": false}, {"text": "B) Encryption in transit", "isCorrect": true}, {"text": "C) Database encryption", "isCorrect": false}, {"text": "D) File system encryption", "isCorrect": false}]',
        'B) Encryption in transit',
        'Encryption in transit protects data during transmission using protocols like TLS/SSL.',
        '{"summary": "Encryption in transit protects:", "breakdown": ["Data moving between client and server", "API calls and responses", "File uploads and downloads", "Database connections and queries"], "otherOptions": "A) At rest protects stored data\nC) Database encryption protects stored database data\nD) File system encryption protects local storage"}'),
       (44, 44, 'Security - Encryption', 'Application', 'Domain 4',
        'Your organization requires that sensitive data be encrypted both when stored and when transmitted. Which encryption strategy should be implemented?',
        '[{"text": "A) Encryption in transit only", "isCorrect": false}, {"text": "B) Encryption at rest only", "isCorrect": false}, {"text": "C) Both encryption at rest and in transit", "isCorrect": true}, {"text": "D) Application-level encryption only", "isCorrect": false}]',
        'C) Both encryption at rest and in transit',
        'Comprehensive data protection requires encrypting data both when stored (at rest) and when transmitted (in transit).',
        '{"summary": "Complete encryption strategy includes:", "breakdown": ["Encryption at rest protects stored data", "Encryption in transit secures data movement", "Protects against both storage and network attacks", "Meets compliance requirements for data protection"], "otherOptions": "A) Transit-only leaves stored data vulnerable\nB) Rest-only leaves network traffic vulnerable\nD) Application-level alone insufficient for comprehensive protection"}'),
       (45, 45, 'Security - Encryption', 'Knowledge', 'Domain 4',
        'What is the primary purpose of key management in cloud encryption?',
        '[{"text": "A) To reduce encryption costs", "isCorrect": false}, {"text": "B) To securely store, rotate, and control access to encryption keys", "isCorrect": true}, {"text": "C) To improve encryption performance", "isCorrect": false}, {"text": "D) To eliminate the need for encryption", "isCorrect": false}]',
        'B) To securely store, rotate, and control access to encryption keys',
        'Key management ensures encryption keys are securely stored, regularly rotated, and access is properly controlled.',
        '{"summary": "Key management responsibilities:", "breakdown": ["Secure key storage and protection", "Regular key rotation and lifecycle management", "Access control and audit logging", "Key recovery and backup procedures"], "otherOptions": "A) Cost reduction is not primary purpose\nC) Performance optimization is secondary\nD) Key management supports encryption, not eliminates it"}'),
       (46, 46, 'DevOps - CI/CD', 'Knowledge', 'Domain 5',
        'What is the primary purpose of Continuous Integration (CI) in a DevOps pipeline?',
        '[{"text": "A) Automatically deploy to production", "isCorrect": false}, {"text": "B) Integrate code changes frequently and run automated tests", "isCorrect": true}, {"text": "C) Monitor application performance", "isCorrect": false}, {"text": "D) Manage infrastructure as code", "isCorrect": false}]',
        'B) Integrate code changes frequently and run automated tests',
        'CI focuses on frequently integrating code changes and running automated builds and tests.',
        '{"summary": "Continuous Integration benefits:", "breakdown": ["Early detection of integration issues", "Automated build and test execution", "Frequent code integration reduces conflicts", "Faster feedback to development teams"], "otherOptions": "A) Production deployment is Continuous Deployment\nC) Performance monitoring is separate from CI\nD) IaC management is infrastructure automation"}'),
       (47, 47, 'DevOps - CI/CD', 'Comprehension', 'Domain 5',
        'What is the difference between Continuous Delivery and Continuous Deployment?',
        '[{"text": "A) Continuous Delivery deploys automatically, Continuous Deployment requires manual approval", "isCorrect": false}, {"text": "B) Continuous Delivery requires manual approval for production, Continuous Deployment is fully automated", "isCorrect": true}, {"text": "C) They are the same thing with different names", "isCorrect": false}, {"text": "D) Continuous Delivery is for testing, Continuous Deployment is for production", "isCorrect": false}]',
        'B) Continuous Delivery requires manual approval for production, Continuous Deployment is fully automated',
        'Continuous Delivery prepares code for production deployment but requires manual approval, while Continuous Deployment automatically deploys to production.',
        '{"summary": "CD vs CD comparison:", "breakdown": ["Continuous Delivery: Automated pipeline with manual production approval", "Continuous Deployment: Fully automated pipeline to production", "Both require robust testing and quality gates", "Continuous Deployment requires higher confidence in automation"], "otherOptions": "A) This reverses the definitions\nC) They have different automation levels\nD) Both involve production deployment"}'),
       (48, 48, 'DevOps - CI/CD', 'Application', 'Domain 5',
        'Your development team wants to catch integration issues early and run automated tests on every code commit. Which DevOps practice should be implemented first?',
        '[{"text": "A) Continuous Deployment", "isCorrect": false}, {"text": "B) Continuous Integration", "isCorrect": true}, {"text": "C) Infrastructure as Code", "isCorrect": false}, {"text": "D) Configuration Management", "isCorrect": false}]',
        'B) Continuous Integration',
        'Continuous Integration should be implemented first to establish automated builds and testing on every code commit.',
        '{"summary": "CI as foundation practice:", "breakdown": ["Establishes automated build processes", "Runs tests on every code commit", "Provides immediate feedback to developers", "Foundation for more advanced DevOps practices"], "otherOptions": "A) CD builds on CI foundation\nC) IaC is infrastructure focused\nD) Configuration management is separate concern"}'),
       (49, 49, 'DevOps - Containers', 'Comprehension', 'Domain 5',
        'Which container orchestration approach is best for production environments requiring automated scaling and service discovery?',
        '[{"text": "A) Standalone containers managed manually", "isCorrect": false}, {"text": "B) Container orchestration platform like Kubernetes", "isCorrect": true}, {"text": "C) Virtual machines with containers installed", "isCorrect": false}, {"text": "D) Containers running on a single host", "isCorrect": false}]',
        'B) Container orchestration platform like Kubernetes',
        'Container orchestration platforms provide automated management, scaling, and service discovery for production workloads.',
        '{"summary": "Orchestration platform benefits:", "breakdown": ["Automated container lifecycle management", "Built-in scaling and load balancing", "Service discovery and networking", "Rolling updates and rollback capabilities"], "otherOptions": "A) Manual management doesn''t scale for production\nC) VMs add unnecessary overhead\nD) Single host creates single point of failure"}'),
       (50, 50, 'DevOps - Containers', 'Application', 'Domain 5',
        'Your team needs to deploy a microservices application that can automatically scale, handle failures, and manage service discovery. Which approach is most suitable?',
        '[{"text": "A) Standalone containers on virtual machines", "isCorrect": false}, {"text": "B) Container orchestration with Kubernetes", "isCorrect": true}, {"text": "C) Virtual machines with manual deployment", "isCorrect": false}, {"text": "D) Serverless functions only", "isCorrect": false}]',
        'B) Container orchestration with Kubernetes',
        'Kubernetes provides comprehensive container orchestration with auto-scaling, self-healing, and service discovery capabilities.',
        '{"summary": "Kubernetes orchestration features:", "breakdown": ["Automatic scaling based on resource utilization", "Self-healing with pod restart and rescheduling", "Built-in service discovery and load balancing", "Rolling updates and rollback capabilities"], "otherOptions": "A) Standalone containers lack orchestration features\nC) VMs with manual deployment don''t provide automation\nD) Serverless alone insufficient for complex microservices"}'),
       (51, 51, 'Troubleshooting - Performance', 'Application', 'Domain 6',
        'Users report slow application response times. Your monitoring shows high CPU utilization but normal memory and disk usage. What should be your first troubleshooting step?',
        '[{"text": "A) Increase memory allocation", "isCorrect": false}, {"text": "B) Analyze CPU-intensive processes and optimize or scale CPU resources", "isCorrect": true}, {"text": "C) Restart all application servers", "isCorrect": false}, {"text": "D) Increase disk storage capacity", "isCorrect": false}]',
        'B) Analyze CPU-intensive processes and optimize or scale CPU resources',
        'High CPU utilization directly correlates with the performance issue, making CPU analysis the logical first step.',
        '{"summary": "CPU troubleshooting approach:", "breakdown": ["Identify CPU-intensive processes or queries", "Analyze application code for optimization opportunities", "Consider vertical scaling for more CPU power", "Implement horizontal scaling to distribute load"], "otherOptions": "A) Memory is not the bottleneck here\nC) Restart is temporary and doesn''t address root cause\nD) Disk capacity is not related to CPU issues"}'),
       (52, 52, 'Troubleshooting - Performance', 'Comprehension', 'Domain 6',
        'What is the most likely cause of high IOPS (Input/Output Operations Per Second) in a cloud environment?',
        '[{"text": "A) Network latency issues", "isCorrect": false}, {"text": "B) Database queries or file system operations", "isCorrect": true}, {"text": "C) CPU overutilization", "isCorrect": false}, {"text": "D) Memory leaks", "isCorrect": false}]',
        'B) Database queries or file system operations',
        'High IOPS typically indicates intensive database operations or file system read/write activities.',
        '{"summary": "Common causes of high IOPS:", "breakdown": ["Database queries and transactions", "File system read/write operations", "Application logging activities", "Backup and data replication processes"], "otherOptions": "A) Network latency affects throughput, not IOPS\nC) CPU issues don''t directly cause IOPS\nD) Memory leaks affect memory usage, not IOPS"}'),
       (53, 53, 'Troubleshooting - Network', 'Comprehension', 'Domain 6',
        'An application cannot connect to a database in another subnet. The database is running and accessible from other sources. What is the most likely cause?',
        '[{"text": "A) Database server is down", "isCorrect": false}, {"text": "B) Network security group or firewall blocking the connection", "isCorrect": true}, {"text": "C) DNS resolution failure", "isCorrect": false}, {"text": "D) Application code error", "isCorrect": false}]',
        'B) Network security group or firewall blocking the connection',
        'Network connectivity issues between subnets typically involve security group or firewall configuration problems.',
        '{"summary": "Network troubleshooting for subnet connectivity:", "breakdown": ["Check security group rules for required ports", "Verify network ACL configurations", "Ensure route table entries for subnet communication", "Confirm firewall rules on both source and destination"], "otherOptions": "A) Database is confirmed running and accessible\nC) DNS would affect name resolution, not subnet connectivity\nD) Code error wouldn''t be subnet-specific"}'),
       (54, 54, 'Troubleshooting - Network', 'Application', 'Domain 6',
        'A multi-tier application can communicate between web and application tiers, but the application tier cannot reach the database tier. What should you check first?',
        '[{"text": "A) Web server configuration", "isCorrect": false}, {"text": "B) Application server logs", "isCorrect": false}, {"text": "C) Network security groups and routing between application and database tiers", "isCorrect": true}, {"text": "D) Database server hardware status", "isCorrect": false}]',
        'C) Network security groups and routing between application and database tiers',
        'Network connectivity issues between specific tiers typically involve security group rules or routing configuration.',
        '{"summary": "Network troubleshooting for tier connectivity:", "breakdown": ["Security groups may block database port access", "Subnet routing tables might be misconfigured", "Network ACLs could prevent tier communication", "VPC peering or transit gateway issues possible"], "otherOptions": "A) Web tier communication works, not a web server issue\nB) App logs won''t show network configuration problems\nD) Hardware status wouldn''t be tier-specific"}'),
       (55, 55, 'Troubleshooting - Security', 'Comprehension', 'Domain 6',
        'An application suddenly cannot access a cloud storage bucket that worked fine yesterday. No code changes were made. What is the most likely cause?',
        '[{"text": "A) Storage bucket has been deleted", "isCorrect": false}, {"text": "B) Network connectivity issues", "isCorrect": false}, {"text": "C) IAM permissions or security policies changed", "isCorrect": true}, {"text": "D) Application server hardware failure", "isCorrect": false}]',
        'C) IAM permissions or security policies changed',
        'Sudden access failures without code changes typically indicate permission or security policy modifications.',
        '{"summary": "Common causes of sudden access loss:", "breakdown": ["IAM role permissions modified or revoked", "Security group rules changed", "Access keys expired or rotated", "Bucket policies or ACLs updated"], "otherOptions": "A) Deletion would affect all access, not just this app\nB) Network issues would show connectivity errors\nD) Hardware failure would cause broader application issues"}'),
       (56, 56, 'Cloud Architecture - Microservices', 'Comprehension', 'Domain 1',
        'What is the primary benefit of using microservices architecture in cloud environments?',
        '[{"text": "A) Reduced development complexity", "isCorrect": false}, {"text": "B) Lower infrastructure costs", "isCorrect": false}, {"text": "C) Independent scaling and deployment of services", "isCorrect": true}, {"text": "D) Simplified monitoring and logging", "isCorrect": false}]',
        'C) Independent scaling and deployment of services',
        'Microservices allow each service to be developed, deployed, and scaled independently, providing flexibility and resilience.',
        '{"summary": "Microservices benefits:", "breakdown": ["Independent service deployment and scaling", "Technology diversity across services", "Fault isolation and resilience", "Team autonomy and faster development cycles"], "otherOptions": "A) Actually increases complexity\nB) May increase infrastructure costs\nD) Monitoring becomes more complex"}'),
       (57, 57, 'Cloud Architecture - Managed Services', 'Application', 'Domain 1',
        'Your organization wants to reduce operational overhead for database management while maintaining high availability. Which approach is most suitable?',
        '[{"text": "A) Self-managed database on virtual machines", "isCorrect": false}, {"text": "B) Managed database service with multi-AZ deployment", "isCorrect": true}, {"text": "C) Containerized database with manual orchestration", "isCorrect": false}, {"text": "D) On-premises database with cloud backup", "isCorrect": false}]',
        'B) Managed database service with multi-AZ deployment',
        'Managed database services with multi-AZ deployment provide high availability while reducing operational overhead.',
        '{"summary": "Managed database benefits:", "breakdown": ["Automated patching and maintenance", "Built-in high availability with multi-AZ", "Automated backups and point-in-time recovery", "Reduced operational overhead"], "otherOptions": "A) Self-managed increases operational overhead\nC) Manual orchestration increases complexity\nD) On-premises doesn''t reduce overhead"}'),
       (58, 58, 'Deployments - Version Control', 'Knowledge', 'Domain 2',
        'Which version control operation allows developers to propose changes for review before merging into the main branch?',
        '[{"text": "A) Git push", "isCorrect": false}, {"text": "B) Git commit", "isCorrect": false}, {"text": "C) Pull request", "isCorrect": true}, {"text": "D) Git merge", "isCorrect": false}]',
        'C) Pull request',
        'Pull requests enable code review and discussion before changes are merged into the main codebase.',
        '{"summary": "Pull request benefits:", "breakdown": ["Facilitates peer code review process", "Enables discussion and feedback on changes", "Maintains code quality through review gates", "Provides audit trail of changes and approvals"], "otherOptions": "A) Push uploads code to repository\nB) Commit saves changes locally\nD) Merge combines branches without review"}'),
       (59, 59, 'Operations - Lifecycle Management', 'Comprehension', 'Domain 3',
        'What is the primary purpose of patch management in cloud environments?',
        '[{"text": "A) Improve application performance", "isCorrect": false}, {"text": "B) Reduce infrastructure costs", "isCorrect": false}, {"text": "C) Address security vulnerabilities and bugs", "isCorrect": true}, {"text": "D) Increase storage capacity", "isCorrect": false}]',
        'C) Address security vulnerabilities and bugs',
        'Patch management primarily addresses security vulnerabilities and software bugs to maintain system security and stability.',
        '{"summary": "Patch management objectives:", "breakdown": ["Address security vulnerabilities", "Fix software bugs and issues", "Maintain system stability", "Ensure compliance with security standards"], "otherOptions": "A) Performance improvements are secondary\nB) Cost reduction is not primary purpose\nD) Storage capacity is unrelated to patching"}'),
       (60, 60, 'Security - Compliance', 'Knowledge', 'Domain 4',
        'Which compliance framework is specifically designed for organizations handling credit card data?',
        '[{"text": "A) SOC 2", "isCorrect": false}, {"text": "B) GDPR", "isCorrect": false}, {"text": "C) PCI DSS", "isCorrect": true}, {"text": "D) HIPAA", "isCorrect": false}]',
        'C) PCI DSS',
        'PCI DSS (Payment Card Industry Data Security Standard) is specifically designed for organizations that handle credit card data.',
        '{"summary": "PCI DSS requirements:", "breakdown": ["Secure network and system configuration", "Protect cardholder data", "Maintain vulnerability management program", "Implement access control measures"], "otherOptions": "A) SOC 2 is for service organizations\nB) GDPR is for data privacy\nD) HIPAA is for healthcare data"}'),
       (61, 61, 'Cloud Migration and Capacity Planning', 'Application', 'Domain 1: Cloud Architecture and Design',
        'A retail company with 200 stores operates a legacy inventory system requiring 48 CPU cores, 256GB RAM, and 10TB storage with 15,000 IOPS. The system experiences 300% load increase during Black Friday sales. They want to migrate to the cloud with the ability to handle peak loads cost-effectively. Which migration strategy best meets these requirements?',
        '[{"text": "A) Lift-and-shift to cloud with 3x capacity provisioned year-round", "isCorrect": false}, {"text": "B) Re-architect as microservices with auto-scaling and cloud-native database", "isCorrect": true}, {"text": "C) Hybrid approach keeping database on-premises with cloud compute", "isCorrect": false}, {"text": "D) Containerize the application as-is and deploy to managed Kubernetes", "isCorrect": false}]',
        'B) Re-architect as microservices with auto-scaling and cloud-native database',
        'Re-architecting as microservices enables auto-scaling for the 300% peak load without over-provisioning year-round, while cloud-native databases provide elastic IOPS scaling.',
        '{"summary": "Microservices migration benefits for variable loads:", "breakdown": ["Auto-scaling handles 300% peak without year-round costs", "Cloud-native database scales IOPS on demand", "Service isolation allows scaling only needed components", "Pay-per-use model optimizes costs during normal operations"], "otherOptions": "A) 3x capacity year-round wastes resources and budget\nC) Hybrid approach doesn''t solve IOPS scaling challenge\nD) Containerizing monolith doesn''t enable granular scaling"}'),
       (62, 62, 'Cloud Security and Compliance', 'Analysis', 'Domain 2: Security',
        'A healthcare provider must implement cloud storage for patient records with these requirements: data encrypted at rest and in transit, 7-year retention for compliance, access logs retained for 1 year, and automatic deletion after retention period. They need to prove compliance during audits. Which security controls combination ensures all requirements are met?',
        '[{"text": "A) Client-side encryption, manual lifecycle policies, and quarterly access reviews", "isCorrect": false}, {"text": "B) Cloud provider encryption, automated lifecycle rules, immutable audit logs, and compliance certificates", "isCorrect": true}, {"text": "C) Third-party encryption tools, backup to tape, and annual compliance audits", "isCorrect": false}, {"text": "D) Database encryption, daily backups, and manual log reviews", "isCorrect": false}]',
        'B) Cloud provider encryption, automated lifecycle rules, immutable audit logs, and compliance certificates',
        'Automated lifecycle rules ensure compliant retention and deletion, immutable audit logs provide tamper-proof evidence, and cloud provider compliance certificates demonstrate adherence to healthcare standards.',
        '{"summary": "Healthcare compliance in cloud storage requires:", "breakdown": ["Automated lifecycle policies prevent human error in retention", "Immutable audit logs ensure tamper-proof compliance evidence", "Provider encryption meets regulatory requirements efficiently", "Compliance certificates (HIPAA, SOC2) simplify audit process"], "otherOptions": "A) Manual processes risk non-compliance through human error\nC) Tape backups don''t provide automated deletion\nD) Manual reviews insufficient for audit requirements"}'),
       (63, 63, 'Cloud Automation and Orchestration', 'Application', 'Domain 3: Deployment',
        'A development team deploys applications across dev, test, and prod environments in multiple cloud regions. They currently spend 15 hours weekly on manual deployments with a 5% error rate causing rollbacks. Which automation approach would best reduce deployment time and errors while maintaining environment-specific configurations?',
        '[{"text": "A) Shell scripts with environment variables for each deployment", "isCorrect": false}, {"text": "B) Infrastructure as Code with parameterized templates and CI/CD pipelines", "isCorrect": true}, {"text": "C) Configuration management tools with manual approval gates", "isCorrect": false}, {"text": "D) Container images with hardcoded environment settings", "isCorrect": false}]',
        'B) Infrastructure as Code with parameterized templates and CI/CD pipelines',
        'Infrastructure as Code with parameterized templates enables consistent deployments across environments while CI/CD pipelines automate the process, reducing both time and error rates.',
        '{"summary": "IaC and CI/CD benefits for multi-environment deployments:", "breakdown": ["Parameterized templates handle environment-specific configs", "Version control tracks all infrastructure changes", "Automated testing catches errors before production", "Consistent deployments reduce error rate from 5% to <1%"], "otherOptions": "A) Shell scripts lack version control and testing capabilities\nC) Manual approvals don''t reduce deployment time\nD) Hardcoded settings prevent environment flexibility"}'),
       (64, 64, 'Cloud Performance Optimization', 'Analysis', 'Domain 4: Operations and Support',
        'A SaaS application experiences intermittent performance issues reported by 15% of users. Monitoring shows normal CPU (40%), memory (60%), and network (30%) utilization. However, user session recordings reveal 3-second delays during specific database queries. What combination of tools and techniques would best identify and resolve the root cause?',
        '[{"text": "A) Increase server resources and implement load balancing", "isCorrect": false}, {"text": "B) Enable database query profiling, analyze execution plans, and implement query optimization with caching", "isCorrect": true}, {"text": "C) Add more monitoring agents and create utilization alerts", "isCorrect": false}, {"text": "D) Migrate to faster storage and increase network bandwidth", "isCorrect": false}]',
        'B) Enable database query profiling, analyze execution plans, and implement query optimization with caching',
        'Database query profiling identifies specific slow queries, execution plan analysis reveals inefficiencies, and strategic caching prevents repeated expensive operations.',
        '{"summary": "Database performance troubleshooting approach:", "breakdown": ["Query profiling pinpoints exact problematic queries", "Execution plans reveal missing indexes or inefficient joins", "Query optimization reduces 3-second delays to milliseconds", "Caching frequently accessed data prevents repeated slow queries"], "otherOptions": "A) Resources aren''t the issue (40% CPU, 60% memory)\nC) More monitoring won''t fix identified query delays\nD) Hardware upgrades don''t address query inefficiency"}'),
       (65, 65, 'Cloud Business Continuity', 'Application', 'Domain 5: Troubleshooting',
        'During a cloud provider outage, a company''s primary region becomes unavailable. Their disaster recovery plan activates, but the failover process takes 6 hours instead of the planned 2 hours. Post-incident analysis reveals DNS propagation delays and cold database replicas. Which improvements would most effectively achieve the 2-hour RTO target?',
        '[{"text": "A) Increase backup frequency and add more regions", "isCorrect": false}, {"text": "B) Implement DNS pre-staging with low TTL and maintain warm database replicas", "isCorrect": true}, {"text": "C) Create detailed runbooks and conduct monthly drills", "isCorrect": false}, {"text": "D) Purchase dedicated network connections between regions", "isCorrect": false}]',
        'B) Implement DNS pre-staging with low TTL and maintain warm database replicas',
        'DNS pre-staging with low TTL ensures rapid traffic redirection while warm database replicas eliminate lengthy data loading and cache warming during failover.',
        '{"summary": "Achieving 2-hour RTO requires addressing specific bottlenecks:", "breakdown": ["Low TTL DNS (5 minutes) enables quick traffic switching", "DNS pre-staging eliminates record creation time during disaster", "Warm replicas maintain recent data and cache, ready for traffic", "Combined approach reduces failover from 6 hours to under 2 hours"], "otherOptions": "A) More backups don''t address DNS or cold replica issues\nC) Runbooks help execution but don''t fix technical delays\nD) Network connections don''t solve DNS propagation delays"}'),
       (66, 66, 'Cloud Cost Management', 'Analysis', 'Domain 1: Cloud Architecture and Design',
        'A company''s cloud bill increased 150% over 6 months despite stable user numbers. Investigation reveals: 500 unused elastic IPs, 200TB of orphaned snapshots, 50 stopped but not terminated instances, and development databases running 24/7 on production-grade hardware. Which cost optimization strategy would yield the greatest immediate savings?',
        '[{"text": "A) Negotiate enterprise discounts and purchase reserved capacity", "isCorrect": false}, {"text": "B) Implement resource tagging and automated cleanup policies for unused resources", "isCorrect": true}, {"text": "C) Migrate to a different cloud provider with lower rates", "isCorrect": false}, {"text": "D) Reduce application features to decrease resource usage", "isCorrect": false}]',
        'B) Implement resource tagging and automated cleanup policies for unused resources',
        'Automated cleanup policies immediately eliminate costs from unused resources (IPs, snapshots, stopped instances) while resource tagging enables ongoing cost visibility and management.',
        '{"summary": "Immediate cost reduction through resource hygiene:", "breakdown": ["Unused elastic IPs: $0.005/hour each = $1,800/month savings", "Orphaned snapshots: $0.05/GB/month = $10,000/month savings", "Stopped instances: Still incur storage costs, termination saves 100%", "Automated policies prevent future resource accumulation"], "otherOptions": "A) Reserved capacity doesn''t address unused resources\nC) Migration costs outweigh potential savings\nD) Feature reduction impacts business unnecessarily"}'),
       (67, 67, 'Cloud Network Architecture', 'Application', 'Domain 1: Cloud Architecture and Design',
        'A global company needs to connect 15 branch offices to their cloud infrastructure. Each office has different bandwidth requirements (10Mbps to 1Gbps) and varying security policies. Current MPLS costs are $50,000/month. Which cloud networking solution provides the most flexible and cost-effective approach?',
        '[{"text": "A) Individual site-to-site VPNs for each branch office", "isCorrect": false}, {"text": "B) SD-WAN overlay with cloud backbone and local internet breakout", "isCorrect": true}, {"text": "C) Direct dedicated connections from each office to cloud", "isCorrect": false}, {"text": "D) Hub-and-spoke topology with central data center", "isCorrect": false}]',
        'B) SD-WAN overlay with cloud backbone and local internet breakout',
        'SD-WAN provides flexible bandwidth allocation, policy-based routing, and leverages cost-effective internet connections while maintaining security and performance.',
        '{"summary": "SD-WAN advantages for multi-site cloud connectivity:", "breakdown": ["Dynamic bandwidth allocation based on real-time needs", "Local internet breakout reduces backhaul costs", "Policy-based routing enforces security requirements per site", "Typical 40-60% cost reduction versus MPLS"], "otherOptions": "A) 15 individual VPNs create management complexity\nC) Dedicated connections too expensive for small sites\nD) Hub-and-spoke creates bottlenecks and latency"}'),
       (68, 68, 'Cloud Monitoring and Logging', 'Analysis', 'Domain 4: Operations and Support',
        'An e-commerce platform processes 1 million transactions daily across 50 microservices. The ops team struggles to troubleshoot issues due to distributed logs, missing correlation IDs, and 10TB daily log volume. Which observability strategy best addresses these challenges?',
        '[{"text": "A) Centralize all logs to a single database with full-text search", "isCorrect": false}, {"text": "B) Implement distributed tracing, structured logging with correlation IDs, and intelligent log sampling", "isCorrect": true}, {"text": "C) Increase log retention and add more verbose logging", "isCorrect": false}, {"text": "D) Create service-specific dashboards with custom metrics", "isCorrect": false}]',
        'B) Implement distributed tracing, structured logging with correlation IDs, and intelligent log sampling',
        'Distributed tracing provides end-to-end visibility, correlation IDs link related events across services, and intelligent sampling reduces volume while preserving important events.',
        '{"summary": "Modern observability for microservices requires:", "breakdown": ["Distributed tracing shows complete request flow across services", "Correlation IDs enable tracking single transactions through 50 services", "Structured logging improves queryability and reduces storage", "Intelligent sampling keeps important events while reducing volume 80%"], "otherOptions": "A) Single database can''t handle 10TB daily efficiently\nC) More logs worsen the volume problem\nD) Dashboards don''t solve log correlation issues"}'),
       (69, 69, 'Cloud Identity Management', 'Application', 'Domain 2: Security',
        'A company acquires three subsidiaries, each with different identity providers (AD, Google Workspace, Okta). They need unified cloud access for 5,000 total users while maintaining each subsidiary''s existing identity system. Compliance requires MFA and privileged access management. Which identity architecture best meets these requirements?',
        '[{"text": "A) Migrate all users to a single corporate identity provider", "isCorrect": false}, {"text": "B) Implement federated identity with SAML/OIDC, centralized MFA, and PAM solution", "isCorrect": true}, {"text": "C) Create cloud accounts for each subsidiary with separate identity systems", "isCorrect": false}, {"text": "D) Sync all identities to cloud provider''s native directory service", "isCorrect": false}]',
        'B) Implement federated identity with SAML/OIDC, centralized MFA, and PAM solution',
        'Federation allows each subsidiary to maintain their identity provider while SAML/OIDC provides secure cloud access. Centralized MFA and PAM ensure consistent security controls.',
        '{"summary": "Federated identity architecture benefits:", "breakdown": ["SAML/OIDC federation preserves existing identity investments", "Users maintain single credentials (reduced password fatigue)", "Centralized MFA policy applies regardless of source IdP", "PAM solution provides consistent privileged access controls"], "otherOptions": "A) Migration disrupts 5,000 users and requires retraining\nC) Separate accounts prevent unified access and compliance\nD) Sync creates password management and security challenges"}'),
       (70, 70, 'Cloud Service Models', 'Comprehension', 'Domain 1: Cloud Architecture and Design',
        'A software startup needs to choose between IaaS, PaaS, and SaaS solutions for their new mobile app backend. They have 3 developers, limited DevOps experience, need to reach market in 3 months, and have $10,000 monthly budget. Which approach best balances their constraints?',
        '[{"text": "A) IaaS with full control over infrastructure and custom configuration", "isCorrect": false}, {"text": "B) PaaS for backend services with managed databases and authentication", "isCorrect": true}, {"text": "C) SaaS solutions only with no custom development", "isCorrect": false}, {"text": "D) Hybrid approach with IaaS compute and SaaS databases", "isCorrect": false}]',
        'B) PaaS for backend services with managed databases and authentication',
        'PaaS provides the right abstraction level for a small team, offering managed services that accelerate development while staying within budget and timeline constraints.',
        '{"summary": "PaaS advantages for startups:", "breakdown": ["Managed infrastructure reduces DevOps burden on 3-person team", "Built-in services (auth, databases) accelerate 3-month timeline", "Pay-per-use model fits $10,000 budget with room to scale", "Focus remains on app development, not infrastructure"], "otherOptions": "A) IaaS requires DevOps expertise they lack\nC) Pure SaaS too limiting for custom mobile backend\nD) Hybrid approach adds unnecessary complexity"}'),
       (71, 71, 'DevOps - Automation', 'Application', 'Domain 5',
        'A DevOps team wants to automate the provisioning of new virtual machines, network configurations, and security groups whenever a new project starts. Which practice is best suited for this task?',
        '[{"text": "A) Manual configuration via cloud console for each project.", "isCorrect": false}, {"text": "B) Using shell scripts for server setup and manual network configuration.", "isCorrect": false}, {"text": "C) Implementing Infrastructure as Code (IaC) with templating tools.", "isCorrect": true}, {"text": "D) Creating a comprehensive manual checklist for infrastructure setup.", "isCorrect": false}]',
        'C) Implementing Infrastructure as Code (IaC) with templating tools.',
        'Infrastructure as Code (IaC) allows defining and provisioning infrastructure using code, ensuring repeatability, consistency, and reduced manual errors for new project environments.',
        '{"summary": "IaC for automated provisioning:", "breakdown": ["**Repeatability:** Ensures identical environments are deployed every time.", "**Consistency:** Eliminates configuration drift between environments.", "**Reduced Errors:** Automates complex setup processes, minimizing human error.", "**Version Control:** Infrastructure definitions can be versioned and managed like application code."], "otherOptions": "A) Manual configuration is prone to errors and inconsistencies, especially for complex setups. \nB) Shell scripts can automate some tasks but typically lack the comprehensive state management and idempotency of IaC tools for full infrastructure provisioning. Network configuration would still likely be manual or poorly managed. \nD) A manual checklist helps but does not automate or guarantee consistency, nor does it reduce the time spent on manual setup."}'),
       (72, 72, 'DevOps - CI/CD', 'Analysis', 'Domain 5',
        'A software company is experiencing frequent integration issues and broken builds after developers merge their code. They also have a slow release cycle. Which two (2) DevOps practices should they prioritize to address these problems?',
        '[{"text": "A) Implement Continuous Integration (CI) and establish automated testing.", "isCorrect": true}, {"text": "B) Adopt a Microservices architecture and use serverless functions.", "isCorrect": false}, {"text": "C) Focus on manual code reviews and increase documentation efforts.", "isCorrect": false}, {"text": "D) Implement Continuous Deployment (CD) and roll back frequently.", "isCorrect": false}]',
        'A) Implement Continuous Integration (CI) and establish automated testing.',
        'Continuous Integration (CI) focuses on frequent code integration and automated testing to catch issues early, while establishing automated testing verifies code quality and functionality, addressing broken builds and integration problems. These are foundational to speeding up the release cycle.',
        '{"summary": "Addressing integration issues and slow releases with CI and automated testing:", "breakdown": ["**Continuous Integration (CI):** Integrates code changes frequently (multiple times a day), reducing integration hell and catching conflicts early.", "**Automated Testing:** Runs tests on every code commit or integration, immediately identifying broken builds and ensuring code quality and functionality before merging.", "These two practices are fundamental for ensuring a stable codebase and enabling faster, more reliable releases."], "otherOptions": "B) Microservices and serverless functions are architectural choices that may *support* better CI/CD, but do not directly solve existing integration issues or broken builds. \nC) Manual code reviews are important but often too slow and cannot reliably catch all integration issues in a fast-paced environment. Increasing documentation does not solve technical problems. \nD) Implementing Continuous Deployment *before* fixing CI issues (broken builds, frequent integration problems) would lead to deploying broken software to production more rapidly, exacerbating the problem. Frequent rollbacks indicate a problematic CI/CD pipeline, not a solution."}'),
       (73, 73, 'Cloud Service Models', 'Knowledge', 'Domain 1: Cloud Architecture and Design',
        'A company wants to migrate their email system to the cloud but maintain full control over the operating system and middleware while letting the cloud provider manage the underlying infrastructure. Which service model BEST meets their requirements?',
        '[{"text": "A) Software as a Service (SaaS)", "isCorrect": false}, {"text": "B) Platform as a Service (PaaS)", "isCorrect": false}, {"text": "C) Infrastructure as a Service (IaaS)", "isCorrect": true}, {"text": "D) Function as a Service (FaaS)", "isCorrect": false}]',
        'C) Infrastructure as a Service (IaaS)',
        'IaaS provides virtual infrastructure while allowing customers to maintain control over the operating system, middleware, and applications.',
        '{"summary": "IaaS characteristics for email migration:", "breakdown": ["Customer controls: OS, middleware, applications, and data", "Provider manages: Physical hardware, hypervisor, networking", "Email flexibility: Can install any email server software", "Full administrative access to customize configurations"], "otherOptions": "A) SaaS provides ready-to-use applications with no OS control\nB) PaaS abstracts OS layer, limiting administrative control\nD) FaaS is for serverless functions, not email systems"}'),
       (74, 74, 'Cloud Service Models', 'Application', 'Domain 1: Cloud Architecture and Design',
        'A development team needs a cloud environment where they can deploy applications without managing servers, operating systems, or runtime environments. They want to focus solely on code development and automatic scaling based on demand. Which service model is MOST appropriate?',
        '[{"text": "A) Infrastructure as a Service (IaaS)", "isCorrect": false}, {"text": "B) Platform as a Service (PaaS)", "isCorrect": true}, {"text": "C) Software as a Service (SaaS)", "isCorrect": false}, {"text": "D) Desktop as a Service (DaaS)", "isCorrect": false}]',
        'B) Platform as a Service (PaaS)',
        'PaaS provides a development platform with automated scaling, allowing developers to focus on code while the platform handles infrastructure management.',
        '{"summary": "PaaS benefits for development teams:", "breakdown": ["Abstracted infrastructure: No server or OS management needed", "Built-in scaling: Automatic resource allocation based on demand", "Development tools: Integrated IDEs, databases, and services", "Focus on code: Developers concentrate on application logic"], "otherOptions": "A) IaaS requires managing servers and OS\nC) SaaS provides ready-made applications, not development platforms\nD) DaaS provides virtual desktops, not development platforms"}'),
       (75, 75, 'Shared Responsibility Model', 'Application', 'Domain 1: Cloud Architecture and Design',
        'A healthcare organization is concerned about data security compliance in their PaaS deployment. According to the shared responsibility model, which security aspects remain the customer''s responsibility in a PaaS environment?',
        '[{"text": "A) Physical security of data centers and network controls", "isCorrect": false}, {"text": "B) Application code security, data encryption, and user access management", "isCorrect": true}, {"text": "C) Hypervisor patching and host operating system security", "isCorrect": false}, {"text": "D) Hardware maintenance and power/cooling systems", "isCorrect": false}]',
        'B) Application code security, data encryption, and user access management',
        'In PaaS, customers are responsible for application-layer security including code security, data encryption, and identity/access management.',
        '{"summary": "PaaS customer responsibilities:", "breakdown": ["Application security: Secure coding practices and vulnerability management", "Data protection: Encryption at rest and in transit", "Identity management: User authentication and authorization", "Compliance: Meeting regulatory requirements for data handling"], "otherOptions": "A) Physical security is provider responsibility\nC) Platform infrastructure managed by provider\nD) Hardware and facilities managed by provider"}'),
       (76, 76, 'Cloud Storage Concepts', 'Analysis', 'Domain 1: Cloud Architecture and Design',
        'A media company needs storage for their video editing workflow with the following requirements: high IOPS for database operations, large capacity for raw video files, and long-term archival with cost optimization. Which storage architecture provides the BEST solution?',
        '[{"text": "A) All data on high-performance SSD storage", "isCorrect": false}, {"text": "B) Tiered storage with SSD for databases, HDD for active files, and cold storage for archives", "isCorrect": true}, {"text": "C) Network-attached storage (NAS) for all data types", "isCorrect": false}, {"text": "D) Object storage for all data with automated lifecycle policies", "isCorrect": false}]',
        'B) Tiered storage with SSD for databases, HDD for active files, and cold storage for archives',
        'Tiered storage matches storage types to specific use cases: SSD for high IOPS databases, HDD for large file capacity, cold storage for cost-effective archival.',
        '{"summary": "Optimal tiered storage strategy:", "breakdown": ["SSD tier: High IOPS and low latency for database operations", "HDD tier: Large capacity and moderate performance for active video files", "Cold storage: Cost-effective for long-term archival with slower retrieval", "Lifecycle automation: Automatic data movement based on access patterns"], "otherOptions": "A) All-SSD expensive for large video files and archives\nC) NAS doesn''t optimize for different performance requirements\nD) Object storage alone may not provide required IOPS for databases"}'),
       (77, 77, 'Container Technologies', 'Application', 'Domain 1: Cloud Architecture and Design',
        'A company runs microservices using standalone containers but experiences challenges with scaling, service discovery, and load balancing during peak traffic. Which approach would BEST address these operational challenges?',
        '[{"text": "A) Increase container resource limits and add more standalone containers", "isCorrect": false}, {"text": "B) Implement container orchestration with Kubernetes or Docker Swarm", "isCorrect": true}, {"text": "C) Migrate all containers to virtual machines", "isCorrect": false}, {"text": "D) Use container registries for better image management", "isCorrect": false}]',
        'B) Implement container orchestration with Kubernetes or Docker Swarm',
        'Container orchestration platforms provide automated scaling, service discovery, load balancing, and health management for containerized applications.',
        '{"summary": "Container orchestration benefits:", "breakdown": ["Auto-scaling: Automatic container scaling based on demand", "Service discovery: Automatic service registration and routing", "Load balancing: Built-in traffic distribution across containers", "Health management: Automatic restart of failed containers"], "otherOptions": "A) Manual scaling doesn''t solve automation challenges\nC) VMs don''t provide the orchestration features needed\nD) Registries help with image management but not runtime orchestration"}'),
       (78, 78, 'Cloud Deployment Models', 'Application', 'Domain 2: Cloud Deployment',
        'A financial institution requires strict data sovereignty, complete infrastructure control, and the ability to meet regulatory compliance requirements while gaining cloud benefits like scalability and self-service provisioning. Which deployment model is MOST suitable?',
        '[{"text": "A) Public cloud with dedicated instances", "isCorrect": false}, {"text": "B) Private cloud hosted on-premises", "isCorrect": true}, {"text": "C) Hybrid cloud with data replication", "isCorrect": false}, {"text": "D) Community cloud shared with other financial institutions", "isCorrect": false}]',
        'B) Private cloud hosted on-premises',
        'Private cloud provides complete control, data sovereignty, and regulatory compliance while offering cloud capabilities like automation and scalability.',
        '{"summary": "Private cloud benefits for financial institutions:", "breakdown": ["Complete control: Full authority over infrastructure and security", "Data sovereignty: Data remains within institutional boundaries", "Regulatory compliance: Easier to meet strict financial regulations", "Cloud benefits: Self-service, automation, and scalability features"], "otherOptions": "A) Public cloud may not meet data sovereignty requirements\nC) Hybrid cloud introduces complexity for strict compliance needs\nD) Community cloud shares resources with other organizations"}'),
       (79, 79, 'Cloud Migration', 'Analysis', 'Domain 2: Cloud Deployment',
        'A company has a legacy monolithic application that works well but has outdated dependencies and architecture. They want to move to the cloud quickly while minimizing risk, then modernize later. Which migration strategy is MOST appropriate for the initial move?',
        '[{"text": "A) Rehost (lift and shift) to move quickly with minimal changes", "isCorrect": true}, {"text": "B) Refactor to microservices architecture immediately", "isCorrect": false}, {"text": "C) Rebuild the entire application using cloud-native services", "isCorrect": false}, {"text": "D) Replace with a SaaS solution", "isCorrect": false}]',
        'A) Rehost (lift and shift) to move quickly with minimal changes',
        'Rehosting allows quick migration with minimal risk and changes, providing immediate cloud benefits while enabling future modernization phases.',
        '{"summary": "Rehost strategy advantages:", "breakdown": ["Speed: Fastest migration approach with minimal changes", "Low risk: Preserves existing functionality and stability", "Immediate benefits: Cost savings and basic cloud features", "Future flexibility: Provides foundation for later modernization"], "otherOptions": "B) Refactoring increases complexity and migration risk\nC) Rebuilding takes significant time and resources\nD) SaaS replacement may not maintain existing functionality"}'),
       (80, 80, 'Infrastructure as Code', 'Application', 'Domain 2: Cloud Deployment',
        'A DevOps team manages infrastructure across development, staging, and production environments. They experience configuration drift and inconsistencies between environments, leading to deployment failures. Which approach would BEST solve these consistency issues?',
        '[{"text": "A) Document all configurations in detailed runbooks", "isCorrect": false}, {"text": "B) Implement Infrastructure as Code (IaC) with version control", "isCorrect": true}, {"text": "C) Create golden images for all server configurations", "isCorrect": false}, {"text": "D) Use configuration management scripts", "isCorrect": false}]',
        'B) Implement Infrastructure as Code (IaC) with version control',
        'Infrastructure as Code ensures consistent, repeatable deployments across environments by defining infrastructure in version-controlled code templates.',
        '{"summary": "IaC benefits for environment consistency:", "breakdown": ["Declarative definitions: Infrastructure defined as code templates", "Version control: Track and rollback infrastructure changes", "Consistency: Identical deployments across all environments", "Automation: Eliminates manual configuration errors"], "otherOptions": "A) Documentation doesn''t prevent manual configuration errors\nC) Golden images don''t address infrastructure configuration drift\nD) Scripts can vary in execution and may not be declarative"}'),
       (81, 81, 'Cloud Observability', 'Analysis', 'Domain 3: Cloud Operations and Support',
        'A microservices application experiences intermittent performance issues that are difficult to trace across multiple services. Standard monitoring shows healthy individual services, but users report slow response times. Which observability approach would BEST identify the root cause?',
        '[{"text": "A) Increase log verbosity on all services", "isCorrect": false}, {"text": "B) Implement distributed tracing across microservices", "isCorrect": true}, {"text": "C) Add more performance counters and metrics", "isCorrect": false}, {"text": "D) Set up alerting based on response time thresholds", "isCorrect": false}]',
        'B) Implement distributed tracing across microservices',
        'Distributed tracing follows requests across multiple microservices, providing visibility into the complete request path and identifying bottlenecks.',
        '{"summary": "Distributed tracing benefits:", "breakdown": ["End-to-end visibility: Tracks requests across all microservices", "Bottleneck identification: Shows where delays occur in the request path", "Service dependencies: Maps interactions between services", "Performance analysis: Measures latency at each service hop"], "otherOptions": "A) More logs don''t provide cross-service correlation\nC) Additional metrics don''t show service interactions\nD) Alerting identifies problems but doesn''t show root cause"}'),
       (82, 82, 'Cloud Scaling', 'Application', 'Domain 3: Cloud Operations and Support',
        'An e-commerce application experiences predictable traffic patterns with gradual increases during business hours and sudden spikes during flash sales. Which auto-scaling strategy would provide the BEST performance and cost optimization?',
        '[{"text": "A) Reactive scaling based only on CPU utilization", "isCorrect": false}, {"text": "B) Predictive scaling with scheduled scaling for business hours and reactive scaling for spikes", "isCorrect": true}, {"text": "C) Fixed scaling with maximum capacity provisioned at all times", "isCorrect": false}, {"text": "D) Manual scaling based on sales calendar events", "isCorrect": false}]',
        'B) Predictive scaling with scheduled scaling for business hours and reactive scaling for spikes',
        'Combined predictive and reactive scaling handles both predictable patterns efficiently and responds to unexpected spikes automatically.',
        '{"summary": "Hybrid scaling strategy benefits:", "breakdown": ["Predictive scaling: Anticipates business hour traffic increases", "Reactive scaling: Responds automatically to unexpected spikes", "Cost optimization: Scales down during low-traffic periods", "Performance assurance: Maintains responsiveness during all scenarios"], "otherOptions": "A) CPU-only reactive scaling too slow for sudden spikes\nC) Fixed capacity wastes resources during low traffic\nD) Manual scaling can''t respond quickly to unexpected events"}'),
       (83, 83, 'Cloud Backup Strategies', 'Analysis', 'Domain 3: Cloud Operations and Support',
        'A company needs to design a backup strategy for critical business data with a Recovery Point Objective (RPO) of 1 hour and Recovery Time Objective (RTO) of 2 hours. The solution must be cost-effective while meeting compliance requirements for 7-year retention. Which backup approach is MOST suitable?',
        '[{"text": "A) Daily full backups with 7-year retention in hot storage", "isCorrect": false}, {"text": "B) Hourly incremental backups with tiered storage and lifecycle policies", "isCorrect": true}, {"text": "C) Real-time replication to a secondary site with immediate failover", "isCorrect": false}, {"text": "D) Weekly full backups with manual restore processes", "isCorrect": false}]',
        'B) Hourly incremental backups with tiered storage and lifecycle policies',
        'Hourly incremental backups meet the RPO requirement, while tiered storage and lifecycle policies optimize costs for long-term retention.',
        '{"summary": "Optimal backup strategy components:", "breakdown": ["Hourly incrementals: Meet 1-hour RPO requirement efficiently", "Tiered storage: Hot storage for recent backups, cold for long-term", "Lifecycle policies: Automatic movement to cheaper storage over time", "Fast recovery: 2-hour RTO achievable from recent incremental backups"], "otherOptions": "A) Daily backups exceed 1-hour RPO requirement\nC) Real-time replication expensive for 7-year retention\nD) Weekly backups far exceed RPO requirement"}'),
       (84, 84, 'Cloud Security - Access Management', 'Application', 'Domain 4: Cloud Security',
        'A global organization needs to manage user access to cloud resources across multiple locations with different security requirements. They want to implement Zero Trust principles while maintaining user productivity. Which approach BEST achieves these goals?',
        '[{"text": "A) VPN access with network-based security controls", "isCorrect": false}, {"text": "B) Multi-factor authentication with conditional access policies based on context", "isCorrect": true}, {"text": "C) Single sign-on with basic username/password authentication", "isCorrect": false}, {"text": "D) Role-based access control with periodic access reviews", "isCorrect": false}]',
        'B) Multi-factor authentication with conditional access policies based on context',
        'Conditional access with MFA implements Zero Trust by continuously verifying users based on context like location, device, and behavior patterns.',
        '{"summary": "Zero Trust conditional access benefits:", "breakdown": ["Context awareness: Considers location, device, time, and behavior", "Continuous verification: Doesn''t trust based solely on network location", "Risk-based decisions: Adjusts requirements based on calculated risk", "User productivity: Seamless access for low-risk scenarios"], "otherOptions": "A) VPN assumes trust based on network location\nC) Basic authentication insufficient for Zero Trust\nD) RBAC alone doesn''t provide continuous verification"}'),
       (85, 85, 'Cloud Compliance', 'Analysis', 'Domain 4: Cloud Security',
        'A healthcare organization moving to the cloud must demonstrate HIPAA compliance for patient data. They need automated compliance monitoring, evidence collection, and remediation capabilities. Which combination of cloud security controls provides the MOST comprehensive compliance framework?',
        '[{"text": "A) Manual security audits with document-based evidence collection", "isCorrect": false}, {"text": "B) Cloud security posture management (CSPM) with automated policy enforcement and audit trails", "isCorrect": true}, {"text": "C) Encryption of all data with annual compliance reviews", "isCorrect": false}, {"text": "D) Network segmentation with firewall rules and access logs", "isCorrect": false}]',
        'B) Cloud security posture management (CSPM) with automated policy enforcement and audit trails',
        'CSPM provides continuous compliance monitoring, automated policy enforcement, and detailed audit trails required for HIPAA compliance demonstration.',
        '{"summary": "CSPM benefits for HIPAA compliance:", "breakdown": ["Continuous monitoring: Real-time compliance posture assessment", "Automated enforcement: Immediate remediation of policy violations", "Audit trails: Comprehensive logging for compliance evidence", "Risk assessment: Identifies and prioritizes compliance gaps"], "otherOptions": "A) Manual audits don''t provide continuous compliance monitoring\nC) Encryption alone doesn''t address all HIPAA requirements\nD) Network controls are part of compliance but not comprehensive"}'),
       (86, 86, 'Cloud Security - Vulnerability Management', 'Application', 'Domain 4: Cloud Security',
        'A development team deploys applications using container images from various sources. Security scans reveal vulnerabilities in base images and third-party components. Which approach provides the BEST security posture for the container supply chain?',
        '[{"text": "A) Scan containers only in production environments", "isCorrect": false}, {"text": "B) Implement security scanning throughout the CI/CD pipeline with policy enforcement", "isCorrect": true}, {"text": "C) Use only official base images from operating system vendors", "isCorrect": false}, {"text": "D) Perform monthly vulnerability assessments on deployed containers", "isCorrect": false}]',
        'B) Implement security scanning throughout the CI/CD pipeline with policy enforcement',
        'Pipeline security scanning catches vulnerabilities early, enforces security policies, and prevents vulnerable images from reaching production.',
        '{"summary": "CI/CD security scanning benefits:", "breakdown": ["Shift-left security: Identifies vulnerabilities early in development", "Policy enforcement: Blocks deployment of vulnerable images", "Continuous scanning: Monitors throughout the software lifecycle", "Supply chain security: Validates all components and dependencies"], "otherOptions": "A) Production-only scanning allows vulnerabilities to reach live systems\nC) Official images can still contain vulnerabilities\nD) Monthly scans too infrequent for active development"}'),
       (87, 87, 'DevOps CI/CD', 'Application', 'Domain 5: DevOps Fundamentals',
        'A development team wants to implement automated deployments while ensuring code quality and minimizing deployment risks. They currently perform manual testing and deployment processes. Which CI/CD pipeline design BEST balances automation with quality assurance?',
        '[{"text": "A) Automated build and deployment without testing stages", "isCorrect": false}, {"text": "B) Automated build, test, and staged deployment with approval gates", "isCorrect": true}, {"text": "C) Manual build with automated deployment to all environments", "isCorrect": false}, {"text": "D) Automated deployment only to development environments", "isCorrect": false}]',
        'B) Automated build, test, and staged deployment with approval gates',
        'Staged deployment with automated testing and approval gates provides comprehensive automation while maintaining quality controls and risk mitigation.',
        '{"summary": "Comprehensive CI/CD pipeline benefits:", "breakdown": ["Automated testing: Catches issues early in the pipeline", "Staged deployment: Progressive rollout reduces risk", "Approval gates: Human oversight for critical stages", "Quality assurance: Multiple validation points ensure code quality"], "otherOptions": "A) No testing increases deployment risk\nC) Manual build defeats automation benefits\nD) Limited to dev environments doesn''t provide full deployment automation"}'),
       (88, 88, 'DevOps Version Control', 'Knowledge', 'Domain 5: DevOps Fundamentals',
        'A DevOps team manages both application code and infrastructure configurations. They need to implement version control strategies that support collaboration, change tracking, and rollback capabilities. Which approach provides the MOST comprehensive version management?',
        '[{"text": "A) Separate repositories for code and infrastructure with different branching strategies", "isCorrect": false}, {"text": "B) Single repository with unified branching strategy for both code and infrastructure", "isCorrect": true}, {"text": "C) Version control for application code only, with manual infrastructure management", "isCorrect": false}, {"text": "D) Multiple repositories per microservice with independent versioning", "isCorrect": false}]',
        'B) Single repository with unified branching strategy for both code and infrastructure',
        'Unified repository and branching strategy ensures synchronized changes between application code and infrastructure, simplifying deployment and rollback procedures.',
        '{"summary": "Unified version control benefits:", "breakdown": ["Synchronized changes: Code and infrastructure changes tracked together", "Simplified rollbacks: Single point to revert both code and infrastructure", "Consistent branching: Same workflow for all team members", "Atomic deployments: Code and infrastructure deployed as single unit"], "otherOptions": "A) Separate repositories can lead to version mismatches\nC) Manual infrastructure management introduces inconsistency\nD) Multiple repositories increase complexity and coordination overhead"}'),
       (89, 89, 'Cloud Troubleshooting - Network', 'Analysis', 'Domain 6: Troubleshooting',
        'Users report intermittent connectivity issues to a cloud-hosted web application. The application works fine from the office but fails sporadically from remote locations. Network monitoring shows no infrastructure issues. Which troubleshooting approach would MOST effectively identify the root cause?',
        '[{"text": "A) Increase server resources and add more instances", "isCorrect": false}, {"text": "B) Analyze network paths and implement distributed monitoring from multiple locations", "isCorrect": true}, {"text": "C) Check firewall logs and security group configurations", "isCorrect": false}, {"text": "D) Review application performance metrics and database queries", "isCorrect": false}]',
        'B) Analyze network paths and implement distributed monitoring from multiple locations',
        'Distributed monitoring from multiple geographic locations helps identify network path issues, ISP problems, or regional connectivity challenges.',
        '{"summary": "Distributed network troubleshooting approach:", "breakdown": ["Geographic perspective: Monitoring from affected user locations", "Network path analysis: Traces routes to identify bottlenecks", "ISP correlation: Identifies provider-specific issues", "Performance baselines: Compares connectivity quality across locations"], "otherOptions": "A) Resource scaling doesn''t address location-specific connectivity\nC) Security configurations affect access, not intermittent connectivity\nD) Application metrics don''t reveal network path issues"}'),
       (90, 90, 'Cloud Troubleshooting - Performance', 'Analysis', 'Domain 6: Troubleshooting',
        'A cloud application experiences performance degradation only during specific hours despite consistent user load. CPU and memory utilization remain within normal ranges. Database queries show normal execution times. Which factor is MOST likely causing the performance issues?',
        '[{"text": "A) Application memory leaks accumulating over time", "isCorrect": false}, {"text": "B) Resource contention with other workloads sharing the same physical infrastructure", "isCorrect": true}, {"text": "C) Database connection pool exhaustion", "isCorrect": false}, {"text": "D) Network bandwidth limitations during peak traffic", "isCorrect": false}]',
        'B) Resource contention with other workloads sharing the same physical infrastructure',
        'Time-specific performance issues with normal resource utilization often indicate "noisy neighbor" problems where other workloads compete for underlying physical resources.',
        '{"summary": "Noisy neighbor characteristics:", "breakdown": ["Time correlation: Performance degrades at specific, recurring times", "Normal metrics: Application-level resources appear adequate", "Shared infrastructure: Multiple workloads compete for physical resources", "External dependency: Performance affected by factors outside direct control"], "otherOptions": "A) Memory leaks would show gradually increasing memory usage\nC) Connection pool issues would show in database connection metrics\nD) Network bandwidth problems would show in network utilization metrics"}'),
       (91, 91, 'Cloud Troubleshooting - Security', 'Application', 'Domain 6: Troubleshooting',
        'A cloud application suddenly starts receiving "Access Denied" errors for API calls that were previously working. No code changes were deployed recently. Security logs show successful authentication but failed authorization. Which troubleshooting approach would MOST quickly identify the issue?',
        '[{"text": "A) Review recent changes to IAM roles, policies, and resource permissions", "isCorrect": true}, {"text": "B) Check network security group rules and firewall configurations", "isCorrect": false}, {"text": "C) Verify SSL certificates and encryption configurations", "isCorrect": false}, {"text": "D) Examine application logs for authentication token issues", "isCorrect": false}]',
        'A) Review recent changes to IAM roles, policies, and resource permissions',
        'Access Denied with successful authentication indicates an authorization problem, typically caused by recent changes to IAM policies or role permissions.',
        '{"summary": "Authorization troubleshooting focus areas:", "breakdown": ["IAM policy changes: Recent modifications to access permissions", "Role updates: Changes to role assignments or capabilities", "Resource permissions: Updates to resource-specific access controls", "Time-based policies: Scheduled changes or policy expirations"], "otherOptions": "B) Network rules affect connectivity, not authorization after authentication\nC) SSL issues would prevent successful authentication\nD) Authentication is working; the issue is with authorization"}'),
       (92, 92, 'Cloud Troubleshooting - Integration', 'Expert', 'Domain 6: Troubleshooting',
        'After migrating a legacy application to the cloud, users report that batch processing jobs that completed in 2 hours on-premises now take 6 hours in the cloud. The application code was not modified during migration. Which factors should be investigated FIRST to identify the performance degradation?',
        '[{"text": "A) Cloud instance sizing and compute resources compared to on-premises hardware", "isCorrect": false}, {"text": "B) Network latency between cloud services and data storage locations", "isCorrect": false}, {"text": "C) Storage I/O performance and disk configuration differences", "isCorrect": true}, {"text": "D) Application timeout settings and connection pool configurations", "isCorrect": false}]',
        'C) Storage I/O performance and disk configuration differences',
        'Batch processing performance is often heavily dependent on storage I/O patterns, which can be significantly different between on-premises and cloud storage configurations.',
        '{"summary": "Storage I/O impact on batch processing:", "breakdown": ["I/O patterns: Batch jobs typically involve intensive read/write operations", "Storage types: Cloud storage may have different performance characteristics", "Configuration differences: RAID, caching, and optimization settings", "Sequential vs random: Batch workloads often require high sequential throughput"], "otherOptions": "A) Compute resources would show in CPU/memory utilization\nB) Network latency affects real-time applications more than batch processing\nD) Configuration issues would likely cause failures, not just slower performance"}');
